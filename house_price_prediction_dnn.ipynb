{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, LassoCV,LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Adam, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>MasVnrArea_absent</th>\n",
       "      <th>MiscVal_absent</th>\n",
       "      <th>OpenPorchSF_absent</th>\n",
       "      <th>PoolArea_absent</th>\n",
       "      <th>ScreenPorch_absent</th>\n",
       "      <th>TotalBsmtSF_absent</th>\n",
       "      <th>WoodDeckSF_absent</th>\n",
       "      <th>compPrice</th>\n",
       "      <th>compIndex</th>\n",
       "      <th>compPriceXcompIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75.920375</td>\n",
       "      <td>37.2</td>\n",
       "      <td>2824.237967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.907207</td>\n",
       "      <td>32.4</td>\n",
       "      <td>2070.593520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>81.912811</td>\n",
       "      <td>37.2</td>\n",
       "      <td>3047.156573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>272</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.113339</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2204.513512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.591085</td>\n",
       "      <td>36.4</td>\n",
       "      <td>3115.515497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       856       854          0             3       706.0         0.0   \n",
       "1      1262         0          0             3       978.0         0.0   \n",
       "2       920       866          0             3       486.0         0.0   \n",
       "3       961       756          0             3       216.0         0.0   \n",
       "4      1145      1053          0             4       655.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  \\\n",
       "0           1.0           0.0      150.0              0  ...   \n",
       "1           0.0           1.0      284.0              0  ...   \n",
       "2           1.0           0.0      434.0              0  ...   \n",
       "3           1.0           0.0      540.0            272  ...   \n",
       "4           1.0           0.0      490.0              0  ...   \n",
       "\n",
       "   MasVnrArea_absent  MiscVal_absent  OpenPorchSF_absent  PoolArea_absent  \\\n",
       "0                  0               1                   0                1   \n",
       "1                  1               1                   1                1   \n",
       "2                  0               1                   0                1   \n",
       "3                  1               1                   0                1   \n",
       "4                  0               1                   0                1   \n",
       "\n",
       "   ScreenPorch_absent  TotalBsmtSF_absent  WoodDeckSF_absent  compPrice  \\\n",
       "0                   1                   0                  1  75.920375   \n",
       "1                   1                   0                  0  63.907207   \n",
       "2                   1                   0                  1  81.912811   \n",
       "3                   1                   0                  1  71.113339   \n",
       "4                   1                   0                  0  85.591085   \n",
       "\n",
       "   compIndex  compPriceXcompIndex  \n",
       "0       37.2          2824.237967  \n",
       "1       32.4          2070.593520  \n",
       "2       37.2          3047.156573  \n",
       "3       31.0          2204.513512  \n",
       "4       36.4          3115.515497  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('dataset2_addfeatures.csv')\n",
    "# sale price\n",
    "SalePrice = data['SalePrice'].dropna()\n",
    "y = np.log1p(SalePrice)\n",
    "data = data.drop(['SalePrice','Id'],axis=1)\n",
    "cols_categoric = data.select_dtypes(include = [\"object\"]).columns.tolist()\n",
    "# number of training samples\n",
    "n_train = SalePrice.shape[0]\n",
    "# add total square footage\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function dummifies categoric features\n",
    "def get_custom_dummies(data,y,n_train):\n",
    "    cols_categoric = data.select_dtypes(include = [\"object\"]).columns.tolist()\n",
    "    cols_numeric = set(data.columns.tolist())-set(cols_categoric)\n",
    "    df = data.loc[:,cols_numeric]\n",
    "    for col in cols_categoric:        \n",
    "        dummies = pd.get_dummies(data[[col]])\n",
    "        idx_min_corr = np.argmin(np.abs(np.array([dummies[:n_train][subcol].corr(y) for subcol in dummies.columns.tolist()])))\n",
    "        col_min_corr = dummies.columns.tolist()[idx_min_corr]\n",
    "        df = pd.concat([df,dummies.drop(col_min_corr,axis=1)],axis=1)    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BsmtFinSF_log</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>LotFrontage_log</th>\n",
       "      <th>BsmtFinSF</th>\n",
       "      <th>TotalBath</th>\n",
       "      <th>compPriceXcompIndex</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_COD</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>Street_Grvl</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.734988</td>\n",
       "      <td>-0.249767</td>\n",
       "      <td>0.992109</td>\n",
       "      <td>-0.359678</td>\n",
       "      <td>0.426131</td>\n",
       "      <td>0.434517</td>\n",
       "      <td>0.466232</td>\n",
       "      <td>1.156979</td>\n",
       "      <td>0.954581</td>\n",
       "      <td>1.047051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.842113</td>\n",
       "      <td>3.820454</td>\n",
       "      <td>-0.286434</td>\n",
       "      <td>-0.359678</td>\n",
       "      <td>-0.475443</td>\n",
       "      <td>0.562857</td>\n",
       "      <td>1.049193</td>\n",
       "      <td>0.378094</td>\n",
       "      <td>-0.692736</td>\n",
       "      <td>0.155579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.612309</td>\n",
       "      <td>-0.249767</td>\n",
       "      <td>-0.286434</td>\n",
       "      <td>-0.359678</td>\n",
       "      <td>0.579077</td>\n",
       "      <td>0.462374</td>\n",
       "      <td>-0.005281</td>\n",
       "      <td>1.156979</td>\n",
       "      <td>1.441837</td>\n",
       "      <td>0.981016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.346270</td>\n",
       "      <td>-0.249767</td>\n",
       "      <td>0.352837</td>\n",
       "      <td>3.872900</td>\n",
       "      <td>0.440219</td>\n",
       "      <td>0.385147</td>\n",
       "      <td>-0.583956</td>\n",
       "      <td>-1.179676</td>\n",
       "      <td>-0.400013</td>\n",
       "      <td>-1.858487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710348</td>\n",
       "      <td>-0.249767</td>\n",
       "      <td>1.631380</td>\n",
       "      <td>-0.359678</td>\n",
       "      <td>1.408203</td>\n",
       "      <td>0.593064</td>\n",
       "      <td>0.356926</td>\n",
       "      <td>1.156979</td>\n",
       "      <td>1.591256</td>\n",
       "      <td>0.947999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BsmtFinSF_log  BsmtHalfBath  TotRmsAbvGrd  EnclosedPorch  GrLivArea  \\\n",
       "0       0.734988     -0.249767      0.992109      -0.359678   0.426131   \n",
       "1       0.842113      3.820454     -0.286434      -0.359678  -0.475443   \n",
       "2       0.612309     -0.249767     -0.286434      -0.359678   0.579077   \n",
       "3       0.346270     -0.249767      0.352837       3.872900   0.440219   \n",
       "4       0.710348     -0.249767      1.631380      -0.359678   1.408203   \n",
       "\n",
       "   LotFrontage_log  BsmtFinSF  TotalBath  compPriceXcompIndex  YearBuilt  ...  \\\n",
       "0         0.434517   0.466232   1.156979             0.954581   1.047051  ...   \n",
       "1         0.562857   1.049193   0.378094            -0.692736   0.155579  ...   \n",
       "2         0.462374  -0.005281   1.156979             1.441837   0.981016  ...   \n",
       "3         0.385147  -0.583956  -1.179676            -0.400013  -1.858487  ...   \n",
       "4         0.593064   0.356926   1.156979             1.591256   0.947999  ...   \n",
       "\n",
       "   SaleType_COD  SaleType_CWD  SaleType_Con  SaleType_ConLD  SaleType_ConLw  \\\n",
       "0     -0.175304      -0.06426      -0.04143       -0.094817       -0.052432   \n",
       "1     -0.175304      -0.06426      -0.04143       -0.094817       -0.052432   \n",
       "2     -0.175304      -0.06426      -0.04143       -0.094817       -0.052432   \n",
       "3     -0.175304      -0.06426      -0.04143       -0.094817       -0.052432   \n",
       "4     -0.175304      -0.06426      -0.04143       -0.094817       -0.052432   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  Street_Grvl  Utilities_AllPub  \n",
       "0     -0.297326     -0.049037     0.393366     -0.06426          0.018515  \n",
       "1     -0.297326     -0.049037     0.393366     -0.06426          0.018515  \n",
       "2     -0.297326     -0.049037     0.393366     -0.06426          0.018515  \n",
       "3     -0.297326     -0.049037     0.393366     -0.06426          0.018515  \n",
       "4     -0.297326     -0.049037     0.393366     -0.06426          0.018515  \n",
       "\n",
       "[5 rows x 337 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummify data\n",
    "data = get_custom_dummies(data,y,n_train)\n",
    "data = (data - data.mean())/data.std()\n",
    "# drop duplicates\n",
    "X = data[:n_train]\n",
    "X = X.T.drop_duplicates().T\n",
    "# find columns\n",
    "cols = X.columns.tolist()\n",
    "data = data.loc[:,cols]\n",
    "X = data[:n_train]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458, 337)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid 1:\n",
      "CV score: 0.11096522844685176\n",
      "{'alpha': 0.001}\n",
      "grid 2:\n",
      "CV score: 0.1074268612067819\n",
      "{'alpha': 0.003430469286314919}\n"
     ]
    }
   ],
   "source": [
    "# lasso model\n",
    "lasso = Lasso(random_state=1)\n",
    "# determine best alpha using GridSearch\n",
    "# grid 1\n",
    "print(\"grid 1:\")\n",
    "alphas = {'alpha': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]}\n",
    "model = GridSearchCV(estimator=lasso, param_grid=alphas, n_jobs=-1, cv=10, scoring='neg_mean_squared_error')\n",
    "model.fit(X,y)\n",
    "print(\"CV score: {}\".format(np.sqrt(-model.best_score_)))\n",
    "print(model.best_params_)\n",
    "# grid 2\n",
    "print(\"grid 2:\")\n",
    "alphas = {'alpha': np.geomspace(1e-2, 1e-4, num=100)}\n",
    "model = GridSearchCV(estimator=lasso, param_grid=alphas, n_jobs=-1, cv=10, scoring='neg_mean_squared_error')\n",
    "model.fit(X,y)\n",
    "print(\"CV score: {}\".format(np.sqrt(-model.best_score_)))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso plot coeff\n",
    "def lasso_plot_coef(model,X,n_features):\n",
    "    # series of coefficients\n",
    "    cols=X.columns.tolist()\n",
    "    # all pandas series\n",
    "    coefs = pd.Series(model.coef_.tolist(),index=cols)\n",
    "    threshold = np.abs(coefs).sort_values(ascending=False)[:n_features].values[-1]\n",
    "    coefs = coefs[np.abs(coefs)>=threshold].sort_values(ascending=True)    \n",
    "    objects = coefs.index.tolist()\n",
    "    y_arange = np.arange(len(objects))\n",
    "    values = coefs.tolist()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.barh(y_arange, values, align='center')\n",
    "    plt.yticks(y_arange, objects)\n",
    "    plt.xlabel('features')\n",
    "    plt.title('LASSO Feature Importance: {}'.format(n_features)+' most important features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set score: 0.0968003561563015\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# score on entire training set\n",
    "print(\"training set score: {}\".format( np.sqrt(mean_squared_error(y,model.predict(X))) ))\n",
    "lasso_plot_coef(model.best_estimator_,X,n_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>BsmtFinSF</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>AllPorchSF</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>5_bedroom_plus_dummy</th>\n",
       "      <th>YearRemodAddAge</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolQC_Gd</th>\n",
       "      <th>RoofMatl_Membran</th>\n",
       "      <th>RoofMatl_WdShngl</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>Street_Grvl</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>0.466232</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>0.307685</td>\n",
       "      <td>-0.761751</td>\n",
       "      <td>-0.216400</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>-0.887744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>2.186999</td>\n",
       "      <td>-0.061413</td>\n",
       "      <td>1.049193</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>0.307685</td>\n",
       "      <td>0.721678</td>\n",
       "      <td>-0.069097</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>0.356886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>-0.005281</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>0.307685</td>\n",
       "      <td>-0.880675</td>\n",
       "      <td>0.142251</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>-0.839874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>-0.583956</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>1.619846</td>\n",
       "      <td>0.778011</td>\n",
       "      <td>-0.075501</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>0.596238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>3.787837</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>1.360350</td>\n",
       "      <td>0.356926</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>1.619846</td>\n",
       "      <td>0.583976</td>\n",
       "      <td>0.527801</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>-0.744133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PoolArea  OverallCond  OverallQual  BsmtFinSF  ScreenPorch  GarageCars  \\\n",
       "0 -0.060437    -0.507416     0.649468   0.466232    -0.285992    0.307685   \n",
       "1 -0.060437     2.186999    -0.061413   1.049193    -0.285992    0.307685   \n",
       "2 -0.060437    -0.507416     0.649468  -0.005281    -0.285992    0.307685   \n",
       "3 -0.060437    -0.507416     0.649468  -0.583956    -0.285992    1.619846   \n",
       "4 -0.060437    -0.507416     1.360350   0.356926    -0.285992    1.619846   \n",
       "\n",
       "   AllPorchSF   LotArea  5_bedroom_plus_dummy  YearRemodAddAge  ...  \\\n",
       "0   -0.761751 -0.216400             -0.156776        -0.887744  ...   \n",
       "1    0.721678 -0.069097             -0.156776         0.356886  ...   \n",
       "2   -0.880675  0.142251             -0.156776        -0.839874  ...   \n",
       "3    0.778011 -0.075501             -0.156776         0.596238  ...   \n",
       "4    0.583976  0.527801             -0.156776        -0.744133  ...   \n",
       "\n",
       "   PoolQC_Gd  RoofMatl_Membran  RoofMatl_WdShngl  SaleCondition_Abnorml  \\\n",
       "0  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "1  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "2  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "3  -0.032081         -0.018515         -0.049037               3.787837   \n",
       "4  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "\n",
       "   SaleCondition_Family  SaleType_ConLD  SaleType_New  SaleType_WD  \\\n",
       "0             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "1             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "2             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "3             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "4             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "\n",
       "   Street_Grvl  Utilities_AllPub  \n",
       "0     -0.06426          0.018515  \n",
       "1     -0.06426          0.018515  \n",
       "2     -0.06426          0.018515  \n",
       "3     -0.06426          0.018515  \n",
       "4     -0.06426          0.018515  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=X.columns.tolist()\n",
    "# all pandas series\n",
    "coefs = pd.Series(model.best_estimator_.coef_.tolist(),index=cols)\n",
    "coefs = coefs[coefs!=0]\n",
    "data = data.loc[:,coefs.index.tolist()]\n",
    "X = data[:n_train]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeated k-fold\n",
    "repeatedkfold = RepeatedKFold()\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=2652124)\n",
    "# LassoCV\n",
    "lassocv = LassoCV(cv=rkf,n_alphas=100,n_jobs=-1)\n",
    "# model validation score\n",
    "def model_validation_score(X,y,model=lassocv,n_split=10,n_repeats=1,msg=False):\n",
    "    repeatedkfold = RepeatedKFold()\n",
    "    rkf = RepeatedKFold(n_splits=n_split, n_repeats=n_repeats, random_state=1)\n",
    "    sample = range(0,X.shape[0])\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "    y_train_lst = []\n",
    "    y_train_estimated_lst = []\n",
    "    y_test_lst = []\n",
    "    y_test_estimated_lst = []\n",
    "    iteration = 1\n",
    "    for train_index, test_index in rkf.split(sample):\n",
    "        if (msg==True): print('iteration {}'.format(iteration))\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        reg = model.fit(X_train,y_train)        \n",
    "        train_score.append(reg.score(X_train,y_train))\n",
    "        test_score.append(reg.score(X_test,y_test))\n",
    "        y_train_lst += y_train.tolist()\n",
    "        y_test_lst += y_test.tolist()\n",
    "        y_train_estimated_lst += model.predict(X_train).tolist()\n",
    "        y_test_estimated_lst += model.predict(X_test).tolist()\n",
    "        iteration += 1\n",
    "    train_score = np.sqrt(mean_squared_error(y_train_lst,y_train_estimated_lst))\n",
    "    test_score = np.sqrt(mean_squared_error(y_test_lst,y_test_estimated_lst))\n",
    "    if (msg==True):\n",
    "        print('train root_mean_squared_log_error: {}'.format(train_score))\n",
    "        print('test root_mean_squared_log_error: {}'.format(test_score))\n",
    "    return(train_score,test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09247022707535052, 0.10557792779844272)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_validation_score(X,y,model=lassocv,n_split=10,n_repeats=10,msg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit lasso with Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Peter\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set score: 0.11312652698333522\n"
     ]
    }
   ],
   "source": [
    "# define custom metric\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def rmse(y_true,y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred-y_true),axis=-1))\n",
    "\n",
    "NN_model = Sequential()\n",
    "\"\"\"\n",
    "    Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n",
    "          bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \n",
    "          activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\"\"\"\n",
    "# The Output Layer:\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', input_dim = X.shape[1], \n",
    "                   kernel_regularizer=l1(0.003430469286314919), activation='linear'))\n",
    "# Compile the network:\n",
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "NN_model.compile(loss='mse', optimizer=sgd, metrics=[rmse])\n",
    "history = NN_model.fit(X, y, epochs=300, batch_size=1000, verbose = 0)\n",
    "plt.plot(history.history['rmse'])\n",
    "plt.ylim((0, 0.2))\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'],loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# score on entire training set\n",
    "print(\"training set score: {}\".format( np.sqrt(mean_squared_error(NN_model.predict(X),y)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit deep neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 2)                 676       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 685\n",
      "Trainable params: 685\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(2, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n",
    "NN_model.add(Dropout(0.2))\n",
    "\n",
    "NN_model.add(Dense(2, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n",
    "NN_model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network:\n",
    "sgd = SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer=sgd, metrics=[rmse])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1166 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1166/1166 [==============================] - 1s 875us/step - loss: 11.8069 - rmse: 11.8069 - val_loss: 11.4142 - val_rmse: 11.4142\n",
      "Epoch 2/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 10.9590 - rmse: 10.9590 - val_loss: 10.3780 - val_rmse: 10.3780\n",
      "Epoch 3/1000\n",
      "1166/1166 [==============================] - 0s 85us/step - loss: 9.8580 - rmse: 9.8580 - val_loss: 9.2235 - val_rmse: 9.2235\n",
      "Epoch 4/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 8.6840 - rmse: 8.6840 - val_loss: 8.0314 - val_rmse: 8.0314\n",
      "Epoch 5/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 7.4797 - rmse: 7.4797 - val_loss: 6.8075 - val_rmse: 6.8075\n",
      "Epoch 6/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 6.2134 - rmse: 6.2134 - val_loss: 5.4522 - val_rmse: 5.4522\n",
      "Epoch 7/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 4.6086 - rmse: 4.6086 - val_loss: 3.2874 - val_rmse: 3.2874\n",
      "Epoch 8/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 1.9870 - rmse: 1.9870 - val_loss: 0.5527 - val_rmse: 0.5527\n",
      "Epoch 9/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.9054 - rmse: 0.9054 - val_loss: 0.6364 - val_rmse: 0.6364\n",
      "Epoch 10/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.7720 - rmse: 0.7720 - val_loss: 0.6123 - val_rmse: 0.6123\n",
      "Epoch 11/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.7048 - rmse: 0.7048 - val_loss: 0.5637 - val_rmse: 0.5637\n",
      "Epoch 12/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.6631 - rmse: 0.6631 - val_loss: 0.4682 - val_rmse: 0.4682\n",
      "Epoch 13/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.5927 - rmse: 0.5927 - val_loss: 0.4485 - val_rmse: 0.4485\n",
      "Epoch 14/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.5437 - rmse: 0.5437 - val_loss: 0.3831 - val_rmse: 0.3831\n",
      "Epoch 15/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.5048 - rmse: 0.5048 - val_loss: 0.3358 - val_rmse: 0.3358\n",
      "Epoch 16/1000\n",
      "1166/1166 [==============================] - 0s 79us/step - loss: 0.4546 - rmse: 0.4546 - val_loss: 0.3363 - val_rmse: 0.3363\n",
      "Epoch 17/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.4010 - rmse: 0.4010 - val_loss: 0.2992 - val_rmse: 0.2992\n",
      "Epoch 18/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.3632 - rmse: 0.3632 - val_loss: 0.2481 - val_rmse: 0.2481\n",
      "Epoch 19/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.3494 - rmse: 0.3494 - val_loss: 0.2299 - val_rmse: 0.2299\n",
      "Epoch 20/1000\n",
      "1166/1166 [==============================] - 0s 60us/step - loss: 0.2759 - rmse: 0.2759 - val_loss: 0.2197 - val_rmse: 0.2197\n",
      "Epoch 21/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.2384 - rmse: 0.23 - 0s 45us/step - loss: 0.2616 - rmse: 0.2616 - val_loss: 0.1873 - val_rmse: 0.1873\n",
      "Epoch 22/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.2271 - rmse: 0.2271 - val_loss: 0.1630 - val_rmse: 0.1630\n",
      "Epoch 23/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.2145 - rmse: 0.2145 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 24/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.2091 - rmse: 0.2091 - val_loss: 0.1612 - val_rmse: 0.1612\n",
      "Epoch 25/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1935 - rmse: 0.1935 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 26/1000\n",
      "1166/1166 [==============================] - 0s 57us/step - loss: 0.2086 - rmse: 0.2086 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 27/1000\n",
      "1166/1166 [==============================] - 0s 70us/step - loss: 0.1834 - rmse: 0.1834 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 28/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1840 - rmse: 0.1840 - val_loss: 0.1634 - val_rmse: 0.1634\n",
      "Epoch 29/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1931 - rmse: 0.1931 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 30/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1916 - rmse: 0.1916 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 31/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1838 - rmse: 0.1838 - val_loss: 0.1495 - val_rmse: 0.1495\n",
      "Epoch 32/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1917 - rmse: 0.1917 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 33/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1866 - rmse: 0.1866 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 34/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1920 - rmse: 0.1920 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 35/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1891 - rmse: 0.1891 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 36/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1871 - rmse: 0.1871 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 37/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1950 - rmse: 0.1950 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 38/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1572 - val_rmse: 0.1572\n",
      "Epoch 39/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1893 - rmse: 0.1893 - val_loss: 0.1606 - val_rmse: 0.1606\n",
      "Epoch 40/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1832 - rmse: 0.1832 - val_loss: 0.1525 - val_rmse: 0.1525\n",
      "Epoch 41/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1972 - rmse: 0.1972 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 42/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1814 - rmse: 0.1814 - val_loss: 0.1590 - val_rmse: 0.1590\n",
      "Epoch 43/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 44/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1851 - rmse: 0.1851 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 45/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1923 - rmse: 0.1923 - val_loss: 0.1525 - val_rmse: 0.1525\n",
      "Epoch 46/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1797 - rmse: 0.1797 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 47/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1864 - rmse: 0.1864 - val_loss: 0.1579 - val_rmse: 0.1579\n",
      "Epoch 48/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1828 - rmse: 0.1828 - val_loss: 0.1496 - val_rmse: 0.1496\n",
      "Epoch 49/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 50/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1901 - rmse: 0.1901 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 51/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 52/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1852 - rmse: 0.1852 - val_loss: 0.1506 - val_rmse: 0.1506\n",
      "Epoch 53/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1790 - rmse: 0.1790 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 54/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 55/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1909 - rmse: 0.1909 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 56/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1883 - rmse: 0.1883 - val_loss: 0.1531 - val_rmse: 0.1531\n",
      "Epoch 57/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1785 - rmse: 0.1785 - val_loss: 0.1419 - val_rmse: 0.1419\n",
      "Epoch 58/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1871 - rmse: 0.1871 - val_loss: 0.1516 - val_rmse: 0.1516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1929 - rmse: 0.1929 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 60/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1812 - rmse: 0.1812 - val_loss: 0.1470 - val_rmse: 0.1470\n",
      "Epoch 61/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1839 - rmse: 0.1839 - val_loss: 0.1502 - val_rmse: 0.1502\n",
      "Epoch 62/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1858 - rmse: 0.1858 - val_loss: 0.1510 - val_rmse: 0.1510\n",
      "Epoch 63/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1765 - rmse: 0.1765 - val_loss: 0.1493 - val_rmse: 0.1493\n",
      "Epoch 64/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 65/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 66/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1843 - rmse: 0.1843 - val_loss: 0.1592 - val_rmse: 0.1592\n",
      "Epoch 67/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1812 - rmse: 0.1812 - val_loss: 0.1497 - val_rmse: 0.1497\n",
      "Epoch 68/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1779 - rmse: 0.1779 - val_loss: 0.1506 - val_rmse: 0.1506\n",
      "Epoch 69/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1807 - rmse: 0.1807 - val_loss: 0.1488 - val_rmse: 0.1488\n",
      "Epoch 70/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1862 - rmse: 0.1862 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 71/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1503 - val_rmse: 0.1503\n",
      "Epoch 72/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1849 - rmse: 0.1849 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 73/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1796 - rmse: 0.1796 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 74/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1888 - rmse: 0.1888 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 75/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1813 - rmse: 0.1813 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 76/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1830 - rmse: 0.1830 - val_loss: 0.1517 - val_rmse: 0.1517\n",
      "Epoch 77/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 78/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1858 - rmse: 0.1858 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 79/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1890 - rmse: 0.1890 - val_loss: 0.1485 - val_rmse: 0.1485\n",
      "Epoch 80/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 81/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 82/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1737 - rmse: 0.1737 - val_loss: 0.1495 - val_rmse: 0.1495\n",
      "Epoch 83/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1493 - val_rmse: 0.1493\n",
      "Epoch 84/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1668 - rmse: 0.1668 - val_loss: 0.1500 - val_rmse: 0.1500\n",
      "Epoch 85/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 86/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1790 - rmse: 0.1790 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 87/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 88/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1958 - rmse: 0.1958 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 89/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 90/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 91/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 92/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1512 - val_rmse: 0.1512\n",
      "Epoch 93/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1716 - rmse: 0.1716 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 94/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 95/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1609 - val_rmse: 0.1609\n",
      "Epoch 96/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1793 - rmse: 0.1793 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 97/1000\n",
      "1166/1166 [==============================] - 0s 57us/step - loss: 0.1716 - rmse: 0.1716 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 98/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1886 - rmse: 0.1886 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 99/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1860 - rmse: 0.1860 - val_loss: 0.1478 - val_rmse: 0.1478\n",
      "Epoch 100/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1741 - rmse: 0.1741 - val_loss: 0.1501 - val_rmse: 0.1501\n",
      "Epoch 101/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1747 - rmse: 0.1747 - val_loss: 0.1518 - val_rmse: 0.1518\n",
      "Epoch 102/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1826 - rmse: 0.1826 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 103/1000\n",
      "1166/1166 [==============================] - 0s 57us/step - loss: 0.1812 - rmse: 0.1812 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 104/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1605 - val_rmse: 0.1605\n",
      "Epoch 105/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1728 - rmse: 0.1728 - val_loss: 0.1503 - val_rmse: 0.1503\n",
      "Epoch 106/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1709 - rmse: 0.1709 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 107/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1883 - rmse: 0.1883 - val_loss: 0.1518 - val_rmse: 0.1518\n",
      "Epoch 108/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1493 - val_rmse: 0.1493\n",
      "Epoch 109/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.2014 - rmse: 0.2014 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 110/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.1676 - rmse: 0.16 - 0s 44us/step - loss: 0.1852 - rmse: 0.1852 - val_loss: 0.1473 - val_rmse: 0.1473\n",
      "Epoch 111/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1890 - rmse: 0.1890 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 112/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1869 - rmse: 0.1869 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 113/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1737 - rmse: 0.1737 - val_loss: 0.1515 - val_rmse: 0.1515\n",
      "Epoch 114/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1784 - rmse: 0.1784 - val_loss: 0.1504 - val_rmse: 0.1504\n",
      "Epoch 115/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1854 - rmse: 0.1854 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 116/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1510 - val_rmse: 0.1510\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 118/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 119/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1888 - rmse: 0.1888 - val_loss: 0.1456 - val_rmse: 0.1456\n",
      "Epoch 120/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1696 - rmse: 0.1696 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 121/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1847 - rmse: 0.1847 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 122/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1740 - rmse: 0.1740 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 123/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1817 - rmse: 0.1817 - val_loss: 0.1501 - val_rmse: 0.1501\n",
      "Epoch 124/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1739 - rmse: 0.1739 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 125/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1893 - rmse: 0.1893 - val_loss: 0.1497 - val_rmse: 0.1497\n",
      "Epoch 126/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1898 - rmse: 0.1898 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 127/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 128/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1529 - val_rmse: 0.1529\n",
      "Epoch 129/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1781 - rmse: 0.1781 - val_loss: 0.1508 - val_rmse: 0.1508\n",
      "Epoch 130/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1797 - rmse: 0.1797 - val_loss: 0.1585 - val_rmse: 0.1585\n",
      "Epoch 131/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 132/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1470 - val_rmse: 0.1470\n",
      "Epoch 133/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1826 - rmse: 0.1826 - val_loss: 0.1512 - val_rmse: 0.1512\n",
      "Epoch 134/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1848 - rmse: 0.1848 - val_loss: 0.1481 - val_rmse: 0.1481\n",
      "Epoch 135/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1485 - val_rmse: 0.1485\n",
      "Epoch 136/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1846 - rmse: 0.1846 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 137/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 138/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1833 - rmse: 0.1833 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 139/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 140/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1466 - val_rmse: 0.1466\n",
      "Epoch 141/1000\n",
      "1166/1166 [==============================] - 0s 21us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 142/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1834 - rmse: 0.1834 - val_loss: 0.1587 - val_rmse: 0.1587\n",
      "Epoch 143/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1713 - rmse: 0.1713 - val_loss: 0.1478 - val_rmse: 0.1478\n",
      "Epoch 144/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1833 - rmse: 0.1833 - val_loss: 0.1502 - val_rmse: 0.1502\n",
      "Epoch 145/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1853 - rmse: 0.1853 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 146/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1875 - rmse: 0.1875 - val_loss: 0.1519 - val_rmse: 0.1519\n",
      "Epoch 147/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1806 - rmse: 0.1806 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 148/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1849 - rmse: 0.1849 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 149/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1800 - rmse: 0.1800 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 150/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1832 - rmse: 0.1832 - val_loss: 0.1499 - val_rmse: 0.1499\n",
      "Epoch 151/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1879 - rmse: 0.1879 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 152/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1675 - rmse: 0.1675 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 153/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 154/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1758 - rmse: 0.1758 - val_loss: 0.1490 - val_rmse: 0.1490\n",
      "Epoch 155/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1861 - rmse: 0.1861 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 156/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1873 - rmse: 0.1873 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 157/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1828 - rmse: 0.1828 - val_loss: 0.1477 - val_rmse: 0.1477\n",
      "Epoch 158/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1739 - rmse: 0.1739 - val_loss: 0.1495 - val_rmse: 0.1495\n",
      "Epoch 159/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1844 - rmse: 0.1844 - val_loss: 0.1499 - val_rmse: 0.1499\n",
      "Epoch 160/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1750 - rmse: 0.1750 - val_loss: 0.1487 - val_rmse: 0.1487\n",
      "Epoch 161/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1446 - val_rmse: 0.1446\n",
      "Epoch 162/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1504 - val_rmse: 0.1504\n",
      "Epoch 163/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1818 - rmse: 0.1818 - val_loss: 0.1450 - val_rmse: 0.1450\n",
      "Epoch 164/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1729 - rmse: 0.1729 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 165/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1817 - rmse: 0.1817 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 166/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1503 - val_rmse: 0.1503\n",
      "Epoch 167/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 168/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1864 - rmse: 0.1864 - val_loss: 0.1488 - val_rmse: 0.1488\n",
      "Epoch 169/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1846 - rmse: 0.1846 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 170/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1698 - rmse: 0.1698 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 171/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1891 - rmse: 0.1891 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 172/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1869 - rmse: 0.1869 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 173/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1769 - rmse: 0.1769 - val_loss: 0.1456 - val_rmse: 0.1456\n",
      "Epoch 174/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1657 - rmse: 0.1657 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1658 - rmse: 0.1658 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 176/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1844 - rmse: 0.1844 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 177/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1943 - rmse: 0.1943 - val_loss: 0.1595 - val_rmse: 0.1595\n",
      "Epoch 178/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 179/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1828 - rmse: 0.1828 - val_loss: 0.1493 - val_rmse: 0.1493\n",
      "Epoch 180/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1834 - rmse: 0.1834 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 181/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1594 - val_rmse: 0.1594\n",
      "Epoch 182/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1486 - val_rmse: 0.1486\n",
      "Epoch 183/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1766 - rmse: 0.1766 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 184/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1822 - rmse: 0.1822 - val_loss: 0.1572 - val_rmse: 0.1572\n",
      "Epoch 185/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1506 - val_rmse: 0.1506\n",
      "Epoch 186/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1932 - rmse: 0.1932 - val_loss: 0.1467 - val_rmse: 0.1467\n",
      "Epoch 187/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1828 - rmse: 0.1828 - val_loss: 0.1509 - val_rmse: 0.1509\n",
      "Epoch 188/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1844 - rmse: 0.1844 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 189/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1858 - rmse: 0.1858 - val_loss: 0.1504 - val_rmse: 0.1504\n",
      "Epoch 190/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 191/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1733 - rmse: 0.1733 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 192/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1822 - rmse: 0.1822 - val_loss: 0.1513 - val_rmse: 0.1513\n",
      "Epoch 193/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 194/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1728 - rmse: 0.1728 - val_loss: 0.1491 - val_rmse: 0.1491\n",
      "Epoch 195/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 196/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 197/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 198/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1897 - rmse: 0.1897 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 199/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1525 - val_rmse: 0.1525\n",
      "Epoch 200/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1492 - val_rmse: 0.1492\n",
      "Epoch 201/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 202/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1595 - val_rmse: 0.1595\n",
      "Epoch 203/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1975 - rmse: 0.1975 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 204/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1756 - rmse: 0.1756 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 205/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1819 - rmse: 0.1819 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 206/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 207/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1743 - rmse: 0.1743 - val_loss: 0.1498 - val_rmse: 0.1498\n",
      "Epoch 208/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1701 - rmse: 0.1701 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 209/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1487 - val_rmse: 0.1487\n",
      "Epoch 210/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1872 - rmse: 0.1872 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 211/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1728 - rmse: 0.1728 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 212/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1744 - rmse: 0.1744 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 213/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1464 - val_rmse: 0.1464\n",
      "Epoch 214/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1718 - rmse: 0.1718 - val_loss: 0.1501 - val_rmse: 0.1501\n",
      "Epoch 215/1000\n",
      "1166/1166 [==============================] - 0s 22us/step - loss: 0.1850 - rmse: 0.1850 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 216/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1730 - rmse: 0.1730 - val_loss: 0.1513 - val_rmse: 0.1513\n",
      "Epoch 217/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1434 - val_rmse: 0.1434\n",
      "Epoch 218/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1755 - rmse: 0.1755 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 219/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1562 - val_rmse: 0.1562\n",
      "Epoch 220/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1762 - rmse: 0.1762 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 221/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1687 - rmse: 0.1687 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 222/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1784 - rmse: 0.1784 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 223/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1901 - rmse: 0.1901 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 224/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1777 - rmse: 0.1777 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 225/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1500 - val_rmse: 0.1500\n",
      "Epoch 226/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1851 - rmse: 0.1851 - val_loss: 0.1498 - val_rmse: 0.1498\n",
      "Epoch 227/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1891 - rmse: 0.1891 - val_loss: 0.1471 - val_rmse: 0.1471\n",
      "Epoch 228/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1860 - rmse: 0.1860 - val_loss: 0.1500 - val_rmse: 0.1500\n",
      "Epoch 229/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1726 - rmse: 0.1726 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 230/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1523 - val_rmse: 0.1523\n",
      "Epoch 231/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1894 - rmse: 0.1894 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 232/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1608 - val_rmse: 0.1608\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 61us/step - loss: 0.1744 - rmse: 0.1744 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 234/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1782 - rmse: 0.1782 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 235/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1785 - rmse: 0.1785 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 236/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1756 - rmse: 0.1756 - val_loss: 0.1519 - val_rmse: 0.1519\n",
      "Epoch 237/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 238/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1475 - val_rmse: 0.1475\n",
      "Epoch 239/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1782 - rmse: 0.1782 - val_loss: 0.1572 - val_rmse: 0.1572\n",
      "Epoch 240/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1792 - rmse: 0.1792 - val_loss: 0.1562 - val_rmse: 0.1562\n",
      "Epoch 241/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1740 - rmse: 0.1740 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 242/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1462 - val_rmse: 0.1462\n",
      "Epoch 243/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1608 - val_rmse: 0.1608\n",
      "Epoch 244/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1830 - rmse: 0.1830 - val_loss: 0.1483 - val_rmse: 0.1483\n",
      "Epoch 245/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1869 - rmse: 0.1869 - val_loss: 0.1503 - val_rmse: 0.1503\n",
      "Epoch 246/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1820 - rmse: 0.1820 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 247/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1672 - rmse: 0.1672 - val_loss: 0.1480 - val_rmse: 0.1480\n",
      "Epoch 248/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1799 - rmse: 0.1799 - val_loss: 0.1509 - val_rmse: 0.1509\n",
      "Epoch 249/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1794 - rmse: 0.1794 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 250/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1827 - rmse: 0.1827 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 251/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1790 - rmse: 0.1790 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 252/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1843 - rmse: 0.1843 - val_loss: 0.1504 - val_rmse: 0.1504\n",
      "Epoch 253/1000\n",
      "1166/1166 [==============================] - 0s 83us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1512 - val_rmse: 0.1512\n",
      "Epoch 254/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1862 - rmse: 0.1862 - val_loss: 0.1518 - val_rmse: 0.1518\n",
      "Epoch 255/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1749 - rmse: 0.1749 - val_loss: 0.1486 - val_rmse: 0.1486\n",
      "Epoch 256/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 257/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1835 - rmse: 0.1835 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 258/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1731 - rmse: 0.1731 - val_loss: 0.1594 - val_rmse: 0.1594\n",
      "Epoch 259/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1718 - rmse: 0.1718 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 260/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1723 - rmse: 0.1723 - val_loss: 0.1606 - val_rmse: 0.1606\n",
      "Epoch 261/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1476 - val_rmse: 0.1476\n",
      "Epoch 262/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1863 - rmse: 0.1863 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 263/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1919 - rmse: 0.1919 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 264/1000\n",
      "1166/1166 [==============================] - 0s 55us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 265/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1845 - rmse: 0.1845 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 266/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 267/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1725 - rmse: 0.1725 - val_loss: 0.1587 - val_rmse: 0.1587\n",
      "Epoch 268/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 269/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1768 - rmse: 0.1768 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 270/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1771 - rmse: 0.1771 - val_loss: 0.1508 - val_rmse: 0.1508\n",
      "Epoch 271/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 272/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1883 - rmse: 0.1883 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 273/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1845 - rmse: 0.1845 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 274/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1834 - rmse: 0.1834 - val_loss: 0.1562 - val_rmse: 0.1562\n",
      "Epoch 275/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1740 - rmse: 0.1740 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 276/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1764 - rmse: 0.1764 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 277/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 278/1000\n",
      "1166/1166 [==============================] - 0s 55us/step - loss: 0.1800 - rmse: 0.1800 - val_loss: 0.1508 - val_rmse: 0.1508\n",
      "Epoch 279/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1833 - rmse: 0.1833 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 280/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 281/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1875 - rmse: 0.1875 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 282/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 283/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1853 - rmse: 0.1853 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 284/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 285/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1861 - rmse: 0.1861 - val_loss: 0.1495 - val_rmse: 0.1495\n",
      "Epoch 286/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1885 - rmse: 0.1885 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 287/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1899 - rmse: 0.1899 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 288/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1870 - rmse: 0.1870 - val_loss: 0.1578 - val_rmse: 0.1578\n",
      "Epoch 289/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1864 - rmse: 0.1864 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 290/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1741 - rmse: 0.1741 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1825 - rmse: 0.1825 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 292/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1578 - val_rmse: 0.1578\n",
      "Epoch 293/1000\n",
      "1166/1166 [==============================] - 0s 58us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 294/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1473 - val_rmse: 0.1473\n",
      "Epoch 295/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 296/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1794 - rmse: 0.1794 - val_loss: 0.1480 - val_rmse: 0.1480\n",
      "Epoch 297/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1857 - rmse: 0.1857 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 298/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1758 - rmse: 0.1758 - val_loss: 0.1525 - val_rmse: 0.1525\n",
      "Epoch 299/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1820 - rmse: 0.1820 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 300/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1846 - rmse: 0.1846 - val_loss: 0.1498 - val_rmse: 0.1498\n",
      "Epoch 301/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1698 - rmse: 0.1698 - val_loss: 0.1481 - val_rmse: 0.1481\n",
      "Epoch 302/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1838 - rmse: 0.1838 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 303/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1835 - rmse: 0.1835 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 304/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 305/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 306/1000\n",
      "1166/1166 [==============================] - 0s 60us/step - loss: 0.1726 - rmse: 0.1726 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 307/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1779 - rmse: 0.1779 - val_loss: 0.1493 - val_rmse: 0.1493\n",
      "Epoch 308/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 309/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1792 - rmse: 0.1792 - val_loss: 0.1595 - val_rmse: 0.1595\n",
      "Epoch 310/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1702 - rmse: 0.1702 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 311/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1754 - rmse: 0.1754 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 312/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1857 - rmse: 0.1857 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 313/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 314/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1748 - rmse: 0.1748 - val_loss: 0.1569 - val_rmse: 0.1569\n",
      "Epoch 315/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1757 - rmse: 0.1757 - val_loss: 0.1531 - val_rmse: 0.1531\n",
      "Epoch 316/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1820 - rmse: 0.1820 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 317/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1709 - rmse: 0.1709 - val_loss: 0.1525 - val_rmse: 0.1525\n",
      "Epoch 318/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1826 - rmse: 0.1826 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 319/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 320/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1510 - val_rmse: 0.1510\n",
      "Epoch 321/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1825 - rmse: 0.1825 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 322/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 323/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1712 - rmse: 0.1712 - val_loss: 0.1531 - val_rmse: 0.1531\n",
      "Epoch 324/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1830 - rmse: 0.1830 - val_loss: 0.1594 - val_rmse: 0.1594\n",
      "Epoch 325/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 326/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 327/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1813 - rmse: 0.1813 - val_loss: 0.1604 - val_rmse: 0.1604\n",
      "Epoch 328/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1497 - val_rmse: 0.1497\n",
      "Epoch 329/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1863 - rmse: 0.1863 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 330/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1847 - rmse: 0.1847 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 331/1000\n",
      "1166/1166 [==============================] - 0s 60us/step - loss: 0.1812 - rmse: 0.1812 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 332/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 333/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1732 - rmse: 0.1732 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 334/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1531 - val_rmse: 0.1531\n",
      "Epoch 335/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1876 - rmse: 0.1876 - val_loss: 0.1529 - val_rmse: 0.1529\n",
      "Epoch 336/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 337/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1827 - rmse: 0.1827 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 338/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 339/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1929 - rmse: 0.1929 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 340/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 341/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 342/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1731 - rmse: 0.1731 - val_loss: 0.1470 - val_rmse: 0.1470\n",
      "Epoch 343/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1761 - rmse: 0.1761 - val_loss: 0.1587 - val_rmse: 0.1587\n",
      "Epoch 344/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1819 - rmse: 0.1819 - val_loss: 0.1477 - val_rmse: 0.1477\n",
      "Epoch 345/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1741 - rmse: 0.1741 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 346/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1794 - rmse: 0.1794 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 347/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1837 - rmse: 0.1837 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 348/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1572 - val_rmse: 0.1572\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1781 - rmse: 0.1781 - val_loss: 0.1579 - val_rmse: 0.1579\n",
      "Epoch 350/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 351/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 352/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1696 - rmse: 0.1696 - val_loss: 0.1492 - val_rmse: 0.1492\n",
      "Epoch 353/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1824 - rmse: 0.1824 - val_loss: 0.1614 - val_rmse: 0.1614\n",
      "Epoch 354/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1817 - rmse: 0.1817 - val_loss: 0.1506 - val_rmse: 0.1506\n",
      "Epoch 355/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 356/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1718 - rmse: 0.1718 - val_loss: 0.1500 - val_rmse: 0.1500\n",
      "Epoch 357/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1793 - rmse: 0.1793 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 358/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1843 - rmse: 0.1843 - val_loss: 0.1610 - val_rmse: 0.1610\n",
      "Epoch 359/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 360/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1509 - val_rmse: 0.1509\n",
      "Epoch 361/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1876 - rmse: 0.1876 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 362/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1861 - rmse: 0.1861 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 363/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1731 - rmse: 0.1731 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 364/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1864 - rmse: 0.1864 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 365/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1764 - rmse: 0.1764 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 366/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1784 - rmse: 0.1784 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 367/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 368/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1855 - rmse: 0.1855 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 369/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1858 - rmse: 0.1858 - val_loss: 0.1510 - val_rmse: 0.1510\n",
      "Epoch 370/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1730 - rmse: 0.1730 - val_loss: 0.1574 - val_rmse: 0.1574\n",
      "Epoch 371/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 372/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1744 - rmse: 0.1744 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 373/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 374/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1768 - rmse: 0.1768 - val_loss: 0.1606 - val_rmse: 0.1606\n",
      "Epoch 375/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1781 - rmse: 0.1781 - val_loss: 0.1617 - val_rmse: 0.1617\n",
      "Epoch 376/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1763 - rmse: 0.1763 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 377/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 378/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1685 - rmse: 0.1685 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 379/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 380/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 381/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1875 - rmse: 0.1875 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 382/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1808 - rmse: 0.1808 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 383/1000\n",
      "1166/1166 [==============================] - 0s 22us/step - loss: 0.1739 - rmse: 0.1739 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 384/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1762 - rmse: 0.1762 - val_loss: 0.1513 - val_rmse: 0.1513\n",
      "Epoch 385/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1764 - rmse: 0.1764 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 386/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1697 - rmse: 0.1697 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 387/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1812 - rmse: 0.1812 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 388/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1879 - rmse: 0.1879 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 389/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 390/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1758 - rmse: 0.1758 - val_loss: 0.1569 - val_rmse: 0.1569\n",
      "Epoch 391/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 392/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1813 - rmse: 0.1813 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 393/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 394/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 395/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1814 - rmse: 0.1814 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 396/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1766 - rmse: 0.1766 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 397/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1867 - rmse: 0.1867 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 398/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 399/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 400/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1820 - rmse: 0.1820 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 401/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1698 - rmse: 0.1698 - val_loss: 0.1502 - val_rmse: 0.1502\n",
      "Epoch 402/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 403/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 404/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1861 - rmse: 0.1861 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 405/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1865 - rmse: 0.1865 - val_loss: 0.1482 - val_rmse: 0.1482\n",
      "Epoch 406/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1876 - rmse: 0.1876 - val_loss: 0.1463 - val_rmse: 0.1463\n",
      "Epoch 408/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 409/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1529 - val_rmse: 0.1529\n",
      "Epoch 410/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 411/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1635 - rmse: 0.1635 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 412/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1739 - rmse: 0.1739 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 413/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1752 - rmse: 0.1752 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 414/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1778 - rmse: 0.1778 - val_loss: 0.1592 - val_rmse: 0.1592\n",
      "Epoch 415/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1748 - rmse: 0.1748 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 416/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1757 - rmse: 0.1757 - val_loss: 0.1578 - val_rmse: 0.1578\n",
      "Epoch 417/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1839 - rmse: 0.1839 - val_loss: 0.1481 - val_rmse: 0.1481\n",
      "Epoch 418/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1725 - rmse: 0.1725 - val_loss: 0.1515 - val_rmse: 0.1515\n",
      "Epoch 419/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1740 - rmse: 0.1740 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 420/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1722 - rmse: 0.1722 - val_loss: 0.1501 - val_rmse: 0.1501\n",
      "Epoch 421/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1757 - rmse: 0.1757 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 422/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1868 - rmse: 0.1868 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 423/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1519 - val_rmse: 0.1519\n",
      "Epoch 424/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1800 - rmse: 0.1800 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 425/1000\n",
      "1166/1166 [==============================] - 0s 55us/step - loss: 0.1669 - rmse: 0.1669 - val_loss: 0.1523 - val_rmse: 0.1523\n",
      "Epoch 426/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1755 - rmse: 0.1755 - val_loss: 0.1585 - val_rmse: 0.1585\n",
      "Epoch 427/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1724 - rmse: 0.1724 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 428/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1832 - rmse: 0.1832 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 429/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1733 - rmse: 0.1733 - val_loss: 0.1460 - val_rmse: 0.1460\n",
      "Epoch 430/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1827 - rmse: 0.1827 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 431/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1797 - rmse: 0.1797 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 432/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 433/1000\n",
      "1166/1166 [==============================] - 0s 73us/step - loss: 0.1822 - rmse: 0.1822 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 434/1000\n",
      "1166/1166 [==============================] - 0s 82us/step - loss: 0.1804 - rmse: 0.1804 - val_loss: 0.1578 - val_rmse: 0.1578\n",
      "Epoch 435/1000\n",
      "1166/1166 [==============================] - 0s 85us/step - loss: 0.1735 - rmse: 0.1735 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 436/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1804 - rmse: 0.1804 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 437/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1589 - val_rmse: 0.1589\n",
      "Epoch 438/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 439/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1700 - rmse: 0.1700 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 440/1000\n",
      "1166/1166 [==============================] - 0s 58us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1494 - val_rmse: 0.1494\n",
      "Epoch 441/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1844 - rmse: 0.1844 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 442/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1833 - rmse: 0.1833 - val_loss: 0.1572 - val_rmse: 0.1572\n",
      "Epoch 443/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1763 - rmse: 0.1763 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 444/1000\n",
      "1166/1166 [==============================] - 0s 64us/step - loss: 0.1828 - rmse: 0.1828 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 445/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1850 - rmse: 0.1850 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 446/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1766 - rmse: 0.1766 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 447/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1806 - rmse: 0.1806 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 448/1000\n",
      "1166/1166 [==============================] - 0s 62us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 449/1000\n",
      "1166/1166 [==============================] - 0s 57us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1590 - val_rmse: 0.1590\n",
      "Epoch 450/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 451/1000\n",
      "1166/1166 [==============================] - 0s 58us/step - loss: 0.1680 - rmse: 0.1680 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 452/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1779 - rmse: 0.1779 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 453/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1848 - rmse: 0.1848 - val_loss: 0.1529 - val_rmse: 0.1529\n",
      "Epoch 454/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1792 - rmse: 0.1792 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 455/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1827 - rmse: 0.1827 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 456/1000\n",
      "1166/1166 [==============================] - 0s 62us/step - loss: 0.1755 - rmse: 0.1755 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 457/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1756 - rmse: 0.1756 - val_loss: 0.1488 - val_rmse: 0.1488\n",
      "Epoch 458/1000\n",
      "1166/1166 [==============================] - 0s 55us/step - loss: 0.1867 - rmse: 0.1867 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 459/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 460/1000\n",
      "1166/1166 [==============================] - 0s 63us/step - loss: 0.1761 - rmse: 0.1761 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 461/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1675 - rmse: 0.1675 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 462/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1796 - rmse: 0.1796 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 463/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1703 - rmse: 0.1703 - val_loss: 0.1569 - val_rmse: 0.1569\n",
      "Epoch 464/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1739 - rmse: 0.1739 - val_loss: 0.1502 - val_rmse: 0.1502\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 72us/step - loss: 0.1827 - rmse: 0.1827 - val_loss: 0.1465 - val_rmse: 0.1465\n",
      "Epoch 466/1000\n",
      "1166/1166 [==============================] - 0s 63us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 467/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1898 - rmse: 0.1898 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 468/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 469/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1780 - rmse: 0.1780 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 470/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1565 - val_rmse: 0.1565\n",
      "Epoch 471/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1752 - rmse: 0.1752 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 472/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1903 - rmse: 0.1903 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 473/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1922 - rmse: 0.1922 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 474/1000\n",
      "1166/1166 [==============================] - 0s 57us/step - loss: 0.1891 - rmse: 0.1891 - val_loss: 0.1619 - val_rmse: 0.1619\n",
      "Epoch 475/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.1668 - rmse: 0.16 - 0s 43us/step - loss: 0.1665 - rmse: 0.1665 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 476/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1887 - rmse: 0.1887 - val_loss: 0.1600 - val_rmse: 0.1600\n",
      "Epoch 477/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 478/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1710 - rmse: 0.1710 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 479/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1819 - rmse: 0.1819 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 480/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1729 - rmse: 0.1729 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 481/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1758 - rmse: 0.1758 - val_loss: 0.1497 - val_rmse: 0.1497\n",
      "Epoch 482/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 483/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 484/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 485/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1878 - rmse: 0.1878 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 486/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1706 - rmse: 0.1706 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 487/1000\n",
      "1166/1166 [==============================] - 0s 57us/step - loss: 0.1827 - rmse: 0.1827 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 488/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1896 - rmse: 0.1896 - val_loss: 0.1579 - val_rmse: 0.1579\n",
      "Epoch 489/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 490/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 491/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1741 - rmse: 0.1741 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 492/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1892 - rmse: 0.1892 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 493/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1848 - rmse: 0.1848 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 494/1000\n",
      "1166/1166 [==============================] - 0s 66us/step - loss: 0.1874 - rmse: 0.1874 - val_loss: 0.1578 - val_rmse: 0.1578\n",
      "Epoch 495/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1822 - rmse: 0.1822 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 496/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1523 - val_rmse: 0.1523\n",
      "Epoch 497/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1750 - rmse: 0.1750 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 498/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1592 - val_rmse: 0.1592\n",
      "Epoch 499/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1877 - rmse: 0.1877 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 500/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1750 - rmse: 0.1750 - val_loss: 0.1610 - val_rmse: 0.1610\n",
      "Epoch 501/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1824 - rmse: 0.1824 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 502/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1755 - rmse: 0.1755 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 503/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1804 - rmse: 0.1804 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 504/1000\n",
      "1166/1166 [==============================] - 0s 55us/step - loss: 0.1690 - rmse: 0.1690 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 505/1000\n",
      "1166/1166 [==============================] - 0s 58us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 506/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1844 - rmse: 0.1844 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 507/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 508/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 509/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1814 - rmse: 0.1814 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 510/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1714 - rmse: 0.1714 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 511/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 512/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1588 - val_rmse: 0.1588\n",
      "Epoch 513/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1753 - rmse: 0.1753 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 514/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1706 - rmse: 0.1706 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 515/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1750 - rmse: 0.1750 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 516/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1855 - rmse: 0.1855 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 517/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 518/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1862 - rmse: 0.1862 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 519/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1588 - val_rmse: 0.1588\n",
      "Epoch 520/1000\n",
      "1166/1166 [==============================] - 0s 61us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 521/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1756 - rmse: 0.1756 - val_loss: 0.1596 - val_rmse: 0.1596\n",
      "Epoch 522/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1757 - rmse: 0.1757 - val_loss: 0.1562 - val_rmse: 0.1562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 524/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 525/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1852 - rmse: 0.1852 - val_loss: 0.1517 - val_rmse: 0.1517\n",
      "Epoch 526/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 527/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1823 - rmse: 0.1823 - val_loss: 0.1494 - val_rmse: 0.1494\n",
      "Epoch 528/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1726 - rmse: 0.1726 - val_loss: 0.1519 - val_rmse: 0.1519\n",
      "Epoch 529/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1771 - rmse: 0.1771 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 530/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1859 - rmse: 0.1859 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 531/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1778 - rmse: 0.1778 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 532/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1667 - rmse: 0.1667 - val_loss: 0.1499 - val_rmse: 0.1499\n",
      "Epoch 533/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1702 - rmse: 0.1702 - val_loss: 0.1585 - val_rmse: 0.1585\n",
      "Epoch 534/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 535/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1877 - rmse: 0.1877 - val_loss: 0.1590 - val_rmse: 0.1590\n",
      "Epoch 536/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 537/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1790 - rmse: 0.1790 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 538/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1647 - rmse: 0.1647 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 539/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 540/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1731 - rmse: 0.1731 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 541/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 542/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1739 - rmse: 0.1739 - val_loss: 0.1592 - val_rmse: 0.1592\n",
      "Epoch 543/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1763 - rmse: 0.1763 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 544/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1785 - rmse: 0.1785 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 545/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 546/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1752 - rmse: 0.1752 - val_loss: 0.1572 - val_rmse: 0.1572\n",
      "Epoch 547/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1851 - rmse: 0.1851 - val_loss: 0.1601 - val_rmse: 0.1601\n",
      "Epoch 548/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1761 - rmse: 0.1761 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 549/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1812 - rmse: 0.1812 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 550/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1843 - rmse: 0.1843 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 551/1000\n",
      "1166/1166 [==============================] - 0s 55us/step - loss: 0.1838 - rmse: 0.1838 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 552/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1763 - rmse: 0.1763 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 553/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1833 - rmse: 0.1833 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 554/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1702 - rmse: 0.1702 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 555/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1850 - rmse: 0.1850 - val_loss: 0.1597 - val_rmse: 0.1597\n",
      "Epoch 556/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1819 - rmse: 0.1819 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 557/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1742 - rmse: 0.1742 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 558/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1574 - val_rmse: 0.1574\n",
      "Epoch 559/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 560/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1757 - rmse: 0.1757 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 561/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1724 - rmse: 0.1724 - val_loss: 0.1515 - val_rmse: 0.1515\n",
      "Epoch 562/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1807 - rmse: 0.1807 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 563/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1711 - rmse: 0.1711 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 564/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1843 - rmse: 0.1843 - val_loss: 0.1591 - val_rmse: 0.1591\n",
      "Epoch 565/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 566/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1745 - rmse: 0.1745 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 567/1000\n",
      "1166/1166 [==============================] - 0s 104us/step - loss: 0.1806 - rmse: 0.1806 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 568/1000\n",
      "1166/1166 [==============================] - 0s 105us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 569/1000\n",
      "1166/1166 [==============================] - 0s 65us/step - loss: 0.1670 - rmse: 0.1670 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 570/1000\n",
      "1166/1166 [==============================] - 0s 115us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 571/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1703 - rmse: 0.1703 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 572/1000\n",
      "1166/1166 [==============================] - 0s 54us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1478 - val_rmse: 0.1478\n",
      "Epoch 573/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1882 - rmse: 0.1882 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 574/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1735 - rmse: 0.1735 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 575/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 576/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1737 - rmse: 0.1737 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 577/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 578/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 579/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 580/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1697 - rmse: 0.1697 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 581/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 582/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1712 - rmse: 0.1712 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 583/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1595 - val_rmse: 0.1595\n",
      "Epoch 584/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1476 - val_rmse: 0.1476\n",
      "Epoch 585/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1778 - rmse: 0.1778 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 586/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1796 - rmse: 0.1796 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 587/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1715 - rmse: 0.1715 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 588/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1680 - rmse: 0.1680 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 589/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 590/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1835 - rmse: 0.1835 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 591/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1768 - rmse: 0.1768 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 592/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1764 - rmse: 0.1764 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 593/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 594/1000\n",
      "1166/1166 [==============================] - 0s 171us/step - loss: 0.1739 - rmse: 0.1739 - val_loss: 0.1512 - val_rmse: 0.1512\n",
      "Epoch 595/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1854 - rmse: 0.1854 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 596/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1867 - rmse: 0.1867 - val_loss: 0.1591 - val_rmse: 0.1591\n",
      "Epoch 597/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 598/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 599/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1828 - rmse: 0.1828 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 600/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1747 - rmse: 0.1747 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 601/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1679 - rmse: 0.1679 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 602/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1743 - rmse: 0.1743 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 603/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 604/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1887 - rmse: 0.1887 - val_loss: 0.1574 - val_rmse: 0.1574\n",
      "Epoch 605/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1581 - val_rmse: 0.1581\n",
      "Epoch 606/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1747 - rmse: 0.1747 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 607/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 608/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1756 - rmse: 0.1756 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 609/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1610 - rmse: 0.1610 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 610/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.1745 - rmse: 0.1745 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 611/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1752 - rmse: 0.1752 - val_loss: 0.1477 - val_rmse: 0.1477\n",
      "Epoch 612/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1508 - val_rmse: 0.1508\n",
      "Epoch 613/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 614/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1721 - rmse: 0.1721 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 615/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1838 - rmse: 0.1838 - val_loss: 0.1510 - val_rmse: 0.1510\n",
      "Epoch 616/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 617/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 618/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 619/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 620/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1766 - rmse: 0.1766 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 621/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1523 - val_rmse: 0.1523\n",
      "Epoch 622/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1745 - rmse: 0.1745 - val_loss: 0.1593 - val_rmse: 0.1593\n",
      "Epoch 623/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1904 - rmse: 0.1904 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 624/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1823 - rmse: 0.1823 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 625/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1755 - rmse: 0.1755 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 626/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1771 - rmse: 0.1771 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 627/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1588 - val_rmse: 0.1588\n",
      "Epoch 628/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1832 - rmse: 0.1832 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 629/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1499 - val_rmse: 0.1499\n",
      "Epoch 630/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1598 - val_rmse: 0.1598\n",
      "Epoch 631/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1702 - rmse: 0.1702 - val_loss: 0.1562 - val_rmse: 0.1562\n",
      "Epoch 632/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1852 - rmse: 0.1852 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 633/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1780 - rmse: 0.1780 - val_loss: 0.1594 - val_rmse: 0.1594\n",
      "Epoch 634/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 635/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1758 - rmse: 0.1758 - val_loss: 0.1562 - val_rmse: 0.1562\n",
      "Epoch 636/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1733 - rmse: 0.1733 - val_loss: 0.1565 - val_rmse: 0.1565\n",
      "Epoch 637/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1800 - rmse: 0.1800 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 638/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1690 - rmse: 0.1690 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1841 - rmse: 0.1841 - val_loss: 0.1613 - val_rmse: 0.1613\n",
      "Epoch 640/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1804 - rmse: 0.1804 - val_loss: 0.1597 - val_rmse: 0.1597\n",
      "Epoch 641/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1708 - rmse: 0.1708 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 642/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1685 - rmse: 0.1685 - val_loss: 0.1519 - val_rmse: 0.1519\n",
      "Epoch 643/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1692 - rmse: 0.1692 - val_loss: 0.1484 - val_rmse: 0.1484\n",
      "Epoch 644/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1658 - rmse: 0.1658 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 645/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1766 - rmse: 0.1766 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 646/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1879 - rmse: 0.1879 - val_loss: 0.1603 - val_rmse: 0.1603\n",
      "Epoch 647/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1690 - rmse: 0.1690 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 648/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 649/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 650/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1819 - rmse: 0.1819 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 651/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1766 - rmse: 0.1766 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 652/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1824 - rmse: 0.1824 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 653/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1759 - rmse: 0.1759 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 654/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1483 - val_rmse: 0.1483\n",
      "Epoch 655/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1779 - rmse: 0.1779 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 656/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1839 - rmse: 0.1839 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 657/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 658/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1863 - rmse: 0.1863 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 659/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1732 - rmse: 0.1732 - val_loss: 0.1590 - val_rmse: 0.1590\n",
      "Epoch 660/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 661/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1723 - rmse: 0.1723 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 662/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1745 - rmse: 0.1745 - val_loss: 0.1576 - val_rmse: 0.1576\n",
      "Epoch 663/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1825 - rmse: 0.1825 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 664/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 665/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1602 - val_rmse: 0.1602\n",
      "Epoch 666/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1720 - rmse: 0.1720 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 667/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 668/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1769 - rmse: 0.1769 - val_loss: 0.1471 - val_rmse: 0.1471\n",
      "Epoch 669/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1721 - rmse: 0.1721 - val_loss: 0.1515 - val_rmse: 0.1515\n",
      "Epoch 670/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 671/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1872 - rmse: 0.1872 - val_loss: 0.1610 - val_rmse: 0.1610\n",
      "Epoch 672/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1579 - val_rmse: 0.1579\n",
      "Epoch 673/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1731 - rmse: 0.1731 - val_loss: 0.1502 - val_rmse: 0.1502\n",
      "Epoch 674/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1796 - rmse: 0.1796 - val_loss: 0.1487 - val_rmse: 0.1487\n",
      "Epoch 675/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1731 - rmse: 0.1731 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 676/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 677/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 678/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1734 - rmse: 0.1734 - val_loss: 0.1587 - val_rmse: 0.1587\n",
      "Epoch 679/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1606 - val_rmse: 0.1606\n",
      "Epoch 680/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 681/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 682/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1498 - val_rmse: 0.1498\n",
      "Epoch 683/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1759 - rmse: 0.1759 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 684/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1814 - rmse: 0.1814 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 685/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1740 - rmse: 0.1740 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 686/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1782 - rmse: 0.1782 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 687/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1836 - rmse: 0.1836 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 688/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1826 - rmse: 0.1826 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 689/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1793 - rmse: 0.1793 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 690/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1782 - rmse: 0.1782 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 691/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1747 - rmse: 0.1747 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 692/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 693/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1790 - rmse: 0.1790 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 694/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 695/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1833 - rmse: 0.1833 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 696/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1837 - rmse: 0.1837 - val_loss: 0.1529 - val_rmse: 0.1529\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1768 - rmse: 0.1768 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 698/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 699/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 700/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 701/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 702/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 703/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1780 - rmse: 0.1780 - val_loss: 0.1497 - val_rmse: 0.1497\n",
      "Epoch 704/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1812 - rmse: 0.1812 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 705/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1891 - rmse: 0.1891 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 706/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 707/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1604 - val_rmse: 0.1604\n",
      "Epoch 708/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1650 - rmse: 0.1650 - val_loss: 0.1576 - val_rmse: 0.1576\n",
      "Epoch 709/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1860 - rmse: 0.1860 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 710/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1765 - rmse: 0.1765 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 711/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1698 - rmse: 0.1698 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 712/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 713/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1813 - rmse: 0.1813 - val_loss: 0.1618 - val_rmse: 0.1618\n",
      "Epoch 714/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1518 - val_rmse: 0.1518\n",
      "Epoch 715/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1839 - rmse: 0.1839 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 716/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1762 - rmse: 0.1762 - val_loss: 0.1579 - val_rmse: 0.1579\n",
      "Epoch 717/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1860 - rmse: 0.1860 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 718/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1785 - rmse: 0.1785 - val_loss: 0.1589 - val_rmse: 0.1589\n",
      "Epoch 719/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1792 - rmse: 0.1792 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 720/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1722 - rmse: 0.1722 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 721/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1686 - rmse: 0.1686 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 722/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1837 - rmse: 0.1837 - val_loss: 0.1517 - val_rmse: 0.1517\n",
      "Epoch 723/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1591 - val_rmse: 0.1591\n",
      "Epoch 724/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 725/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 726/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1850 - rmse: 0.1850 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 727/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1857 - rmse: 0.1857 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 728/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1792 - rmse: 0.1792 - val_loss: 0.1594 - val_rmse: 0.1594\n",
      "Epoch 729/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1785 - rmse: 0.1785 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 730/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1782 - rmse: 0.1782 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 731/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 732/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 733/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1630 - rmse: 0.1630 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 734/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1797 - rmse: 0.1797 - val_loss: 0.1595 - val_rmse: 0.1595\n",
      "Epoch 735/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1722 - rmse: 0.1722 - val_loss: 0.1507 - val_rmse: 0.1507\n",
      "Epoch 736/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 737/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1754 - rmse: 0.1754 - val_loss: 0.1582 - val_rmse: 0.1582\n",
      "Epoch 738/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1880 - rmse: 0.1880 - val_loss: 0.1600 - val_rmse: 0.1600\n",
      "Epoch 739/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1737 - rmse: 0.1737 - val_loss: 0.1581 - val_rmse: 0.1581\n",
      "Epoch 740/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 741/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1509 - val_rmse: 0.1509\n",
      "Epoch 742/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 743/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1728 - rmse: 0.1728 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 744/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1569 - val_rmse: 0.1569\n",
      "Epoch 745/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1749 - rmse: 0.1749 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 746/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1762 - rmse: 0.1762 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 747/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1813 - rmse: 0.1813 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 748/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 749/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1707 - rmse: 0.1707 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 750/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1788 - rmse: 0.1788 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 751/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 752/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 753/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1844 - rmse: 0.1844 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 754/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1773 - rmse: 0.1773 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1747 - rmse: 0.1747 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 756/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1715 - rmse: 0.1715 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 757/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1713 - rmse: 0.1713 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 758/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1666 - rmse: 0.1666 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 759/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1484 - val_rmse: 0.1484\n",
      "Epoch 760/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1741 - rmse: 0.1741 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 761/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1900 - rmse: 0.1900 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 762/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1518 - val_rmse: 0.1518\n",
      "Epoch 763/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1836 - rmse: 0.1836 - val_loss: 0.1576 - val_rmse: 0.1576\n",
      "Epoch 764/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1848 - rmse: 0.1848 - val_loss: 0.1590 - val_rmse: 0.1590\n",
      "Epoch 765/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1796 - rmse: 0.1796 - val_loss: 0.1504 - val_rmse: 0.1504\n",
      "Epoch 766/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 767/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 768/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1777 - rmse: 0.1777 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 769/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1856 - rmse: 0.1856 - val_loss: 0.1517 - val_rmse: 0.1517\n",
      "Epoch 770/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1694 - rmse: 0.1694 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 771/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 772/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1778 - rmse: 0.1778 - val_loss: 0.1626 - val_rmse: 0.1626\n",
      "Epoch 773/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1857 - rmse: 0.1857 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 774/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1884 - rmse: 0.1884 - val_loss: 0.1517 - val_rmse: 0.1517\n",
      "Epoch 775/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 776/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1766 - rmse: 0.1766 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 777/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.1737 - rmse: 0.1737 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 778/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1810 - rmse: 0.1810 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 779/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1748 - rmse: 0.1748 - val_loss: 0.1529 - val_rmse: 0.1529\n",
      "Epoch 780/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 781/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1689 - rmse: 0.1689 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 782/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1769 - rmse: 0.1769 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 783/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1864 - rmse: 0.1864 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 784/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1569 - val_rmse: 0.1569\n",
      "Epoch 785/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1747 - rmse: 0.1747 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 786/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 787/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 788/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 789/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1744 - rmse: 0.1744 - val_loss: 0.1590 - val_rmse: 0.1590\n",
      "Epoch 790/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1750 - rmse: 0.1750 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 791/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1848 - rmse: 0.1848 - val_loss: 0.1577 - val_rmse: 0.1577\n",
      "Epoch 792/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1771 - rmse: 0.1771 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 793/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1845 - rmse: 0.1845 - val_loss: 0.1562 - val_rmse: 0.1562\n",
      "Epoch 794/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1912 - rmse: 0.1912 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 795/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1884 - rmse: 0.1884 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 796/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1767 - rmse: 0.1767 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 797/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1815 - rmse: 0.1815 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 798/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1826 - rmse: 0.1826 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 799/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 800/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1878 - rmse: 0.1878 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 801/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1837 - rmse: 0.1837 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 802/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1835 - rmse: 0.1835 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 803/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1713 - rmse: 0.1713 - val_loss: 0.1517 - val_rmse: 0.1517\n",
      "Epoch 804/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1848 - rmse: 0.1848 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 805/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1684 - rmse: 0.1684 - val_loss: 0.1565 - val_rmse: 0.1565\n",
      "Epoch 806/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1834 - rmse: 0.1834 - val_loss: 0.1519 - val_rmse: 0.1519\n",
      "Epoch 807/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1677 - rmse: 0.1677 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 808/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1756 - rmse: 0.1756 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 809/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1819 - rmse: 0.1819 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 810/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1887 - rmse: 0.1887 - val_loss: 0.1588 - val_rmse: 0.1588\n",
      "Epoch 811/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1757 - rmse: 0.1757 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 812/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1892 - rmse: 0.1892 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1801 - rmse: 0.1801 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 814/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 815/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1804 - rmse: 0.1804 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 816/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1882 - rmse: 0.1882 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 817/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1799 - rmse: 0.1799 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 818/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1803 - rmse: 0.1803 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 819/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1738 - rmse: 0.1738 - val_loss: 0.1594 - val_rmse: 0.1594\n",
      "Epoch 820/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 821/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1797 - rmse: 0.1797 - val_loss: 0.1612 - val_rmse: 0.1612\n",
      "Epoch 822/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1523 - val_rmse: 0.1523\n",
      "Epoch 823/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1755 - rmse: 0.1755 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 824/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1800 - rmse: 0.1800 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 825/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1729 - rmse: 0.1729 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 826/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1798 - rmse: 0.1798 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 827/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1752 - rmse: 0.1752 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 828/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 829/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1569 - val_rmse: 0.1569\n",
      "Epoch 830/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1737 - rmse: 0.1737 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 831/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1669 - rmse: 0.1669 - val_loss: 0.1516 - val_rmse: 0.1516\n",
      "Epoch 832/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1715 - rmse: 0.1715 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 833/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1684 - rmse: 0.1684 - val_loss: 0.1595 - val_rmse: 0.1595\n",
      "Epoch 834/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1525 - val_rmse: 0.1525\n",
      "Epoch 835/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1457 - val_rmse: 0.1457\n",
      "Epoch 836/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 837/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1793 - rmse: 0.1793 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 838/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1846 - rmse: 0.1846 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 839/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1804 - rmse: 0.1804 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 840/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1842 - rmse: 0.1842 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 841/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1730 - rmse: 0.1730 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 842/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1728 - rmse: 0.1728 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 843/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1728 - rmse: 0.1728 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 844/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1669 - rmse: 0.1669 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 845/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1710 - rmse: 0.1710 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 846/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1790 - rmse: 0.1790 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 847/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1796 - rmse: 0.1796 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 848/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 849/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 850/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1763 - rmse: 0.1763 - val_loss: 0.1578 - val_rmse: 0.1578\n",
      "Epoch 851/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1720 - rmse: 0.1720 - val_loss: 0.1597 - val_rmse: 0.1597\n",
      "Epoch 852/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1688 - rmse: 0.1688 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 853/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1853 - rmse: 0.1853 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 854/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1807 - rmse: 0.1807 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 855/1000\n",
      "1166/1166 [==============================] - 0s 26us/step - loss: 0.1840 - rmse: 0.1840 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 856/1000\n",
      "1166/1166 [==============================] - 0s 124us/step - loss: 0.1761 - rmse: 0.1761 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 857/1000\n",
      "1166/1166 [==============================] - 0s 97us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1579 - val_rmse: 0.1579\n",
      "Epoch 858/1000\n",
      "1166/1166 [==============================] - 0s 66us/step - loss: 0.1789 - rmse: 0.1789 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 859/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1593 - val_rmse: 0.1593\n",
      "Epoch 860/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1862 - rmse: 0.1862 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 861/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 862/1000\n",
      "1166/1166 [==============================] - 0s 65us/step - loss: 0.1698 - rmse: 0.1698 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 863/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1512 - val_rmse: 0.1512\n",
      "Epoch 864/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1863 - rmse: 0.1863 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 865/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1632 - rmse: 0.1632 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 866/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1709 - rmse: 0.1709 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 867/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1785 - rmse: 0.1785 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 868/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1689 - rmse: 0.1689 - val_loss: 0.1595 - val_rmse: 0.1595\n",
      "Epoch 869/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1793 - rmse: 0.1793 - val_loss: 0.1536 - val_rmse: 0.1536\n",
      "Epoch 870/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1615 - val_rmse: 0.1615\n",
      "Epoch 871/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1932 - rmse: 0.1932 - val_loss: 0.1576 - val_rmse: 0.1576\n",
      "Epoch 872/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1782 - rmse: 0.1782 - val_loss: 0.1554 - val_rmse: 0.1554\n",
      "Epoch 873/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1745 - rmse: 0.1745 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 874/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1708 - rmse: 0.1708 - val_loss: 0.1602 - val_rmse: 0.1602\n",
      "Epoch 875/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1920 - rmse: 0.1920 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 876/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1728 - rmse: 0.1728 - val_loss: 0.1578 - val_rmse: 0.1578\n",
      "Epoch 877/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1729 - rmse: 0.1729 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 878/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1749 - rmse: 0.1749 - val_loss: 0.1517 - val_rmse: 0.1517\n",
      "Epoch 879/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1877 - rmse: 0.1877 - val_loss: 0.1522 - val_rmse: 0.1522\n",
      "Epoch 880/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1777 - rmse: 0.1777 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 881/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1874 - rmse: 0.1874 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 882/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1831 - rmse: 0.1831 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 883/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1774 - rmse: 0.1774 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 884/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1667 - rmse: 0.1667 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 885/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1794 - rmse: 0.1794 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 886/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1523 - val_rmse: 0.1523\n",
      "Epoch 887/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1792 - rmse: 0.1792 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 888/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1497 - val_rmse: 0.1497\n",
      "Epoch 889/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1874 - rmse: 0.1874 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 890/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1754 - rmse: 0.1754 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 891/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1828 - rmse: 0.1828 - val_loss: 0.1550 - val_rmse: 0.1550\n",
      "Epoch 892/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1811 - rmse: 0.1811 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 893/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1726 - rmse: 0.1726 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 894/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1838 - rmse: 0.1838 - val_loss: 0.1509 - val_rmse: 0.1509\n",
      "Epoch 895/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1782 - rmse: 0.1782 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 896/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1781 - rmse: 0.1781 - val_loss: 0.1581 - val_rmse: 0.1581\n",
      "Epoch 897/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 898/1000\n",
      "1166/1166 [==============================] - 0s 24us/step - loss: 0.1681 - rmse: 0.1681 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 899/1000\n",
      "1166/1166 [==============================] - 0s 20us/step - loss: 0.1800 - rmse: 0.1800 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 900/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1755 - rmse: 0.1755 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 901/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1871 - rmse: 0.1871 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 902/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 903/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1760 - rmse: 0.1760 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 904/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1794 - rmse: 0.1794 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 905/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1762 - rmse: 0.1762 - val_loss: 0.1589 - val_rmse: 0.1589\n",
      "Epoch 906/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1567 - val_rmse: 0.1567\n",
      "Epoch 907/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1851 - rmse: 0.1851 - val_loss: 0.1562 - val_rmse: 0.1562\n",
      "Epoch 908/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1836 - rmse: 0.1836 - val_loss: 0.1574 - val_rmse: 0.1574\n",
      "Epoch 909/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 910/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.1817 - rmse: 0.1817 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 911/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1857 - rmse: 0.1857 - val_loss: 0.1575 - val_rmse: 0.1575\n",
      "Epoch 912/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1715 - rmse: 0.1715 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 913/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1859 - rmse: 0.1859 - val_loss: 0.1600 - val_rmse: 0.1600\n",
      "Epoch 914/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1795 - rmse: 0.1795 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 915/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1786 - rmse: 0.1786 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 916/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1838 - rmse: 0.1838 - val_loss: 0.1528 - val_rmse: 0.1528\n",
      "Epoch 917/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1872 - rmse: 0.1872 - val_loss: 0.1628 - val_rmse: 0.1628\n",
      "Epoch 918/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1802 - rmse: 0.1802 - val_loss: 0.1572 - val_rmse: 0.1572\n",
      "Epoch 919/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1826 - rmse: 0.1826 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 920/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1873 - rmse: 0.1873 - val_loss: 0.1622 - val_rmse: 0.1622\n",
      "Epoch 921/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1742 - rmse: 0.1742 - val_loss: 0.1535 - val_rmse: 0.1535\n",
      "Epoch 922/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1834 - rmse: 0.1834 - val_loss: 0.1570 - val_rmse: 0.1570\n",
      "Epoch 923/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1754 - rmse: 0.1754 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 924/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 925/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 926/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1691 - rmse: 0.1691 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 927/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1825 - rmse: 0.1825 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 928/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1742 - rmse: 0.1742 - val_loss: 0.1521 - val_rmse: 0.1521\n",
      "Epoch 929/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1770 - rmse: 0.1770 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 930/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1778 - rmse: 0.1778 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 931/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1772 - rmse: 0.1772 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 932/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1665 - rmse: 0.1665 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 933/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1749 - rmse: 0.1749 - val_loss: 0.1592 - val_rmse: 0.1592\n",
      "Epoch 934/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1858 - rmse: 0.1858 - val_loss: 0.1534 - val_rmse: 0.1534\n",
      "Epoch 935/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1797 - rmse: 0.1797 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 936/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1768 - rmse: 0.1768 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 937/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1691 - rmse: 0.1691 - val_loss: 0.1509 - val_rmse: 0.1509\n",
      "Epoch 938/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1674 - rmse: 0.1674 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 939/1000\n",
      "1166/1166 [==============================] - 0s 52us/step - loss: 0.1744 - rmse: 0.1744 - val_loss: 0.1618 - val_rmse: 0.1618\n",
      "Epoch 940/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1790 - rmse: 0.1790 - val_loss: 0.1560 - val_rmse: 0.1560\n",
      "Epoch 941/1000\n",
      "1166/1166 [==============================] - 0s 58us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1492 - val_rmse: 0.1492\n",
      "Epoch 942/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1750 - rmse: 0.1750 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 943/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1851 - rmse: 0.1851 - val_loss: 0.1563 - val_rmse: 0.1563\n",
      "Epoch 944/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1593 - val_rmse: 0.1593\n",
      "Epoch 945/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1691 - rmse: 0.1691 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 946/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1870 - rmse: 0.1870 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 947/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1747 - rmse: 0.1747 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 948/1000\n",
      "1166/1166 [==============================] - 0s 51us/step - loss: 0.1851 - rmse: 0.1851 - val_loss: 0.1581 - val_rmse: 0.1581\n",
      "Epoch 949/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1885 - rmse: 0.1885 - val_loss: 0.1559 - val_rmse: 0.1559\n",
      "Epoch 950/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1805 - rmse: 0.1805 - val_loss: 0.1515 - val_rmse: 0.1515\n",
      "Epoch 951/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1721 - rmse: 0.1721 - val_loss: 0.1571 - val_rmse: 0.1571\n",
      "Epoch 952/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1791 - rmse: 0.1791 - val_loss: 0.1541 - val_rmse: 0.1541\n",
      "Epoch 953/1000\n",
      "1166/1166 [==============================] - 0s 53us/step - loss: 0.1848 - rmse: 0.1848 - val_loss: 0.1574 - val_rmse: 0.1574\n",
      "Epoch 954/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1742 - rmse: 0.1742 - val_loss: 0.1551 - val_rmse: 0.1551\n",
      "Epoch 955/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1727 - rmse: 0.1727 - val_loss: 0.1527 - val_rmse: 0.1527\n",
      "Epoch 956/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1777 - rmse: 0.1777 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 957/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1840 - rmse: 0.1840 - val_loss: 0.1549 - val_rmse: 0.1549\n",
      "Epoch 958/1000\n",
      "1166/1166 [==============================] - 0s 42us/step - loss: 0.1746 - rmse: 0.1746 - val_loss: 0.1518 - val_rmse: 0.1518\n",
      "Epoch 959/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 960/1000\n",
      "1166/1166 [==============================] - 0s 29us/step - loss: 0.1872 - rmse: 0.1872 - val_loss: 0.1514 - val_rmse: 0.1514\n",
      "Epoch 961/1000\n",
      "1166/1166 [==============================] - 0s 55us/step - loss: 0.1820 - rmse: 0.1820 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 962/1000\n",
      "1166/1166 [==============================] - 0s 46us/step - loss: 0.1783 - rmse: 0.1783 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 963/1000\n",
      "1166/1166 [==============================] - 0s 36us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 964/1000\n",
      "1166/1166 [==============================] - 0s 32us/step - loss: 0.1793 - rmse: 0.1793 - val_loss: 0.1512 - val_rmse: 0.1512\n",
      "Epoch 965/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1821 - rmse: 0.1821 - val_loss: 0.1525 - val_rmse: 0.1525\n",
      "Epoch 966/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1758 - rmse: 0.1758 - val_loss: 0.1547 - val_rmse: 0.1547\n",
      "Epoch 967/1000\n",
      "1166/1166 [==============================] - 0s 33us/step - loss: 0.1736 - rmse: 0.1736 - val_loss: 0.1602 - val_rmse: 0.1602\n",
      "Epoch 968/1000\n",
      "1166/1166 [==============================] - 0s 45us/step - loss: 0.1686 - rmse: 0.1686 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 969/1000\n",
      "1166/1166 [==============================] - 0s 28us/step - loss: 0.1741 - rmse: 0.1741 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 970/1000\n",
      "1166/1166 [==============================] - 0s 27us/step - loss: 0.1836 - rmse: 0.1836 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 971/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1820 - rmse: 0.1820 - val_loss: 0.1524 - val_rmse: 0.1524\n",
      "Epoch 972/1000\n",
      "1166/1166 [==============================] - 0s 65us/step - loss: 0.1692 - rmse: 0.1692 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 973/1000\n",
      "1166/1166 [==============================] - 0s 48us/step - loss: 0.1735 - rmse: 0.1735 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 974/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1775 - rmse: 0.1775 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 975/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1793 - rmse: 0.1793 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 976/1000\n",
      "1166/1166 [==============================] - 0s 40us/step - loss: 0.1696 - rmse: 0.1696 - val_loss: 0.1555 - val_rmse: 0.1555\n",
      "Epoch 977/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1765 - rmse: 0.1765 - val_loss: 0.1546 - val_rmse: 0.1546\n",
      "Epoch 978/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1610 - rmse: 0.1610 - val_loss: 0.1568 - val_rmse: 0.1568\n",
      "Epoch 979/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1796 - rmse: 0.1796 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 980/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1787 - rmse: 0.1787 - val_loss: 0.1505 - val_rmse: 0.1505\n",
      "Epoch 981/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.1613 - rmse: 0.16 - 0s 34us/step - loss: 0.1749 - rmse: 0.1749 - val_loss: 0.1531 - val_rmse: 0.1531\n",
      "Epoch 982/1000\n",
      "1166/1166 [==============================] - 0s 44us/step - loss: 0.1806 - rmse: 0.1806 - val_loss: 0.1480 - val_rmse: 0.1480\n",
      "Epoch 983/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1751 - rmse: 0.1751 - val_loss: 0.1490 - val_rmse: 0.1490\n",
      "Epoch 984/1000\n",
      "1166/1166 [==============================] - 0s 31us/step - loss: 0.1852 - rmse: 0.1852 - val_loss: 0.1556 - val_rmse: 0.1556\n",
      "Epoch 985/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1718 - rmse: 0.1718 - val_loss: 0.1557 - val_rmse: 0.1557\n",
      "Epoch 986/1000\n",
      "1166/1166 [==============================] - 0s 43us/step - loss: 0.1866 - rmse: 0.1866 - val_loss: 0.1560 - val_rmse: 0.1560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 987/1000\n",
      "1166/1166 [==============================] - 0s 38us/step - loss: 0.1884 - rmse: 0.1884 - val_loss: 0.1513 - val_rmse: 0.1513\n",
      "Epoch 988/1000\n",
      "1166/1166 [==============================] - 0s 35us/step - loss: 0.1750 - rmse: 0.1750 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 989/1000\n",
      "1166/1166 [==============================] - 0s 49us/step - loss: 0.1853 - rmse: 0.1853 - val_loss: 0.1584 - val_rmse: 0.1584\n",
      "Epoch 990/1000\n",
      "1166/1166 [==============================] - 0s 20us/step - loss: 0.1846 - rmse: 0.1846 - val_loss: 0.1508 - val_rmse: 0.1508\n",
      "Epoch 991/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.1769 - rmse: 0.1769 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 992/1000\n",
      "1166/1166 [==============================] - 0s 56us/step - loss: 0.1768 - rmse: 0.1768 - val_loss: 0.1542 - val_rmse: 0.1542\n",
      "Epoch 993/1000\n",
      "1166/1166 [==============================] - 0s 50us/step - loss: 0.1845 - rmse: 0.1845 - val_loss: 0.1561 - val_rmse: 0.1561\n",
      "Epoch 994/1000\n",
      "1166/1166 [==============================] - 0s 39us/step - loss: 0.1830 - rmse: 0.1830 - val_loss: 0.1553 - val_rmse: 0.1553\n",
      "Epoch 995/1000\n",
      "1166/1166 [==============================] - 0s 41us/step - loss: 0.1680 - rmse: 0.1680 - val_loss: 0.1544 - val_rmse: 0.1544\n",
      "Epoch 996/1000\n",
      "1166/1166 [==============================] - 0s 34us/step - loss: 0.1849 - rmse: 0.1849 - val_loss: 0.1552 - val_rmse: 0.1552\n",
      "Epoch 997/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1749 - rmse: 0.1749 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 998/1000\n",
      "1166/1166 [==============================] - 0s 30us/step - loss: 0.1709 - rmse: 0.1709 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 999/1000\n",
      "1166/1166 [==============================] - 0s 37us/step - loss: 0.1737 - rmse: 0.1737 - val_loss: 0.1602 - val_rmse: 0.1602\n",
      "Epoch 1000/1000\n",
      "1166/1166 [==============================] - 0s 47us/step - loss: 0.1719 - rmse: 0.1719 - val_loss: 0.1510 - val_rmse: 0.1510\n"
     ]
    }
   ],
   "source": [
    "history = NN_model.fit(X, y, batch_size=100, epochs=1000,\n",
    "              validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot loss function and accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_perf(history):\n",
    "    acc = history.history['rmse']\n",
    "    val_acc = history.history['val_rmse']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, label='Training acc')\n",
    "    plt.plot(epochs, val_acc, label='Validation acc')\n",
    "    plt.title('RMSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim((0, 0.2))\n",
    "    plt.figure()\n",
    "\n",
    "    #plt.plot(epochs, loss, label='Training loss')\n",
    "    #plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    #plt.title('RMSE')\n",
    "    #plt.legend()\n",
    "    #plt.grid(True)\n",
    "    #plt.ylim((0, 0.2))\n",
    "    #plt.show()\n",
    "# plot loss and accuracy\n",
    "plot_perf(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
