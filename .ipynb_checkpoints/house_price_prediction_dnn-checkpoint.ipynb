{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, LassoCV,LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Adam, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>...</th>\n",
       "      <th>MasVnrArea_absent</th>\n",
       "      <th>MiscVal_absent</th>\n",
       "      <th>OpenPorchSF_absent</th>\n",
       "      <th>PoolArea_absent</th>\n",
       "      <th>ScreenPorch_absent</th>\n",
       "      <th>TotalBsmtSF_absent</th>\n",
       "      <th>WoodDeckSF_absent</th>\n",
       "      <th>compPrice</th>\n",
       "      <th>compIndex</th>\n",
       "      <th>compPriceXcompIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75.920375</td>\n",
       "      <td>37.2</td>\n",
       "      <td>2824.237967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.907207</td>\n",
       "      <td>32.4</td>\n",
       "      <td>2070.593520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>81.912811</td>\n",
       "      <td>37.2</td>\n",
       "      <td>3047.156573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>272</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.113339</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2204.513512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.591085</td>\n",
       "      <td>36.4</td>\n",
       "      <td>3115.515497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       856       854          0             3       706.0         0.0   \n",
       "1      1262         0          0             3       978.0         0.0   \n",
       "2       920       866          0             3       486.0         0.0   \n",
       "3       961       756          0             3       216.0         0.0   \n",
       "4      1145      1053          0             4       655.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  \\\n",
       "0           1.0           0.0      150.0              0  ...   \n",
       "1           0.0           1.0      284.0              0  ...   \n",
       "2           1.0           0.0      434.0              0  ...   \n",
       "3           1.0           0.0      540.0            272  ...   \n",
       "4           1.0           0.0      490.0              0  ...   \n",
       "\n",
       "   MasVnrArea_absent  MiscVal_absent  OpenPorchSF_absent  PoolArea_absent  \\\n",
       "0                  0               1                   0                1   \n",
       "1                  1               1                   1                1   \n",
       "2                  0               1                   0                1   \n",
       "3                  1               1                   0                1   \n",
       "4                  0               1                   0                1   \n",
       "\n",
       "   ScreenPorch_absent  TotalBsmtSF_absent  WoodDeckSF_absent  compPrice  \\\n",
       "0                   1                   0                  1  75.920375   \n",
       "1                   1                   0                  0  63.907207   \n",
       "2                   1                   0                  1  81.912811   \n",
       "3                   1                   0                  1  71.113339   \n",
       "4                   1                   0                  0  85.591085   \n",
       "\n",
       "   compIndex  compPriceXcompIndex  \n",
       "0       37.2          2824.237967  \n",
       "1       32.4          2070.593520  \n",
       "2       37.2          3047.156573  \n",
       "3       31.0          2204.513512  \n",
       "4       36.4          3115.515497  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('dataset2_addfeatures.csv')\n",
    "# sale price\n",
    "SalePrice = data['SalePrice'].dropna()\n",
    "y = np.log1p(SalePrice)\n",
    "data = data.drop(['SalePrice','Id'],axis=1)\n",
    "cols_categoric = data.select_dtypes(include = [\"object\"]).columns.tolist()\n",
    "# number of training samples\n",
    "n_train = SalePrice.shape[0]\n",
    "# add total square footage\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function dummifies categoric features\n",
    "def get_custom_dummies(data,y,n_train):\n",
    "    cols_categoric = data.select_dtypes(include = [\"object\"]).columns.tolist()\n",
    "    cols_numeric = set(data.columns.tolist())-set(cols_categoric)\n",
    "    df = data.loc[:,cols_numeric]\n",
    "    for col in cols_categoric:        \n",
    "        dummies = pd.get_dummies(data[[col]])\n",
    "        idx_min_corr = np.argmin(np.abs(np.array([dummies[:n_train][subcol].corr(y) for subcol in dummies.columns.tolist()])))\n",
    "        col_min_corr = dummies.columns.tolist()[idx_min_corr]\n",
    "        df = pd.concat([df,dummies.drop(col_min_corr,axis=1)],axis=1)    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>BsmtFinSF1_log</th>\n",
       "      <th>GarageArea_log</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>12_rooms_plus_dummy</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>MasVnrArea_log</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_COD</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>Street_Grvl</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.204412</td>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.207737</td>\n",
       "      <td>0.784555</td>\n",
       "      <td>0.344734</td>\n",
       "      <td>0.897548</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>-0.078784</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>1.224522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.704422</td>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.207737</td>\n",
       "      <td>0.893745</td>\n",
       "      <td>0.223396</td>\n",
       "      <td>-0.394797</td>\n",
       "      <td>2.186999</td>\n",
       "      <td>-0.078784</td>\n",
       "      <td>-0.061413</td>\n",
       "      <td>-0.792686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.078668</td>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.207737</td>\n",
       "      <td>0.659509</td>\n",
       "      <td>0.416773</td>\n",
       "      <td>0.849683</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>-0.078784</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>1.152186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.182960</td>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.207737</td>\n",
       "      <td>0.388339</td>\n",
       "      <td>0.454505</td>\n",
       "      <td>-0.681985</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>-0.078784</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>-0.792686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.547087</td>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.207737</td>\n",
       "      <td>0.759440</td>\n",
       "      <td>0.637644</td>\n",
       "      <td>0.753954</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>-0.078784</td>\n",
       "      <td>1.360350</td>\n",
       "      <td>1.445052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175304</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>-0.04143</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OpenPorchSF  PoolArea  KitchenAbvGr  BsmtFinSF1_log  GarageArea_log  \\\n",
       "0     0.204412 -0.060437     -0.207737        0.784555        0.344734   \n",
       "1    -0.704422 -0.060437     -0.207737        0.893745        0.223396   \n",
       "2    -0.078668 -0.060437     -0.207737        0.659509        0.416773   \n",
       "3    -0.182960 -0.060437     -0.207737        0.388339        0.454505   \n",
       "4     0.547087 -0.060437     -0.207737        0.759440        0.637644   \n",
       "\n",
       "   YearRemodAdd  OverallCond  12_rooms_plus_dummy  OverallQual  \\\n",
       "0      0.897548    -0.507416            -0.078784     0.649468   \n",
       "1     -0.394797     2.186999            -0.078784    -0.061413   \n",
       "2      0.849683    -0.507416            -0.078784     0.649468   \n",
       "3     -0.681985    -0.507416            -0.078784     0.649468   \n",
       "4      0.753954    -0.507416            -0.078784     1.360350   \n",
       "\n",
       "   MasVnrArea_log  ...  SaleType_COD  SaleType_CWD  SaleType_Con  \\\n",
       "0        1.224522  ...     -0.175304      -0.06426      -0.04143   \n",
       "1       -0.792686  ...     -0.175304      -0.06426      -0.04143   \n",
       "2        1.152186  ...     -0.175304      -0.06426      -0.04143   \n",
       "3       -0.792686  ...     -0.175304      -0.06426      -0.04143   \n",
       "4        1.445052  ...     -0.175304      -0.06426      -0.04143   \n",
       "\n",
       "   SaleType_ConLD  SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "0       -0.094817       -0.052432     -0.297326     -0.049037     0.393366   \n",
       "1       -0.094817       -0.052432     -0.297326     -0.049037     0.393366   \n",
       "2       -0.094817       -0.052432     -0.297326     -0.049037     0.393366   \n",
       "3       -0.094817       -0.052432     -0.297326     -0.049037     0.393366   \n",
       "4       -0.094817       -0.052432     -0.297326     -0.049037     0.393366   \n",
       "\n",
       "   Street_Grvl  Utilities_AllPub  \n",
       "0     -0.06426          0.018515  \n",
       "1     -0.06426          0.018515  \n",
       "2     -0.06426          0.018515  \n",
       "3     -0.06426          0.018515  \n",
       "4     -0.06426          0.018515  \n",
       "\n",
       "[5 rows x 337 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummify data\n",
    "data = get_custom_dummies(data,y,n_train)\n",
    "data = (data - data.mean())/data.std()\n",
    "# drop duplicates\n",
    "X = data[:n_train]\n",
    "X = X.T.drop_duplicates().T\n",
    "# find columns\n",
    "cols = X.columns.tolist()\n",
    "data = data.loc[:,cols]\n",
    "X = data[:n_train]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid 1:\n",
      "CV score: 0.11096522844685176\n",
      "{'alpha': 0.001}\n",
      "grid 2:\n",
      "CV score: 0.1074268612067819\n",
      "{'alpha': 0.003430469286314919}\n"
     ]
    }
   ],
   "source": [
    "# lasso model\n",
    "lasso = Lasso(random_state=1)\n",
    "# determine best alpha using GridSearch\n",
    "# grid 1\n",
    "print(\"grid 1:\")\n",
    "alphas = {'alpha': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]}\n",
    "model = GridSearchCV(estimator=lasso, param_grid=alphas, n_jobs=-1, cv=10, scoring='neg_mean_squared_error')\n",
    "model.fit(X,y)\n",
    "print(\"CV score: {}\".format(np.sqrt(-model.best_score_)))\n",
    "print(model.best_params_)\n",
    "# grid 2\n",
    "print(\"grid 2:\")\n",
    "alphas = {'alpha': np.geomspace(1e-2, 1e-4, num=100)}\n",
    "model = GridSearchCV(estimator=lasso, param_grid=alphas, n_jobs=-1, cv=10, scoring='neg_mean_squared_error')\n",
    "model.fit(X,y)\n",
    "print(\"CV score: {}\".format(np.sqrt(-model.best_score_)))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso plot coeff\n",
    "def lasso_plot_coef(model,X,n_features):\n",
    "    # series of coefficients\n",
    "    cols=X.columns.tolist()\n",
    "    # all pandas series\n",
    "    coefs = pd.Series(model.coef_.tolist(),index=cols)\n",
    "    threshold = np.abs(coefs).sort_values(ascending=False)[:n_features].values[-1]\n",
    "    coefs = coefs[np.abs(coefs)>=threshold].sort_values(ascending=True)    \n",
    "    objects = coefs.index.tolist()\n",
    "    y_arange = np.arange(len(objects))\n",
    "    values = coefs.tolist()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.barh(y_arange, values, align='center')\n",
    "    plt.yticks(y_arange, objects)\n",
    "    plt.xlabel('features')\n",
    "    plt.title('LASSO Feature Importance: {}'.format(n_features)+' most important features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set score: 0.0968003561563015\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# score on entire training set\n",
    "print(\"training set score: {}\".format( np.sqrt(mean_squared_error(y,model.predict(X))) ))\n",
    "lasso_plot_coef(model.best_estimator_,X,n_features=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>BsmtFinSF</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>AllPorchSF</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>5_bedroom_plus_dummy</th>\n",
       "      <th>YearRemodAddAge</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolQC_Gd</th>\n",
       "      <th>RoofMatl_Membran</th>\n",
       "      <th>RoofMatl_WdShngl</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>Street_Grvl</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>0.466232</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>0.307685</td>\n",
       "      <td>-0.761751</td>\n",
       "      <td>-0.216400</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>-0.887744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>2.186999</td>\n",
       "      <td>-0.061413</td>\n",
       "      <td>1.049193</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>0.307685</td>\n",
       "      <td>0.721678</td>\n",
       "      <td>-0.069097</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>0.356886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>-0.005281</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>0.307685</td>\n",
       "      <td>-0.880675</td>\n",
       "      <td>0.142251</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>-0.839874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>0.649468</td>\n",
       "      <td>-0.583956</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>1.619846</td>\n",
       "      <td>0.778011</td>\n",
       "      <td>-0.075501</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>0.596238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>3.787837</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.060437</td>\n",
       "      <td>-0.507416</td>\n",
       "      <td>1.360350</td>\n",
       "      <td>0.356926</td>\n",
       "      <td>-0.285992</td>\n",
       "      <td>1.619846</td>\n",
       "      <td>0.583976</td>\n",
       "      <td>0.527801</td>\n",
       "      <td>-0.156776</td>\n",
       "      <td>-0.744133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032081</td>\n",
       "      <td>-0.018515</td>\n",
       "      <td>-0.049037</td>\n",
       "      <td>-0.263912</td>\n",
       "      <td>-0.126557</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.297326</td>\n",
       "      <td>0.393366</td>\n",
       "      <td>-0.06426</td>\n",
       "      <td>0.018515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PoolArea  OverallCond  OverallQual  BsmtFinSF  ScreenPorch  GarageCars  \\\n",
       "0 -0.060437    -0.507416     0.649468   0.466232    -0.285992    0.307685   \n",
       "1 -0.060437     2.186999    -0.061413   1.049193    -0.285992    0.307685   \n",
       "2 -0.060437    -0.507416     0.649468  -0.005281    -0.285992    0.307685   \n",
       "3 -0.060437    -0.507416     0.649468  -0.583956    -0.285992    1.619846   \n",
       "4 -0.060437    -0.507416     1.360350   0.356926    -0.285992    1.619846   \n",
       "\n",
       "   AllPorchSF   LotArea  5_bedroom_plus_dummy  YearRemodAddAge  ...  \\\n",
       "0   -0.761751 -0.216400             -0.156776        -0.887744  ...   \n",
       "1    0.721678 -0.069097             -0.156776         0.356886  ...   \n",
       "2   -0.880675  0.142251             -0.156776        -0.839874  ...   \n",
       "3    0.778011 -0.075501             -0.156776         0.596238  ...   \n",
       "4    0.583976  0.527801             -0.156776        -0.744133  ...   \n",
       "\n",
       "   PoolQC_Gd  RoofMatl_Membran  RoofMatl_WdShngl  SaleCondition_Abnorml  \\\n",
       "0  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "1  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "2  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "3  -0.032081         -0.018515         -0.049037               3.787837   \n",
       "4  -0.032081         -0.018515         -0.049037              -0.263912   \n",
       "\n",
       "   SaleCondition_Family  SaleType_ConLD  SaleType_New  SaleType_WD  \\\n",
       "0             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "1             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "2             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "3             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "4             -0.126557       -0.094817     -0.297326     0.393366   \n",
       "\n",
       "   Street_Grvl  Utilities_AllPub  \n",
       "0     -0.06426          0.018515  \n",
       "1     -0.06426          0.018515  \n",
       "2     -0.06426          0.018515  \n",
       "3     -0.06426          0.018515  \n",
       "4     -0.06426          0.018515  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=X.columns.tolist()\n",
    "# all pandas series\n",
    "coefs = pd.Series(model.best_estimator_.coef_.tolist(),index=cols)\n",
    "coefs = coefs[coefs!=0]\n",
    "data = data.loc[:,coefs.index.tolist()]\n",
    "X = data[:n_train]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeated k-fold\n",
    "repeatedkfold = RepeatedKFold()\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=2652124)\n",
    "# LassoCV\n",
    "lassocv = LassoCV(cv=rkf,n_alphas=100,n_jobs=-1)\n",
    "# model validation score\n",
    "def model_validation_score(X,y,model=lassocv,n_split=10,n_repeats=1,msg=False):\n",
    "    repeatedkfold = RepeatedKFold()\n",
    "    rkf = RepeatedKFold(n_splits=n_split, n_repeats=n_repeats, random_state=1)\n",
    "    sample = range(0,X.shape[0])\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "    y_train_lst = []\n",
    "    y_train_estimated_lst = []\n",
    "    y_test_lst = []\n",
    "    y_test_estimated_lst = []\n",
    "    iteration = 1\n",
    "    for train_index, test_index in rkf.split(sample):\n",
    "        if (msg==True): print('iteration {}'.format(iteration))\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        reg = model.fit(X_train,y_train)        \n",
    "        train_score.append(reg.score(X_train,y_train))\n",
    "        test_score.append(reg.score(X_test,y_test))\n",
    "        y_train_lst += y_train.tolist()\n",
    "        y_test_lst += y_test.tolist()\n",
    "        y_train_estimated_lst += model.predict(X_train).tolist()\n",
    "        y_test_estimated_lst += model.predict(X_test).tolist()\n",
    "        iteration += 1\n",
    "    train_score = np.sqrt(mean_squared_error(y_train_lst,y_train_estimated_lst))\n",
    "    test_score = np.sqrt(mean_squared_error(y_test_lst,y_test_estimated_lst))\n",
    "    if (msg==True):\n",
    "        print('train root_mean_squared_log_error: {}'.format(train_score))\n",
    "        print('test root_mean_squared_log_error: {}'.format(test_score))\n",
    "    return(train_score,test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09247022707535052, 0.10557792779844272)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_validation_score(X,y,model=lassocv,n_split=10,n_repeats=10,msg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit lasso with Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 07:39:43.744500 10500 deprecation_wrapper.py:119] From C:\\Users\\tsyurmasto\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0822 07:39:43.752501 10500 deprecation_wrapper.py:119] From C:\\Users\\tsyurmasto\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set score: 0.09696698231919339\n"
     ]
    }
   ],
   "source": [
    "# define custom metric\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def rmse(y_true,y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred-y_true),axis=-1))\n",
    "\n",
    "NN_model = Sequential()\n",
    "\"\"\"\n",
    "    Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n",
    "          bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \n",
    "          activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\"\"\"\n",
    "# The Output Layer:\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', input_dim = X.shape[1], \n",
    "                   kernel_regularizer=l1(0.003430469286314919), activation='linear'))\n",
    "# Compile the network:\n",
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "NN_model.compile(loss='mse', optimizer=sgd, metrics=[rmse])\n",
    "history = NN_model.fit(X, y, epochs=300, batch_size=1000, verbose = 0)\n",
    "plt.plot(history.history['rmse'])\n",
    "plt.ylim((0, 0.2))\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'],loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# score on entire training set\n",
    "print(\"training set score: {}\".format( np.sqrt(mean_squared_error(NN_model.predict(X),y)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit deep neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 10,301\n",
      "Trainable params: 10,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(100, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n",
    "\n",
    "NN_model.add(Dropout(0.5))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network:\n",
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer=sgd, metrics=[rmse])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1166 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1166/1166 [==============================] - 0s 185us/step - loss: 11.7844 - rmse: 11.7844 - val_loss: 11.4693 - val_rmse: 11.4693\n",
      "Epoch 2/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 11.2115 - rmse: 11.2115 - val_loss: 10.8611 - val_rmse: 10.8611\n",
      "Epoch 3/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 10.5926 - rmse: 10.5926 - val_loss: 10.2011 - val_rmse: 10.2011\n",
      "Epoch 4/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 9.9161 - rmse: 9.9161 - val_loss: 9.4854 - val_rmse: 9.4854\n",
      "Epoch 5/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 9.1543 - rmse: 9.1543 - val_loss: 8.6440 - val_rmse: 8.6440\n",
      "Epoch 6/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 8.1696 - rmse: 8.1696 - val_loss: 7.6017 - val_rmse: 7.6017\n",
      "Epoch 7/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 7.0017 - rmse: 7.0017 - val_loss: 6.2972 - val_rmse: 6.2972\n",
      "Epoch 8/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 5.7474 - rmse: 5.7474 - val_loss: 4.9839 - val_rmse: 4.9839\n",
      "Epoch 9/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 4.4709 - rmse: 4.4709 - val_loss: 3.8734 - val_rmse: 3.8734\n",
      "Epoch 10/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 3.6496 - rmse: 3.6496 - val_loss: 3.0606 - val_rmse: 3.0606\n",
      "Epoch 11/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 3.1020 - rmse: 3.1020 - val_loss: 2.4998 - val_rmse: 2.4998\n",
      "Epoch 12/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 2.6380 - rmse: 2.6380 - val_loss: 2.1252 - val_rmse: 2.1252\n",
      "Epoch 13/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 2.4069 - rmse: 2.4069 - val_loss: 1.8365 - val_rmse: 1.8365\n",
      "Epoch 14/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 2.2003 - rmse: 2.2003 - val_loss: 1.6100 - val_rmse: 1.6100\n",
      "Epoch 15/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 2.0073 - rmse: 2.0073 - val_loss: 1.4533 - val_rmse: 1.4533\n",
      "Epoch 16/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.9366 - rmse: 1.9366 - val_loss: 1.2885 - val_rmse: 1.2885\n",
      "Epoch 17/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.8177 - rmse: 1.8177 - val_loss: 1.1868 - val_rmse: 1.1868\n",
      "Epoch 18/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.7768 - rmse: 1.7768 - val_loss: 1.1308 - val_rmse: 1.1308\n",
      "Epoch 19/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.7125 - rmse: 1.7125 - val_loss: 1.1199 - val_rmse: 1.1199\n",
      "Epoch 20/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.6975 - rmse: 1.6975 - val_loss: 1.0567 - val_rmse: 1.0567\n",
      "Epoch 21/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.6132 - rmse: 1.6132 - val_loss: 1.0180 - val_rmse: 1.0180\n",
      "Epoch 22/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.6620 - rmse: 1.6620 - val_loss: 1.0051 - val_rmse: 1.0051\n",
      "Epoch 23/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.6089 - rmse: 1.6089 - val_loss: 0.9679 - val_rmse: 0.9679\n",
      "Epoch 24/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 1.5478 - rmse: 1.5478 - val_loss: 0.9458 - val_rmse: 0.9458\n",
      "Epoch 25/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.5846 - rmse: 1.5846 - val_loss: 0.9643 - val_rmse: 0.9643\n",
      "Epoch 26/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.5690 - rmse: 1.5690 - val_loss: 0.9123 - val_rmse: 0.9123\n",
      "Epoch 27/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.5469 - rmse: 1.5469 - val_loss: 0.8807 - val_rmse: 0.8807\n",
      "Epoch 28/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.5255 - rmse: 1.5255 - val_loss: 0.8827 - val_rmse: 0.8827\n",
      "Epoch 29/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.5291 - rmse: 1.5291 - val_loss: 0.8513 - val_rmse: 0.8513\n",
      "Epoch 30/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.4791 - rmse: 1.4791 - val_loss: 0.8842 - val_rmse: 0.8842\n",
      "Epoch 31/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.4561 - rmse: 1.4561 - val_loss: 0.8487 - val_rmse: 0.8487\n",
      "Epoch 32/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.4720 - rmse: 1.4720 - val_loss: 0.8213 - val_rmse: 0.8213\n",
      "Epoch 33/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.4800 - rmse: 1.4800 - val_loss: 0.8012 - val_rmse: 0.8012\n",
      "Epoch 34/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3867 - rmse: 1.3867 - val_loss: 0.7795 - val_rmse: 0.7795\n",
      "Epoch 35/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 1.4487 - rmse: 1.4487 - val_loss: 0.8000 - val_rmse: 0.8000\n",
      "Epoch 36/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3742 - rmse: 1.3742 - val_loss: 0.7904 - val_rmse: 0.7904\n",
      "Epoch 37/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3997 - rmse: 1.3997 - val_loss: 0.8009 - val_rmse: 0.8009\n",
      "Epoch 38/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.4044 - rmse: 1.4044 - val_loss: 0.7529 - val_rmse: 0.7529\n",
      "Epoch 39/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3797 - rmse: 1.3797 - val_loss: 0.7538 - val_rmse: 0.7538\n",
      "Epoch 40/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.4176 - rmse: 1.4176 - val_loss: 0.7704 - val_rmse: 0.7704\n",
      "Epoch 41/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.4125 - rmse: 1.4125 - val_loss: 0.7353 - val_rmse: 0.7353\n",
      "Epoch 42/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.3997 - rmse: 1.3997 - val_loss: 0.7316 - val_rmse: 0.7316\n",
      "Epoch 43/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3580 - rmse: 1.3580 - val_loss: 0.7434 - val_rmse: 0.7434\n",
      "Epoch 44/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.3828 - rmse: 1.3828 - val_loss: 0.7456 - val_rmse: 0.7456\n",
      "Epoch 45/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.3765 - rmse: 1.3765 - val_loss: 0.7458 - val_rmse: 0.7458\n",
      "Epoch 46/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3337 - rmse: 1.3337 - val_loss: 0.7280 - val_rmse: 0.7280\n",
      "Epoch 47/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.3416 - rmse: 1.3416 - val_loss: 0.7370 - val_rmse: 0.7370\n",
      "Epoch 48/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.3422 - rmse: 1.3422 - val_loss: 0.7334 - val_rmse: 0.7334\n",
      "Epoch 49/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3467 - rmse: 1.3467 - val_loss: 0.6926 - val_rmse: 0.6926\n",
      "Epoch 50/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.3175 - rmse: 1.3175 - val_loss: 0.7220 - val_rmse: 0.7220\n",
      "Epoch 51/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2945 - rmse: 1.2945 - val_loss: 0.6919 - val_rmse: 0.6919\n",
      "Epoch 52/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2965 - rmse: 1.2965 - val_loss: 0.6683 - val_rmse: 0.6683\n",
      "Epoch 53/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3045 - rmse: 1.3045 - val_loss: 0.6874 - val_rmse: 0.6874\n",
      "Epoch 54/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2783 - rmse: 1.2783 - val_loss: 0.6584 - val_rmse: 0.6584\n",
      "Epoch 55/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.3056 - rmse: 1.3056 - val_loss: 0.6486 - val_rmse: 0.6486\n",
      "Epoch 56/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2985 - rmse: 1.2985 - val_loss: 0.6867 - val_rmse: 0.6867\n",
      "Epoch 57/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2946 - rmse: 1.2946 - val_loss: 0.6520 - val_rmse: 0.6520\n",
      "Epoch 58/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.2557 - rmse: 1.2557 - val_loss: 0.6476 - val_rmse: 0.6476\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2942 - rmse: 1.2942 - val_loss: 0.6913 - val_rmse: 0.6913\n",
      "Epoch 60/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2718 - rmse: 1.2718 - val_loss: 0.6551 - val_rmse: 0.6551\n",
      "Epoch 61/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2840 - rmse: 1.2840 - val_loss: 0.6263 - val_rmse: 0.6263\n",
      "Epoch 62/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2692 - rmse: 1.2692 - val_loss: 0.6245 - val_rmse: 0.6245\n",
      "Epoch 63/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.3140 - rmse: 1.3140 - val_loss: 0.6348 - val_rmse: 0.6348\n",
      "Epoch 64/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2354 - rmse: 1.2354 - val_loss: 0.6071 - val_rmse: 0.6071\n",
      "Epoch 65/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2288 - rmse: 1.2288 - val_loss: 0.6352 - val_rmse: 0.6352\n",
      "Epoch 66/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2262 - rmse: 1.2262 - val_loss: 0.6354 - val_rmse: 0.6354\n",
      "Epoch 67/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2477 - rmse: 1.2477 - val_loss: 0.6127 - val_rmse: 0.6127\n",
      "Epoch 68/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1929 - rmse: 1.1929 - val_loss: 0.6500 - val_rmse: 0.6500\n",
      "Epoch 69/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2854 - rmse: 1.2854 - val_loss: 0.6268 - val_rmse: 0.6268\n",
      "Epoch 70/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2315 - rmse: 1.2315 - val_loss: 0.6084 - val_rmse: 0.6084\n",
      "Epoch 71/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1921 - rmse: 1.1921 - val_loss: 0.6363 - val_rmse: 0.6363\n",
      "Epoch 72/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2072 - rmse: 1.2072 - val_loss: 0.5939 - val_rmse: 0.5939\n",
      "Epoch 73/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2119 - rmse: 1.2119 - val_loss: 0.6001 - val_rmse: 0.6001\n",
      "Epoch 74/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2420 - rmse: 1.2420 - val_loss: 0.6101 - val_rmse: 0.6101\n",
      "Epoch 75/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2091 - rmse: 1.2091 - val_loss: 0.5837 - val_rmse: 0.5837\n",
      "Epoch 76/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1860 - rmse: 1.1860 - val_loss: 0.5817 - val_rmse: 0.5817\n",
      "Epoch 77/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1906 - rmse: 1.1906 - val_loss: 0.5945 - val_rmse: 0.5945\n",
      "Epoch 78/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1903 - rmse: 1.1903 - val_loss: 0.6031 - val_rmse: 0.6031\n",
      "Epoch 79/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.2017 - rmse: 1.2017 - val_loss: 0.5761 - val_rmse: 0.5761\n",
      "Epoch 80/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.2029 - rmse: 1.2029 - val_loss: 0.5939 - val_rmse: 0.5939\n",
      "Epoch 81/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 1.1777 - rmse: 1.1777 - val_loss: 0.6366 - val_rmse: 0.6366\n",
      "Epoch 82/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1716 - rmse: 1.1716 - val_loss: 0.6179 - val_rmse: 0.6179\n",
      "Epoch 83/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1688 - rmse: 1.1688 - val_loss: 0.6524 - val_rmse: 0.6524\n",
      "Epoch 84/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2093 - rmse: 1.2093 - val_loss: 0.6069 - val_rmse: 0.6069\n",
      "Epoch 85/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.1558 - rmse: 1.1558 - val_loss: 0.6276 - val_rmse: 0.6276\n",
      "Epoch 86/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.2250 - rmse: 1.2250 - val_loss: 0.5976 - val_rmse: 0.5976\n",
      "Epoch 87/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1805 - rmse: 1.1805 - val_loss: 0.5870 - val_rmse: 0.5870\n",
      "Epoch 88/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1765 - rmse: 1.1765 - val_loss: 0.5779 - val_rmse: 0.5779\n",
      "Epoch 89/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1780 - rmse: 1.1780 - val_loss: 0.5735 - val_rmse: 0.5735\n",
      "Epoch 90/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.1590 - rmse: 1.1590 - val_loss: 0.5601 - val_rmse: 0.5601\n",
      "Epoch 91/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1796 - rmse: 1.1796 - val_loss: 0.6049 - val_rmse: 0.6049\n",
      "Epoch 92/1000\n",
      "1166/1166 [==============================] - 0s 21us/step - loss: 1.1418 - rmse: 1.1418 - val_loss: 0.5618 - val_rmse: 0.5618\n",
      "Epoch 93/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1664 - rmse: 1.1664 - val_loss: 0.5819 - val_rmse: 0.5819\n",
      "Epoch 94/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.1462 - rmse: 1.1462 - val_loss: 0.5688 - val_rmse: 0.5688\n",
      "Epoch 95/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1294 - rmse: 1.1294 - val_loss: 0.5709 - val_rmse: 0.5709\n",
      "Epoch 96/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1566 - rmse: 1.1566 - val_loss: 0.5776 - val_rmse: 0.5776\n",
      "Epoch 97/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1527 - rmse: 1.1527 - val_loss: 0.5300 - val_rmse: 0.5300\n",
      "Epoch 98/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0934 - rmse: 1.0934 - val_loss: 0.5459 - val_rmse: 0.5459\n",
      "Epoch 99/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.1244 - rmse: 1.1244 - val_loss: 0.5397 - val_rmse: 0.5397\n",
      "Epoch 100/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1407 - rmse: 1.1407 - val_loss: 0.5760 - val_rmse: 0.5760\n",
      "Epoch 101/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1329 - rmse: 1.1329 - val_loss: 0.5646 - val_rmse: 0.5646\n",
      "Epoch 102/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1140 - rmse: 1.1140 - val_loss: 0.5593 - val_rmse: 0.5593\n",
      "Epoch 103/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1254 - rmse: 1.1254 - val_loss: 0.5696 - val_rmse: 0.5696\n",
      "Epoch 104/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1356 - rmse: 1.1356 - val_loss: 0.5537 - val_rmse: 0.5537\n",
      "Epoch 105/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.1422 - rmse: 1.1422 - val_loss: 0.5413 - val_rmse: 0.5413\n",
      "Epoch 106/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1137 - rmse: 1.1137 - val_loss: 0.5510 - val_rmse: 0.5510\n",
      "Epoch 107/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.1222 - rmse: 1.1222 - val_loss: 0.5201 - val_rmse: 0.5201\n",
      "Epoch 108/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0826 - rmse: 1.0826 - val_loss: 0.5213 - val_rmse: 0.5213\n",
      "Epoch 109/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1228 - rmse: 1.1228 - val_loss: 0.5496 - val_rmse: 0.5496\n",
      "Epoch 110/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0891 - rmse: 1.0891 - val_loss: 0.5310 - val_rmse: 0.5310\n",
      "Epoch 111/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1115 - rmse: 1.1115 - val_loss: 0.5260 - val_rmse: 0.5260\n",
      "Epoch 112/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0918 - rmse: 1.0918 - val_loss: 0.5044 - val_rmse: 0.5044\n",
      "Epoch 113/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0695 - rmse: 1.0695 - val_loss: 0.5212 - val_rmse: 0.5212\n",
      "Epoch 114/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1057 - rmse: 1.1057 - val_loss: 0.5381 - val_rmse: 0.5381\n",
      "Epoch 115/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0750 - rmse: 1.0750 - val_loss: 0.5051 - val_rmse: 0.5051\n",
      "Epoch 116/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1007 - rmse: 1.1007 - val_loss: 0.5171 - val_rmse: 0.5171\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 16us/step - loss: 1.0829 - rmse: 1.0829 - val_loss: 0.5073 - val_rmse: 0.5073\n",
      "Epoch 118/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0887 - rmse: 1.0887 - val_loss: 0.5203 - val_rmse: 0.5203\n",
      "Epoch 119/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0799 - rmse: 1.0799 - val_loss: 0.5547 - val_rmse: 0.5547\n",
      "Epoch 120/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 1.0803 - rmse: 1.0803 - val_loss: 0.5197 - val_rmse: 0.5197\n",
      "Epoch 121/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.1033 - rmse: 1.1033 - val_loss: 0.5046 - val_rmse: 0.5046\n",
      "Epoch 122/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 1.0821 - rmse: 1.0821 - val_loss: 0.5425 - val_rmse: 0.5425\n",
      "Epoch 123/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0545 - rmse: 1.0545 - val_loss: 0.4903 - val_rmse: 0.4903\n",
      "Epoch 124/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0859 - rmse: 1.0859 - val_loss: 0.5194 - val_rmse: 0.5194\n",
      "Epoch 125/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0970 - rmse: 1.0970 - val_loss: 0.4868 - val_rmse: 0.4868\n",
      "Epoch 126/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0502 - rmse: 1.0502 - val_loss: 0.4988 - val_rmse: 0.4988\n",
      "Epoch 127/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0333 - rmse: 1.0333 - val_loss: 0.4827 - val_rmse: 0.4827\n",
      "Epoch 128/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 1.0790 - rmse: 1.0790 - val_loss: 0.5167 - val_rmse: 0.5167\n",
      "Epoch 129/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0901 - rmse: 1.0901 - val_loss: 0.5208 - val_rmse: 0.5208\n",
      "Epoch 130/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0619 - rmse: 1.0619 - val_loss: 0.5180 - val_rmse: 0.5180\n",
      "Epoch 131/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0226 - rmse: 1.0226 - val_loss: 0.5079 - val_rmse: 0.5079\n",
      "Epoch 132/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0586 - rmse: 1.0586 - val_loss: 0.4799 - val_rmse: 0.4799\n",
      "Epoch 133/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0213 - rmse: 1.0213 - val_loss: 0.5066 - val_rmse: 0.5066\n",
      "Epoch 134/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0602 - rmse: 1.0602 - val_loss: 0.4857 - val_rmse: 0.4857\n",
      "Epoch 135/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0810 - rmse: 1.0810 - val_loss: 0.4915 - val_rmse: 0.4915\n",
      "Epoch 136/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 1.0050 - rmse: 1.0050 - val_loss: 0.4974 - val_rmse: 0.4974\n",
      "Epoch 137/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0468 - rmse: 1.0468 - val_loss: 0.4847 - val_rmse: 0.4847\n",
      "Epoch 138/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0082 - rmse: 1.0082 - val_loss: 0.4795 - val_rmse: 0.4795\n",
      "Epoch 139/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 1.0229 - rmse: 1.0229 - val_loss: 0.5104 - val_rmse: 0.5104\n",
      "Epoch 140/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0288 - rmse: 1.0288 - val_loss: 0.5372 - val_rmse: 0.5372\n",
      "Epoch 141/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0391 - rmse: 1.0391 - val_loss: 0.5107 - val_rmse: 0.5107\n",
      "Epoch 142/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0314 - rmse: 1.0314 - val_loss: 0.4811 - val_rmse: 0.4811\n",
      "Epoch 143/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0569 - rmse: 1.0569 - val_loss: 0.4741 - val_rmse: 0.4741\n",
      "Epoch 144/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0106 - rmse: 1.0106 - val_loss: 0.4701 - val_rmse: 0.4701\n",
      "Epoch 145/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9967 - rmse: 0.9967 - val_loss: 0.4822 - val_rmse: 0.4822\n",
      "Epoch 146/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0636 - rmse: 1.0636 - val_loss: 0.4637 - val_rmse: 0.4637\n",
      "Epoch 147/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9976 - rmse: 0.9976 - val_loss: 0.4715 - val_rmse: 0.4715\n",
      "Epoch 148/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 1.0318 - rmse: 1.0318 - val_loss: 0.4671 - val_rmse: 0.4671\n",
      "Epoch 149/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0303 - rmse: 1.0303 - val_loss: 0.4672 - val_rmse: 0.4672\n",
      "Epoch 150/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.9869 - rmse: 0.9869 - val_loss: 0.4647 - val_rmse: 0.4647\n",
      "Epoch 151/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9835 - rmse: 0.9835 - val_loss: 0.4718 - val_rmse: 0.4718\n",
      "Epoch 152/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.0161 - rmse: 1.0161 - val_loss: 0.4616 - val_rmse: 0.4616\n",
      "Epoch 153/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0184 - rmse: 1.0184 - val_loss: 0.4609 - val_rmse: 0.4609\n",
      "Epoch 154/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0006 - rmse: 1.0006 - val_loss: 0.5106 - val_rmse: 0.5106\n",
      "Epoch 155/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 1.0162 - rmse: 1.0162 - val_loss: 0.4470 - val_rmse: 0.4470\n",
      "Epoch 156/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0340 - rmse: 1.0340 - val_loss: 0.4562 - val_rmse: 0.4562\n",
      "Epoch 157/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9890 - rmse: 0.9890 - val_loss: 0.4676 - val_rmse: 0.4676\n",
      "Epoch 158/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9566 - rmse: 0.9566 - val_loss: 0.4556 - val_rmse: 0.4556\n",
      "Epoch 159/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0196 - rmse: 1.0196 - val_loss: 0.4704 - val_rmse: 0.4704\n",
      "Epoch 160/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9846 - rmse: 0.9846 - val_loss: 0.4983 - val_rmse: 0.4983\n",
      "Epoch 161/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9776 - rmse: 0.9776 - val_loss: 0.4788 - val_rmse: 0.4788\n",
      "Epoch 162/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0229 - rmse: 1.0229 - val_loss: 0.4645 - val_rmse: 0.4645\n",
      "Epoch 163/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0083 - rmse: 1.0083 - val_loss: 0.4726 - val_rmse: 0.4726\n",
      "Epoch 164/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9571 - rmse: 0.9571 - val_loss: 0.4343 - val_rmse: 0.4343\n",
      "Epoch 165/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 1.0014 - rmse: 1.0014 - val_loss: 0.4621 - val_rmse: 0.4621\n",
      "Epoch 166/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9520 - rmse: 0.9520 - val_loss: 0.4686 - val_rmse: 0.4686\n",
      "Epoch 167/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9422 - rmse: 0.9422 - val_loss: 0.4690 - val_rmse: 0.4690\n",
      "Epoch 168/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9831 - rmse: 0.9831 - val_loss: 0.4786 - val_rmse: 0.4786\n",
      "Epoch 169/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9550 - rmse: 0.9550 - val_loss: 0.4658 - val_rmse: 0.4658\n",
      "Epoch 170/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9589 - rmse: 0.9589 - val_loss: 0.4363 - val_rmse: 0.4363\n",
      "Epoch 171/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 1.0044 - rmse: 1.0044 - val_loss: 0.4598 - val_rmse: 0.4598\n",
      "Epoch 172/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9924 - rmse: 0.9924 - val_loss: 0.4488 - val_rmse: 0.4488\n",
      "Epoch 173/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9077 - rmse: 0.9077 - val_loss: 0.4589 - val_rmse: 0.4589\n",
      "Epoch 174/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9537 - rmse: 0.9537 - val_loss: 0.4727 - val_rmse: 0.4727\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9616 - rmse: 0.9616 - val_loss: 0.4384 - val_rmse: 0.4384\n",
      "Epoch 176/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9410 - rmse: 0.9410 - val_loss: 0.4549 - val_rmse: 0.4549\n",
      "Epoch 177/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9599 - rmse: 0.9599 - val_loss: 0.4549 - val_rmse: 0.4549\n",
      "Epoch 178/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9330 - rmse: 0.9330 - val_loss: 0.4383 - val_rmse: 0.4383\n",
      "Epoch 179/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9294 - rmse: 0.9294 - val_loss: 0.4234 - val_rmse: 0.4234\n",
      "Epoch 180/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9358 - rmse: 0.9358 - val_loss: 0.4315 - val_rmse: 0.4315\n",
      "Epoch 181/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9637 - rmse: 0.9637 - val_loss: 0.4185 - val_rmse: 0.4185\n",
      "Epoch 182/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9500 - rmse: 0.9500 - val_loss: 0.4093 - val_rmse: 0.4093\n",
      "Epoch 183/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9558 - rmse: 0.9558 - val_loss: 0.4498 - val_rmse: 0.4498\n",
      "Epoch 184/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9725 - rmse: 0.9725 - val_loss: 0.4310 - val_rmse: 0.4310\n",
      "Epoch 185/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8988 - rmse: 0.8988 - val_loss: 0.4170 - val_rmse: 0.4170\n",
      "Epoch 186/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9367 - rmse: 0.9367 - val_loss: 0.4028 - val_rmse: 0.4028\n",
      "Epoch 187/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9384 - rmse: 0.9384 - val_loss: 0.4056 - val_rmse: 0.4056\n",
      "Epoch 188/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9223 - rmse: 0.9223 - val_loss: 0.4052 - val_rmse: 0.4052\n",
      "Epoch 189/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9248 - rmse: 0.9248 - val_loss: 0.4685 - val_rmse: 0.4685\n",
      "Epoch 190/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9177 - rmse: 0.9177 - val_loss: 0.4485 - val_rmse: 0.4485\n",
      "Epoch 191/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9124 - rmse: 0.9124 - val_loss: 0.4517 - val_rmse: 0.4517\n",
      "Epoch 192/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8866 - rmse: 0.8866 - val_loss: 0.4348 - val_rmse: 0.4348\n",
      "Epoch 193/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8972 - rmse: 0.8972 - val_loss: 0.4680 - val_rmse: 0.4680\n",
      "Epoch 194/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8904 - rmse: 0.8904 - val_loss: 0.4134 - val_rmse: 0.4134\n",
      "Epoch 195/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9073 - rmse: 0.9073 - val_loss: 0.4498 - val_rmse: 0.4498\n",
      "Epoch 196/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9171 - rmse: 0.9171 - val_loss: 0.4253 - val_rmse: 0.4253\n",
      "Epoch 197/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9169 - rmse: 0.9169 - val_loss: 0.4171 - val_rmse: 0.4171\n",
      "Epoch 198/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9217 - rmse: 0.9217 - val_loss: 0.4130 - val_rmse: 0.4130\n",
      "Epoch 199/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8893 - rmse: 0.8893 - val_loss: 0.4253 - val_rmse: 0.4253\n",
      "Epoch 200/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9076 - rmse: 0.9076 - val_loss: 0.4167 - val_rmse: 0.4167\n",
      "Epoch 201/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8804 - rmse: 0.8804 - val_loss: 0.4321 - val_rmse: 0.4321\n",
      "Epoch 202/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9007 - rmse: 0.9007 - val_loss: 0.4070 - val_rmse: 0.4070\n",
      "Epoch 203/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8425 - rmse: 0.8425 - val_loss: 0.3936 - val_rmse: 0.3936\n",
      "Epoch 204/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.9176 - rmse: 0.9176 - val_loss: 0.3977 - val_rmse: 0.3977\n",
      "Epoch 205/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8941 - rmse: 0.8941 - val_loss: 0.3987 - val_rmse: 0.3987\n",
      "Epoch 206/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8854 - rmse: 0.8854 - val_loss: 0.4327 - val_rmse: 0.4327\n",
      "Epoch 207/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9080 - rmse: 0.9080 - val_loss: 0.3999 - val_rmse: 0.3999\n",
      "Epoch 208/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9038 - rmse: 0.9038 - val_loss: 0.4474 - val_rmse: 0.4474\n",
      "Epoch 209/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9095 - rmse: 0.9095 - val_loss: 0.4020 - val_rmse: 0.4020\n",
      "Epoch 210/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8743 - rmse: 0.8743 - val_loss: 0.4090 - val_rmse: 0.4090\n",
      "Epoch 211/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8615 - rmse: 0.8615 - val_loss: 0.3804 - val_rmse: 0.3804\n",
      "Epoch 212/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8995 - rmse: 0.8995 - val_loss: 0.4051 - val_rmse: 0.4051\n",
      "Epoch 213/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8870 - rmse: 0.8870 - val_loss: 0.3824 - val_rmse: 0.3824\n",
      "Epoch 214/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.9175 - rmse: 0.9175 - val_loss: 0.3704 - val_rmse: 0.3704\n",
      "Epoch 215/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8661 - rmse: 0.8661 - val_loss: 0.3768 - val_rmse: 0.3768\n",
      "Epoch 216/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.9021 - rmse: 0.9021 - val_loss: 0.3804 - val_rmse: 0.3804\n",
      "Epoch 217/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.9105 - rmse: 0.9105 - val_loss: 0.3763 - val_rmse: 0.3763\n",
      "Epoch 218/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8450 - rmse: 0.8450 - val_loss: 0.3699 - val_rmse: 0.3699\n",
      "Epoch 219/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8743 - rmse: 0.8743 - val_loss: 0.3783 - val_rmse: 0.3783\n",
      "Epoch 220/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8603 - rmse: 0.8603 - val_loss: 0.3838 - val_rmse: 0.3838\n",
      "Epoch 221/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8714 - rmse: 0.8714 - val_loss: 0.4136 - val_rmse: 0.4136\n",
      "Epoch 222/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8465 - rmse: 0.8465 - val_loss: 0.3847 - val_rmse: 0.3847\n",
      "Epoch 223/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8587 - rmse: 0.8587 - val_loss: 0.3887 - val_rmse: 0.3887\n",
      "Epoch 224/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8898 - rmse: 0.8898 - val_loss: 0.3782 - val_rmse: 0.3782\n",
      "Epoch 225/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8715 - rmse: 0.8715 - val_loss: 0.4106 - val_rmse: 0.4106\n",
      "Epoch 226/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8573 - rmse: 0.8573 - val_loss: 0.3797 - val_rmse: 0.3797\n",
      "Epoch 227/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8779 - rmse: 0.8779 - val_loss: 0.3997 - val_rmse: 0.3997\n",
      "Epoch 228/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.8850 - rmse: 0.8850 - val_loss: 0.3935 - val_rmse: 0.3935\n",
      "Epoch 229/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8479 - rmse: 0.8479 - val_loss: 0.3546 - val_rmse: 0.3546\n",
      "Epoch 230/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8594 - rmse: 0.8594 - val_loss: 0.3614 - val_rmse: 0.3614\n",
      "Epoch 231/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8615 - rmse: 0.8615 - val_loss: 0.3804 - val_rmse: 0.3804\n",
      "Epoch 232/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8693 - rmse: 0.8693 - val_loss: 0.3950 - val_rmse: 0.3950\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8075 - rmse: 0.8075 - val_loss: 0.3691 - val_rmse: 0.3691\n",
      "Epoch 234/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8402 - rmse: 0.8402 - val_loss: 0.4004 - val_rmse: 0.4004\n",
      "Epoch 235/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8561 - rmse: 0.8561 - val_loss: 0.3776 - val_rmse: 0.3776\n",
      "Epoch 236/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8340 - rmse: 0.8340 - val_loss: 0.4067 - val_rmse: 0.4067\n",
      "Epoch 237/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8407 - rmse: 0.8407 - val_loss: 0.3934 - val_rmse: 0.3934\n",
      "Epoch 238/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.8354 - rmse: 0.8354 - val_loss: 0.4355 - val_rmse: 0.4355\n",
      "Epoch 239/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8423 - rmse: 0.8423 - val_loss: 0.4313 - val_rmse: 0.4313\n",
      "Epoch 240/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8331 - rmse: 0.8331 - val_loss: 0.3956 - val_rmse: 0.3956\n",
      "Epoch 241/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8642 - rmse: 0.8642 - val_loss: 0.3605 - val_rmse: 0.3605\n",
      "Epoch 242/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8335 - rmse: 0.8335 - val_loss: 0.4013 - val_rmse: 0.4013\n",
      "Epoch 243/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8267 - rmse: 0.8267 - val_loss: 0.3775 - val_rmse: 0.3775\n",
      "Epoch 244/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.8401 - rmse: 0.8401 - val_loss: 0.3660 - val_rmse: 0.3660\n",
      "Epoch 245/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8188 - rmse: 0.8188 - val_loss: 0.3834 - val_rmse: 0.3834\n",
      "Epoch 246/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8562 - rmse: 0.8562 - val_loss: 0.3568 - val_rmse: 0.3568\n",
      "Epoch 247/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7931 - rmse: 0.7931 - val_loss: 0.3578 - val_rmse: 0.3578\n",
      "Epoch 248/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8302 - rmse: 0.8302 - val_loss: 0.3474 - val_rmse: 0.3474\n",
      "Epoch 249/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8046 - rmse: 0.8046 - val_loss: 0.3559 - val_rmse: 0.3559\n",
      "Epoch 250/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8096 - rmse: 0.8096 - val_loss: 0.3434 - val_rmse: 0.3434\n",
      "Epoch 251/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.8029 - rmse: 0.8029 - val_loss: 0.3389 - val_rmse: 0.3389\n",
      "Epoch 252/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8285 - rmse: 0.8285 - val_loss: 0.3381 - val_rmse: 0.3381\n",
      "Epoch 253/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8157 - rmse: 0.8157 - val_loss: 0.3853 - val_rmse: 0.3853\n",
      "Epoch 254/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8005 - rmse: 0.8005 - val_loss: 0.3673 - val_rmse: 0.3673\n",
      "Epoch 255/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.8323 - rmse: 0.8323 - val_loss: 0.4016 - val_rmse: 0.4016\n",
      "Epoch 256/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8028 - rmse: 0.8028 - val_loss: 0.3672 - val_rmse: 0.3672\n",
      "Epoch 257/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8028 - rmse: 0.8028 - val_loss: 0.4037 - val_rmse: 0.4037\n",
      "Epoch 258/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8103 - rmse: 0.8103 - val_loss: 0.3495 - val_rmse: 0.3495\n",
      "Epoch 259/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7915 - rmse: 0.7915 - val_loss: 0.3525 - val_rmse: 0.3525\n",
      "Epoch 260/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.8037 - rmse: 0.8037 - val_loss: 0.3454 - val_rmse: 0.3454\n",
      "Epoch 261/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.8305 - rmse: 0.8305 - val_loss: 0.3765 - val_rmse: 0.3765\n",
      "Epoch 262/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.8128 - rmse: 0.8128 - val_loss: 0.3638 - val_rmse: 0.3638\n",
      "Epoch 263/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7851 - rmse: 0.7851 - val_loss: 0.3853 - val_rmse: 0.3853\n",
      "Epoch 264/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7849 - rmse: 0.7849 - val_loss: 0.3784 - val_rmse: 0.3784\n",
      "Epoch 265/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7735 - rmse: 0.7735 - val_loss: 0.3348 - val_rmse: 0.3348\n",
      "Epoch 266/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8238 - rmse: 0.8238 - val_loss: 0.3747 - val_rmse: 0.3747\n",
      "Epoch 267/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8078 - rmse: 0.8078 - val_loss: 0.3636 - val_rmse: 0.3636\n",
      "Epoch 268/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.8018 - rmse: 0.8018 - val_loss: 0.3540 - val_rmse: 0.3540\n",
      "Epoch 269/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.8034 - rmse: 0.8034 - val_loss: 0.3515 - val_rmse: 0.3515\n",
      "Epoch 270/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7825 - rmse: 0.7825 - val_loss: 0.3641 - val_rmse: 0.3641\n",
      "Epoch 271/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.7914 - rmse: 0.79 - 0s 14us/step - loss: 0.7806 - rmse: 0.7806 - val_loss: 0.3384 - val_rmse: 0.3384\n",
      "Epoch 272/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7937 - rmse: 0.7937 - val_loss: 0.3320 - val_rmse: 0.3320\n",
      "Epoch 273/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7982 - rmse: 0.7982 - val_loss: 0.3758 - val_rmse: 0.3758\n",
      "Epoch 274/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7670 - rmse: 0.7670 - val_loss: 0.3574 - val_rmse: 0.3574\n",
      "Epoch 275/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7976 - rmse: 0.7976 - val_loss: 0.3443 - val_rmse: 0.3443\n",
      "Epoch 276/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7694 - rmse: 0.7694 - val_loss: 0.3712 - val_rmse: 0.3712\n",
      "Epoch 277/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.7842 - rmse: 0.7842 - val_loss: 0.3577 - val_rmse: 0.3577\n",
      "Epoch 278/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7892 - rmse: 0.7892 - val_loss: 0.3365 - val_rmse: 0.3365\n",
      "Epoch 279/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7804 - rmse: 0.7804 - val_loss: 0.3185 - val_rmse: 0.3185\n",
      "Epoch 280/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7969 - rmse: 0.7969 - val_loss: 0.3461 - val_rmse: 0.3461\n",
      "Epoch 281/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7777 - rmse: 0.7777 - val_loss: 0.3875 - val_rmse: 0.3875\n",
      "Epoch 282/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7644 - rmse: 0.7644 - val_loss: 0.3860 - val_rmse: 0.3860\n",
      "Epoch 283/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7720 - rmse: 0.7720 - val_loss: 0.3161 - val_rmse: 0.3161\n",
      "Epoch 284/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7769 - rmse: 0.7769 - val_loss: 0.3473 - val_rmse: 0.3473\n",
      "Epoch 285/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7678 - rmse: 0.7678 - val_loss: 0.3335 - val_rmse: 0.3335\n",
      "Epoch 286/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7565 - rmse: 0.7565 - val_loss: 0.3135 - val_rmse: 0.3135\n",
      "Epoch 287/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7551 - rmse: 0.7551 - val_loss: 0.3659 - val_rmse: 0.3659\n",
      "Epoch 288/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7787 - rmse: 0.7787 - val_loss: 0.3262 - val_rmse: 0.3262\n",
      "Epoch 289/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7536 - rmse: 0.7536 - val_loss: 0.3343 - val_rmse: 0.3343\n",
      "Epoch 290/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7210 - rmse: 0.7210 - val_loss: 0.3157 - val_rmse: 0.3157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7593 - rmse: 0.7593 - val_loss: 0.3277 - val_rmse: 0.3277\n",
      "Epoch 292/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7483 - rmse: 0.7483 - val_loss: 0.3454 - val_rmse: 0.3454\n",
      "Epoch 293/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7531 - rmse: 0.7531 - val_loss: 0.3504 - val_rmse: 0.3504\n",
      "Epoch 294/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7424 - rmse: 0.7424 - val_loss: 0.3495 - val_rmse: 0.3495\n",
      "Epoch 295/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7446 - rmse: 0.7446 - val_loss: 0.3238 - val_rmse: 0.3238\n",
      "Epoch 296/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7307 - rmse: 0.7307 - val_loss: 0.3347 - val_rmse: 0.3347\n",
      "Epoch 297/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7720 - rmse: 0.7720 - val_loss: 0.3016 - val_rmse: 0.3016\n",
      "Epoch 298/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7319 - rmse: 0.7319 - val_loss: 0.3135 - val_rmse: 0.3135\n",
      "Epoch 299/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7253 - rmse: 0.7253 - val_loss: 0.3222 - val_rmse: 0.3222\n",
      "Epoch 300/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7598 - rmse: 0.7598 - val_loss: 0.3420 - val_rmse: 0.3420\n",
      "Epoch 301/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7305 - rmse: 0.7305 - val_loss: 0.3519 - val_rmse: 0.3519\n",
      "Epoch 302/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7274 - rmse: 0.7274 - val_loss: 0.3096 - val_rmse: 0.3096\n",
      "Epoch 303/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7446 - rmse: 0.7446 - val_loss: 0.3233 - val_rmse: 0.3233\n",
      "Epoch 304/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7421 - rmse: 0.7421 - val_loss: 0.3167 - val_rmse: 0.3167\n",
      "Epoch 305/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7472 - rmse: 0.7472 - val_loss: 0.3163 - val_rmse: 0.3163\n",
      "Epoch 306/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7209 - rmse: 0.7209 - val_loss: 0.2944 - val_rmse: 0.2944\n",
      "Epoch 307/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7351 - rmse: 0.7351 - val_loss: 0.2998 - val_rmse: 0.2998\n",
      "Epoch 308/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7193 - rmse: 0.7193 - val_loss: 0.3173 - val_rmse: 0.3173\n",
      "Epoch 309/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7093 - rmse: 0.7093 - val_loss: 0.3052 - val_rmse: 0.3052\n",
      "Epoch 310/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7360 - rmse: 0.7360 - val_loss: 0.3154 - val_rmse: 0.3154\n",
      "Epoch 311/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7281 - rmse: 0.7281 - val_loss: 0.3134 - val_rmse: 0.3134\n",
      "Epoch 312/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7194 - rmse: 0.7194 - val_loss: 0.3257 - val_rmse: 0.3257\n",
      "Epoch 313/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.7169 - rmse: 0.7169 - val_loss: 0.3187 - val_rmse: 0.3187\n",
      "Epoch 314/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6911 - rmse: 0.6911 - val_loss: 0.3309 - val_rmse: 0.3309\n",
      "Epoch 315/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7218 - rmse: 0.7218 - val_loss: 0.3101 - val_rmse: 0.3101\n",
      "Epoch 316/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7387 - rmse: 0.7387 - val_loss: 0.3046 - val_rmse: 0.3046\n",
      "Epoch 317/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7001 - rmse: 0.7001 - val_loss: 0.3237 - val_rmse: 0.3237\n",
      "Epoch 318/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7221 - rmse: 0.7221 - val_loss: 0.3088 - val_rmse: 0.3088\n",
      "Epoch 319/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.7097 - rmse: 0.7097 - val_loss: 0.3250 - val_rmse: 0.3250\n",
      "Epoch 320/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.7071 - rmse: 0.7071 - val_loss: 0.3046 - val_rmse: 0.3046\n",
      "Epoch 321/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6901 - rmse: 0.6901 - val_loss: 0.3245 - val_rmse: 0.3245\n",
      "Epoch 322/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.7265 - rmse: 0.7265 - val_loss: 0.3245 - val_rmse: 0.3245\n",
      "Epoch 323/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6887 - rmse: 0.6887 - val_loss: 0.2851 - val_rmse: 0.2851\n",
      "Epoch 324/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6982 - rmse: 0.6982 - val_loss: 0.2884 - val_rmse: 0.2884\n",
      "Epoch 325/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.7030 - rmse: 0.7030 - val_loss: 0.3011 - val_rmse: 0.3011\n",
      "Epoch 326/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6746 - rmse: 0.6746 - val_loss: 0.3155 - val_rmse: 0.3155\n",
      "Epoch 327/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6975 - rmse: 0.6975 - val_loss: 0.3094 - val_rmse: 0.3094\n",
      "Epoch 328/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6909 - rmse: 0.6909 - val_loss: 0.2935 - val_rmse: 0.2935\n",
      "Epoch 329/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6726 - rmse: 0.6726 - val_loss: 0.3153 - val_rmse: 0.3153\n",
      "Epoch 330/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6837 - rmse: 0.6837 - val_loss: 0.3039 - val_rmse: 0.3039\n",
      "Epoch 331/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6850 - rmse: 0.6850 - val_loss: 0.2897 - val_rmse: 0.2897\n",
      "Epoch 332/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6622 - rmse: 0.6622 - val_loss: 0.3016 - val_rmse: 0.3016\n",
      "Epoch 333/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6935 - rmse: 0.6935 - val_loss: 0.3034 - val_rmse: 0.3034\n",
      "Epoch 334/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6921 - rmse: 0.6921 - val_loss: 0.2867 - val_rmse: 0.2867\n",
      "Epoch 335/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6943 - rmse: 0.6943 - val_loss: 0.2971 - val_rmse: 0.2971\n",
      "Epoch 336/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6867 - rmse: 0.6867 - val_loss: 0.2881 - val_rmse: 0.2881\n",
      "Epoch 337/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6957 - rmse: 0.6957 - val_loss: 0.3001 - val_rmse: 0.3001\n",
      "Epoch 338/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6739 - rmse: 0.6739 - val_loss: 0.2805 - val_rmse: 0.2805\n",
      "Epoch 339/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6860 - rmse: 0.6860 - val_loss: 0.3251 - val_rmse: 0.3251\n",
      "Epoch 340/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6520 - rmse: 0.6520 - val_loss: 0.3151 - val_rmse: 0.3151\n",
      "Epoch 341/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6564 - rmse: 0.6564 - val_loss: 0.3207 - val_rmse: 0.3207\n",
      "Epoch 342/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6831 - rmse: 0.6831 - val_loss: 0.3003 - val_rmse: 0.3003\n",
      "Epoch 343/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6552 - rmse: 0.6552 - val_loss: 0.3021 - val_rmse: 0.3021\n",
      "Epoch 344/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6949 - rmse: 0.6949 - val_loss: 0.3301 - val_rmse: 0.3301\n",
      "Epoch 345/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6393 - rmse: 0.6393 - val_loss: 0.3144 - val_rmse: 0.3144\n",
      "Epoch 346/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6841 - rmse: 0.6841 - val_loss: 0.3120 - val_rmse: 0.3120\n",
      "Epoch 347/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6461 - rmse: 0.6461 - val_loss: 0.2933 - val_rmse: 0.2933\n",
      "Epoch 348/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6794 - rmse: 0.6794 - val_loss: 0.2851 - val_rmse: 0.2851\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6571 - rmse: 0.6571 - val_loss: 0.2917 - val_rmse: 0.2917\n",
      "Epoch 350/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6771 - rmse: 0.6771 - val_loss: 0.3108 - val_rmse: 0.3108\n",
      "Epoch 351/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6683 - rmse: 0.6683 - val_loss: 0.2822 - val_rmse: 0.2822\n",
      "Epoch 352/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6334 - rmse: 0.6334 - val_loss: 0.2905 - val_rmse: 0.2905\n",
      "Epoch 353/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6555 - rmse: 0.6555 - val_loss: 0.2863 - val_rmse: 0.2863\n",
      "Epoch 354/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6392 - rmse: 0.6392 - val_loss: 0.3007 - val_rmse: 0.3007\n",
      "Epoch 355/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6376 - rmse: 0.6376 - val_loss: 0.3053 - val_rmse: 0.3053\n",
      "Epoch 356/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6547 - rmse: 0.6547 - val_loss: 0.3151 - val_rmse: 0.3151\n",
      "Epoch 357/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6474 - rmse: 0.6474 - val_loss: 0.2944 - val_rmse: 0.2944\n",
      "Epoch 358/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6621 - rmse: 0.6621 - val_loss: 0.3008 - val_rmse: 0.3008\n",
      "Epoch 359/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6601 - rmse: 0.6601 - val_loss: 0.3056 - val_rmse: 0.3056\n",
      "Epoch 360/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6283 - rmse: 0.6283 - val_loss: 0.2889 - val_rmse: 0.2889\n",
      "Epoch 361/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6455 - rmse: 0.6455 - val_loss: 0.2868 - val_rmse: 0.2868\n",
      "Epoch 362/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6218 - rmse: 0.6218 - val_loss: 0.2986 - val_rmse: 0.2986\n",
      "Epoch 363/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6296 - rmse: 0.6296 - val_loss: 0.3121 - val_rmse: 0.3121\n",
      "Epoch 364/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6249 - rmse: 0.6249 - val_loss: 0.3140 - val_rmse: 0.3140\n",
      "Epoch 365/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6292 - rmse: 0.6292 - val_loss: 0.2849 - val_rmse: 0.2849\n",
      "Epoch 366/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.6330 - rmse: 0.6330 - val_loss: 0.2950 - val_rmse: 0.2950\n",
      "Epoch 367/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6453 - rmse: 0.6453 - val_loss: 0.2882 - val_rmse: 0.2882\n",
      "Epoch 368/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6144 - rmse: 0.6144 - val_loss: 0.3196 - val_rmse: 0.3196\n",
      "Epoch 369/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6477 - rmse: 0.6477 - val_loss: 0.3035 - val_rmse: 0.3035\n",
      "Epoch 370/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6731 - rmse: 0.6731 - val_loss: 0.3344 - val_rmse: 0.3344\n",
      "Epoch 371/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6109 - rmse: 0.6109 - val_loss: 0.3241 - val_rmse: 0.3241\n",
      "Epoch 372/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6446 - rmse: 0.6446 - val_loss: 0.2959 - val_rmse: 0.2959\n",
      "Epoch 373/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6217 - rmse: 0.6217 - val_loss: 0.3030 - val_rmse: 0.3030\n",
      "Epoch 374/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5991 - rmse: 0.5991 - val_loss: 0.2751 - val_rmse: 0.2751\n",
      "Epoch 375/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6095 - rmse: 0.6095 - val_loss: 0.2587 - val_rmse: 0.2587\n",
      "Epoch 376/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6373 - rmse: 0.6373 - val_loss: 0.2731 - val_rmse: 0.2731\n",
      "Epoch 377/1000\n",
      "1166/1166 [==============================] - 0s 20us/step - loss: 0.6146 - rmse: 0.6146 - val_loss: 0.2953 - val_rmse: 0.2953\n",
      "Epoch 378/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.6168 - rmse: 0.6168 - val_loss: 0.2713 - val_rmse: 0.2713\n",
      "Epoch 379/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.6312 - rmse: 0.6312 - val_loss: 0.2680 - val_rmse: 0.2680\n",
      "Epoch 380/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6025 - rmse: 0.6025 - val_loss: 0.2784 - val_rmse: 0.2784\n",
      "Epoch 381/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6186 - rmse: 0.6186 - val_loss: 0.2796 - val_rmse: 0.2796\n",
      "Epoch 382/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6118 - rmse: 0.6118 - val_loss: 0.2935 - val_rmse: 0.2935\n",
      "Epoch 383/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6354 - rmse: 0.6354 - val_loss: 0.2570 - val_rmse: 0.2570\n",
      "Epoch 384/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6103 - rmse: 0.6103 - val_loss: 0.2630 - val_rmse: 0.2630\n",
      "Epoch 385/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.6006 - rmse: 0.6006 - val_loss: 0.2710 - val_rmse: 0.2710\n",
      "Epoch 386/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6019 - rmse: 0.6019 - val_loss: 0.2831 - val_rmse: 0.2831\n",
      "Epoch 387/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5958 - rmse: 0.5958 - val_loss: 0.2493 - val_rmse: 0.2493\n",
      "Epoch 388/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6093 - rmse: 0.6093 - val_loss: 0.2585 - val_rmse: 0.2585\n",
      "Epoch 389/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6103 - rmse: 0.6103 - val_loss: 0.2540 - val_rmse: 0.2540\n",
      "Epoch 390/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6088 - rmse: 0.6088 - val_loss: 0.2565 - val_rmse: 0.2565\n",
      "Epoch 391/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6105 - rmse: 0.6105 - val_loss: 0.2791 - val_rmse: 0.2791\n",
      "Epoch 392/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.6014 - rmse: 0.6014 - val_loss: 0.2586 - val_rmse: 0.2586\n",
      "Epoch 393/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.6018 - rmse: 0.6018 - val_loss: 0.2482 - val_rmse: 0.2482\n",
      "Epoch 394/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5914 - rmse: 0.5914 - val_loss: 0.2607 - val_rmse: 0.2607\n",
      "Epoch 395/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5999 - rmse: 0.5999 - val_loss: 0.2656 - val_rmse: 0.2656\n",
      "Epoch 396/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6062 - rmse: 0.6062 - val_loss: 0.2763 - val_rmse: 0.2763\n",
      "Epoch 397/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5951 - rmse: 0.5951 - val_loss: 0.2520 - val_rmse: 0.2520\n",
      "Epoch 398/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6072 - rmse: 0.6072 - val_loss: 0.2416 - val_rmse: 0.2416\n",
      "Epoch 399/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5992 - rmse: 0.5992 - val_loss: 0.2515 - val_rmse: 0.2515\n",
      "Epoch 400/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5762 - rmse: 0.5762 - val_loss: 0.2562 - val_rmse: 0.2562\n",
      "Epoch 401/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5842 - rmse: 0.5842 - val_loss: 0.2584 - val_rmse: 0.2584\n",
      "Epoch 402/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5817 - rmse: 0.5817 - val_loss: 0.2416 - val_rmse: 0.2416\n",
      "Epoch 403/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5720 - rmse: 0.5720 - val_loss: 0.2587 - val_rmse: 0.2587\n",
      "Epoch 404/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.6103 - rmse: 0.6103 - val_loss: 0.2526 - val_rmse: 0.2526\n",
      "Epoch 405/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5860 - rmse: 0.5860 - val_loss: 0.2945 - val_rmse: 0.2945\n",
      "Epoch 406/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5846 - rmse: 0.5846 - val_loss: 0.2496 - val_rmse: 0.2496\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5893 - rmse: 0.5893 - val_loss: 0.2498 - val_rmse: 0.2498\n",
      "Epoch 408/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5857 - rmse: 0.5857 - val_loss: 0.2524 - val_rmse: 0.2524\n",
      "Epoch 409/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.5872 - rmse: 0.5872 - val_loss: 0.2653 - val_rmse: 0.2653\n",
      "Epoch 410/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5687 - rmse: 0.5687 - val_loss: 0.2801 - val_rmse: 0.2801\n",
      "Epoch 411/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5738 - rmse: 0.5738 - val_loss: 0.2448 - val_rmse: 0.2448\n",
      "Epoch 412/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.6062 - rmse: 0.6062 - val_loss: 0.2567 - val_rmse: 0.2567\n",
      "Epoch 413/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5811 - rmse: 0.5811 - val_loss: 0.2779 - val_rmse: 0.2779\n",
      "Epoch 414/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5762 - rmse: 0.5762 - val_loss: 0.2550 - val_rmse: 0.2550\n",
      "Epoch 415/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5835 - rmse: 0.5835 - val_loss: 0.2377 - val_rmse: 0.2377\n",
      "Epoch 416/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5893 - rmse: 0.5893 - val_loss: 0.2797 - val_rmse: 0.2797\n",
      "Epoch 417/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5634 - rmse: 0.5634 - val_loss: 0.2273 - val_rmse: 0.2273\n",
      "Epoch 418/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5482 - rmse: 0.5482 - val_loss: 0.2555 - val_rmse: 0.2555\n",
      "Epoch 419/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5733 - rmse: 0.5733 - val_loss: 0.2564 - val_rmse: 0.2564\n",
      "Epoch 420/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5850 - rmse: 0.5850 - val_loss: 0.2273 - val_rmse: 0.2273\n",
      "Epoch 421/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5594 - rmse: 0.5594 - val_loss: 0.2341 - val_rmse: 0.2341\n",
      "Epoch 422/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5790 - rmse: 0.5790 - val_loss: 0.2436 - val_rmse: 0.2436\n",
      "Epoch 423/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5581 - rmse: 0.5581 - val_loss: 0.2555 - val_rmse: 0.2555\n",
      "Epoch 424/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5767 - rmse: 0.5767 - val_loss: 0.2351 - val_rmse: 0.2351\n",
      "Epoch 425/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5796 - rmse: 0.5796 - val_loss: 0.2288 - val_rmse: 0.2288\n",
      "Epoch 426/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.5532 - rmse: 0.5532 - val_loss: 0.2422 - val_rmse: 0.2422\n",
      "Epoch 427/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5592 - rmse: 0.5592 - val_loss: 0.2598 - val_rmse: 0.2598\n",
      "Epoch 428/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5757 - rmse: 0.5757 - val_loss: 0.2295 - val_rmse: 0.2295\n",
      "Epoch 429/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5686 - rmse: 0.5686 - val_loss: 0.2489 - val_rmse: 0.2489\n",
      "Epoch 430/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5341 - rmse: 0.5341 - val_loss: 0.2354 - val_rmse: 0.2354\n",
      "Epoch 431/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5325 - rmse: 0.5325 - val_loss: 0.2181 - val_rmse: 0.2181\n",
      "Epoch 432/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5696 - rmse: 0.5696 - val_loss: 0.2454 - val_rmse: 0.2454\n",
      "Epoch 433/1000\n",
      "1166/1166 [==============================] - 0s 21us/step - loss: 0.5536 - rmse: 0.5536 - val_loss: 0.2362 - val_rmse: 0.2362\n",
      "Epoch 434/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5386 - rmse: 0.5386 - val_loss: 0.2345 - val_rmse: 0.2345\n",
      "Epoch 435/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5502 - rmse: 0.5502 - val_loss: 0.2528 - val_rmse: 0.2528\n",
      "Epoch 436/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.5441 - rmse: 0.5441 - val_loss: 0.2293 - val_rmse: 0.2293\n",
      "Epoch 437/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.5514 - rmse: 0.5514 - val_loss: 0.2538 - val_rmse: 0.2538\n",
      "Epoch 438/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5668 - rmse: 0.5668 - val_loss: 0.2437 - val_rmse: 0.2437\n",
      "Epoch 439/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5527 - rmse: 0.5527 - val_loss: 0.2407 - val_rmse: 0.2407\n",
      "Epoch 440/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5409 - rmse: 0.5409 - val_loss: 0.2306 - val_rmse: 0.2306\n",
      "Epoch 441/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5558 - rmse: 0.5558 - val_loss: 0.2819 - val_rmse: 0.2819\n",
      "Epoch 442/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5308 - rmse: 0.5308 - val_loss: 0.2246 - val_rmse: 0.2246\n",
      "Epoch 443/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5364 - rmse: 0.5364 - val_loss: 0.2479 - val_rmse: 0.2479\n",
      "Epoch 444/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5386 - rmse: 0.5386 - val_loss: 0.2237 - val_rmse: 0.2237\n",
      "Epoch 445/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5572 - rmse: 0.5572 - val_loss: 0.2801 - val_rmse: 0.2801\n",
      "Epoch 446/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5474 - rmse: 0.5474 - val_loss: 0.2540 - val_rmse: 0.2540\n",
      "Epoch 447/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5204 - rmse: 0.5204 - val_loss: 0.2314 - val_rmse: 0.2314\n",
      "Epoch 448/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5393 - rmse: 0.5393 - val_loss: 0.2237 - val_rmse: 0.2237\n",
      "Epoch 449/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5128 - rmse: 0.5128 - val_loss: 0.2438 - val_rmse: 0.2438\n",
      "Epoch 450/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5414 - rmse: 0.5414 - val_loss: 0.2308 - val_rmse: 0.2308\n",
      "Epoch 451/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5267 - rmse: 0.5267 - val_loss: 0.2508 - val_rmse: 0.2508\n",
      "Epoch 452/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5220 - rmse: 0.5220 - val_loss: 0.2198 - val_rmse: 0.2198\n",
      "Epoch 453/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5177 - rmse: 0.5177 - val_loss: 0.2531 - val_rmse: 0.2531\n",
      "Epoch 454/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5389 - rmse: 0.5389 - val_loss: 0.2231 - val_rmse: 0.2231\n",
      "Epoch 455/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5320 - rmse: 0.5320 - val_loss: 0.2108 - val_rmse: 0.2108\n",
      "Epoch 456/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5164 - rmse: 0.5164 - val_loss: 0.2291 - val_rmse: 0.2291\n",
      "Epoch 457/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5505 - rmse: 0.5505 - val_loss: 0.2187 - val_rmse: 0.2187\n",
      "Epoch 458/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5040 - rmse: 0.5040 - val_loss: 0.2465 - val_rmse: 0.2465\n",
      "Epoch 459/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5380 - rmse: 0.5380 - val_loss: 0.2493 - val_rmse: 0.2493\n",
      "Epoch 460/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5331 - rmse: 0.5331 - val_loss: 0.2339 - val_rmse: 0.2339\n",
      "Epoch 461/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5105 - rmse: 0.5105 - val_loss: 0.2160 - val_rmse: 0.2160\n",
      "Epoch 462/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4932 - rmse: 0.4932 - val_loss: 0.2531 - val_rmse: 0.2531\n",
      "Epoch 463/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5005 - rmse: 0.5005 - val_loss: 0.2266 - val_rmse: 0.2266\n",
      "Epoch 464/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5023 - rmse: 0.5023 - val_loss: 0.2199 - val_rmse: 0.2199\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5116 - rmse: 0.5116 - val_loss: 0.2238 - val_rmse: 0.2238\n",
      "Epoch 466/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5116 - rmse: 0.5116 - val_loss: 0.2163 - val_rmse: 0.2163\n",
      "Epoch 467/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5183 - rmse: 0.5183 - val_loss: 0.2071 - val_rmse: 0.2071\n",
      "Epoch 468/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5170 - rmse: 0.5170 - val_loss: 0.2105 - val_rmse: 0.2105\n",
      "Epoch 469/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5026 - rmse: 0.5026 - val_loss: 0.2527 - val_rmse: 0.2527\n",
      "Epoch 470/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5169 - rmse: 0.5169 - val_loss: 0.2087 - val_rmse: 0.2087\n",
      "Epoch 471/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4936 - rmse: 0.4936 - val_loss: 0.2227 - val_rmse: 0.2227\n",
      "Epoch 472/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4988 - rmse: 0.4988 - val_loss: 0.2128 - val_rmse: 0.2128\n",
      "Epoch 473/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4975 - rmse: 0.4975 - val_loss: 0.2151 - val_rmse: 0.2151\n",
      "Epoch 474/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4911 - rmse: 0.4911 - val_loss: 0.2104 - val_rmse: 0.2104\n",
      "Epoch 475/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4951 - rmse: 0.4951 - val_loss: 0.2210 - val_rmse: 0.2210\n",
      "Epoch 476/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.5008 - rmse: 0.5008 - val_loss: 0.2052 - val_rmse: 0.2052\n",
      "Epoch 477/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5002 - rmse: 0.5002 - val_loss: 0.2166 - val_rmse: 0.2166\n",
      "Epoch 478/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.5124 - rmse: 0.5124 - val_loss: 0.2140 - val_rmse: 0.2140\n",
      "Epoch 479/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4962 - rmse: 0.4962 - val_loss: 0.2400 - val_rmse: 0.2400\n",
      "Epoch 480/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4977 - rmse: 0.4977 - val_loss: 0.2007 - val_rmse: 0.2007\n",
      "Epoch 481/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4884 - rmse: 0.4884 - val_loss: 0.2158 - val_rmse: 0.2158\n",
      "Epoch 482/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4826 - rmse: 0.4826 - val_loss: 0.2016 - val_rmse: 0.2016\n",
      "Epoch 483/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.5060 - rmse: 0.5060 - val_loss: 0.2058 - val_rmse: 0.2058\n",
      "Epoch 484/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4748 - rmse: 0.4748 - val_loss: 0.2178 - val_rmse: 0.2178\n",
      "Epoch 485/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.4818 - rmse: 0.4818 - val_loss: 0.2145 - val_rmse: 0.2145\n",
      "Epoch 486/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4811 - rmse: 0.4811 - val_loss: 0.2078 - val_rmse: 0.2078\n",
      "Epoch 487/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4918 - rmse: 0.4918 - val_loss: 0.2197 - val_rmse: 0.2197\n",
      "Epoch 488/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4838 - rmse: 0.4838 - val_loss: 0.2138 - val_rmse: 0.2138\n",
      "Epoch 489/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4903 - rmse: 0.4903 - val_loss: 0.1946 - val_rmse: 0.1946\n",
      "Epoch 490/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.4901 - rmse: 0.4901 - val_loss: 0.2129 - val_rmse: 0.2129\n",
      "Epoch 491/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4793 - rmse: 0.4793 - val_loss: 0.2001 - val_rmse: 0.2001\n",
      "Epoch 492/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4700 - rmse: 0.4700 - val_loss: 0.2164 - val_rmse: 0.2164\n",
      "Epoch 493/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4757 - rmse: 0.4757 - val_loss: 0.2058 - val_rmse: 0.2058\n",
      "Epoch 494/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4953 - rmse: 0.4953 - val_loss: 0.2094 - val_rmse: 0.2094\n",
      "Epoch 495/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4656 - rmse: 0.4656 - val_loss: 0.2279 - val_rmse: 0.2279\n",
      "Epoch 496/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4769 - rmse: 0.4769 - val_loss: 0.2172 - val_rmse: 0.2172\n",
      "Epoch 497/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4806 - rmse: 0.4806 - val_loss: 0.2088 - val_rmse: 0.2088\n",
      "Epoch 498/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4538 - rmse: 0.4538 - val_loss: 0.1923 - val_rmse: 0.1923\n",
      "Epoch 499/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4667 - rmse: 0.4667 - val_loss: 0.2127 - val_rmse: 0.2127\n",
      "Epoch 500/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4555 - rmse: 0.4555 - val_loss: 0.2118 - val_rmse: 0.2118\n",
      "Epoch 501/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4495 - rmse: 0.4495 - val_loss: 0.2088 - val_rmse: 0.2088\n",
      "Epoch 502/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4820 - rmse: 0.4820 - val_loss: 0.1927 - val_rmse: 0.1927\n",
      "Epoch 503/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4779 - rmse: 0.4779 - val_loss: 0.2027 - val_rmse: 0.2027\n",
      "Epoch 504/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4625 - rmse: 0.4625 - val_loss: 0.1867 - val_rmse: 0.1867\n",
      "Epoch 505/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4736 - rmse: 0.4736 - val_loss: 0.2106 - val_rmse: 0.2106\n",
      "Epoch 506/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4488 - rmse: 0.4488 - val_loss: 0.2388 - val_rmse: 0.2388\n",
      "Epoch 507/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4437 - rmse: 0.4437 - val_loss: 0.1973 - val_rmse: 0.1973\n",
      "Epoch 508/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4564 - rmse: 0.4564 - val_loss: 0.1919 - val_rmse: 0.1919\n",
      "Epoch 509/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4585 - rmse: 0.4585 - val_loss: 0.2006 - val_rmse: 0.2006\n",
      "Epoch 510/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.4523 - rmse: 0.45 - 0s 15us/step - loss: 0.4756 - rmse: 0.4756 - val_loss: 0.1997 - val_rmse: 0.1997\n",
      "Epoch 511/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4487 - rmse: 0.4487 - val_loss: 0.1925 - val_rmse: 0.1925\n",
      "Epoch 512/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4456 - rmse: 0.4456 - val_loss: 0.1795 - val_rmse: 0.1795\n",
      "Epoch 513/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4405 - rmse: 0.4405 - val_loss: 0.1834 - val_rmse: 0.1834\n",
      "Epoch 514/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4650 - rmse: 0.4650 - val_loss: 0.1808 - val_rmse: 0.1808\n",
      "Epoch 515/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4479 - rmse: 0.4479 - val_loss: 0.1935 - val_rmse: 0.1935\n",
      "Epoch 516/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4678 - rmse: 0.4678 - val_loss: 0.2000 - val_rmse: 0.2000\n",
      "Epoch 517/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4353 - rmse: 0.4353 - val_loss: 0.1948 - val_rmse: 0.1948\n",
      "Epoch 518/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4445 - rmse: 0.4445 - val_loss: 0.2048 - val_rmse: 0.2048\n",
      "Epoch 519/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4465 - rmse: 0.4465 - val_loss: 0.1958 - val_rmse: 0.1958\n",
      "Epoch 520/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4649 - rmse: 0.4649 - val_loss: 0.2048 - val_rmse: 0.2048\n",
      "Epoch 521/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4378 - rmse: 0.4378 - val_loss: 0.2211 - val_rmse: 0.2211\n",
      "Epoch 522/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4534 - rmse: 0.4534 - val_loss: 0.1879 - val_rmse: 0.1879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4374 - rmse: 0.4374 - val_loss: 0.1885 - val_rmse: 0.1885\n",
      "Epoch 524/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4453 - rmse: 0.4453 - val_loss: 0.1876 - val_rmse: 0.1876\n",
      "Epoch 525/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4389 - rmse: 0.4389 - val_loss: 0.2094 - val_rmse: 0.2094\n",
      "Epoch 526/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4279 - rmse: 0.4279 - val_loss: 0.1941 - val_rmse: 0.1941\n",
      "Epoch 527/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4380 - rmse: 0.4380 - val_loss: 0.1873 - val_rmse: 0.1873\n",
      "Epoch 528/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4412 - rmse: 0.4412 - val_loss: 0.1930 - val_rmse: 0.1930\n",
      "Epoch 529/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4253 - rmse: 0.4253 - val_loss: 0.1864 - val_rmse: 0.1864\n",
      "Epoch 530/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4241 - rmse: 0.4241 - val_loss: 0.1978 - val_rmse: 0.1978\n",
      "Epoch 531/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4352 - rmse: 0.4352 - val_loss: 0.1817 - val_rmse: 0.1817\n",
      "Epoch 532/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4344 - rmse: 0.4344 - val_loss: 0.1884 - val_rmse: 0.1884\n",
      "Epoch 533/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4226 - rmse: 0.4226 - val_loss: 0.1955 - val_rmse: 0.1955\n",
      "Epoch 534/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4143 - rmse: 0.4143 - val_loss: 0.1883 - val_rmse: 0.1883\n",
      "Epoch 535/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4202 - rmse: 0.4202 - val_loss: 0.1910 - val_rmse: 0.1910\n",
      "Epoch 536/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4295 - rmse: 0.4295 - val_loss: 0.1860 - val_rmse: 0.1860\n",
      "Epoch 537/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4263 - rmse: 0.4263 - val_loss: 0.2025 - val_rmse: 0.2025\n",
      "Epoch 538/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4154 - rmse: 0.4154 - val_loss: 0.1970 - val_rmse: 0.1970\n",
      "Epoch 539/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4123 - rmse: 0.4123 - val_loss: 0.1817 - val_rmse: 0.1817\n",
      "Epoch 540/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4154 - rmse: 0.4154 - val_loss: 0.1716 - val_rmse: 0.1716\n",
      "Epoch 541/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4150 - rmse: 0.4150 - val_loss: 0.2266 - val_rmse: 0.2266\n",
      "Epoch 542/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4350 - rmse: 0.4350 - val_loss: 0.1692 - val_rmse: 0.1692\n",
      "Epoch 543/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4125 - rmse: 0.4125 - val_loss: 0.1830 - val_rmse: 0.1830\n",
      "Epoch 544/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4179 - rmse: 0.4179 - val_loss: 0.1701 - val_rmse: 0.1701\n",
      "Epoch 545/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.4185 - rmse: 0.4185 - val_loss: 0.1792 - val_rmse: 0.1792\n",
      "Epoch 546/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.4188 - rmse: 0.41 - 0s 15us/step - loss: 0.4074 - rmse: 0.4074 - val_loss: 0.1725 - val_rmse: 0.1725\n",
      "Epoch 547/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.4364 - rmse: 0.4364 - val_loss: 0.1722 - val_rmse: 0.1722\n",
      "Epoch 548/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4309 - rmse: 0.4309 - val_loss: 0.1743 - val_rmse: 0.1743\n",
      "Epoch 549/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4289 - rmse: 0.4289 - val_loss: 0.1811 - val_rmse: 0.1811\n",
      "Epoch 550/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4218 - rmse: 0.4218 - val_loss: 0.2017 - val_rmse: 0.2017\n",
      "Epoch 551/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4121 - rmse: 0.4121 - val_loss: 0.1754 - val_rmse: 0.1754\n",
      "Epoch 552/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4020 - rmse: 0.4020 - val_loss: 0.1768 - val_rmse: 0.1768\n",
      "Epoch 553/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4193 - rmse: 0.4193 - val_loss: 0.2190 - val_rmse: 0.2190\n",
      "Epoch 554/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4017 - rmse: 0.4017 - val_loss: 0.1871 - val_rmse: 0.1871\n",
      "Epoch 555/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.4057 - rmse: 0.4057 - val_loss: 0.1793 - val_rmse: 0.1793\n",
      "Epoch 556/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3977 - rmse: 0.3977 - val_loss: 0.1961 - val_rmse: 0.1961\n",
      "Epoch 557/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4038 - rmse: 0.4038 - val_loss: 0.1861 - val_rmse: 0.1861\n",
      "Epoch 558/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4165 - rmse: 0.4165 - val_loss: 0.1724 - val_rmse: 0.1724\n",
      "Epoch 559/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3810 - rmse: 0.3810 - val_loss: 0.1673 - val_rmse: 0.1673\n",
      "Epoch 560/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3947 - rmse: 0.3947 - val_loss: 0.1823 - val_rmse: 0.1823\n",
      "Epoch 561/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4012 - rmse: 0.4012 - val_loss: 0.1821 - val_rmse: 0.1821\n",
      "Epoch 562/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.4110 - rmse: 0.4110 - val_loss: 0.1815 - val_rmse: 0.1815\n",
      "Epoch 563/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.3812 - rmse: 0.3812 - val_loss: 0.1808 - val_rmse: 0.1808\n",
      "Epoch 564/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3900 - rmse: 0.3900 - val_loss: 0.1873 - val_rmse: 0.1873\n",
      "Epoch 565/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.4007 - rmse: 0.4007 - val_loss: 0.1768 - val_rmse: 0.1768\n",
      "Epoch 566/1000\n",
      "1166/1166 [==============================] - 0s 20us/step - loss: 0.3989 - rmse: 0.3989 - val_loss: 0.1904 - val_rmse: 0.1904\n",
      "Epoch 567/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3957 - rmse: 0.3957 - val_loss: 0.1767 - val_rmse: 0.1767\n",
      "Epoch 568/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.3869 - rmse: 0.3869 - val_loss: 0.1957 - val_rmse: 0.1957\n",
      "Epoch 569/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.4021 - rmse: 0.4021 - val_loss: 0.1712 - val_rmse: 0.1712\n",
      "Epoch 570/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3910 - rmse: 0.3910 - val_loss: 0.1768 - val_rmse: 0.1768\n",
      "Epoch 571/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3868 - rmse: 0.3868 - val_loss: 0.1621 - val_rmse: 0.1621\n",
      "Epoch 572/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3998 - rmse: 0.3998 - val_loss: 0.1683 - val_rmse: 0.1683\n",
      "Epoch 573/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3889 - rmse: 0.3889 - val_loss: 0.1861 - val_rmse: 0.1861\n",
      "Epoch 574/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3720 - rmse: 0.3720 - val_loss: 0.1751 - val_rmse: 0.1751\n",
      "Epoch 575/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3914 - rmse: 0.3914 - val_loss: 0.1715 - val_rmse: 0.1715\n",
      "Epoch 576/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3801 - rmse: 0.3801 - val_loss: 0.2177 - val_rmse: 0.2177\n",
      "Epoch 577/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3697 - rmse: 0.3697 - val_loss: 0.1689 - val_rmse: 0.1689\n",
      "Epoch 578/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3941 - rmse: 0.3941 - val_loss: 0.1611 - val_rmse: 0.1611\n",
      "Epoch 579/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.3766 - rmse: 0.3766 - val_loss: 0.1649 - val_rmse: 0.1649\n",
      "Epoch 580/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3850 - rmse: 0.3850 - val_loss: 0.1650 - val_rmse: 0.1650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 581/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3727 - rmse: 0.3727 - val_loss: 0.1661 - val_rmse: 0.1661\n",
      "Epoch 582/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3813 - rmse: 0.3813 - val_loss: 0.1671 - val_rmse: 0.1671\n",
      "Epoch 583/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3649 - rmse: 0.3649 - val_loss: 0.1696 - val_rmse: 0.1696\n",
      "Epoch 584/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3549 - rmse: 0.3549 - val_loss: 0.1638 - val_rmse: 0.1638\n",
      "Epoch 585/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3705 - rmse: 0.3705 - val_loss: 0.2004 - val_rmse: 0.2004\n",
      "Epoch 586/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3668 - rmse: 0.3668 - val_loss: 0.1628 - val_rmse: 0.1628\n",
      "Epoch 587/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3665 - rmse: 0.3665 - val_loss: 0.1789 - val_rmse: 0.1789\n",
      "Epoch 588/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.3629 - rmse: 0.3629 - val_loss: 0.1721 - val_rmse: 0.1721\n",
      "Epoch 589/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3707 - rmse: 0.3707 - val_loss: 0.1723 - val_rmse: 0.1723\n",
      "Epoch 590/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3685 - rmse: 0.3685 - val_loss: 0.1600 - val_rmse: 0.1600\n",
      "Epoch 591/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3695 - rmse: 0.3695 - val_loss: 0.1680 - val_rmse: 0.1680\n",
      "Epoch 592/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3561 - rmse: 0.3561 - val_loss: 0.1800 - val_rmse: 0.1800\n",
      "Epoch 593/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3624 - rmse: 0.3624 - val_loss: 0.1543 - val_rmse: 0.1543\n",
      "Epoch 594/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.3651 - rmse: 0.3651 - val_loss: 0.1626 - val_rmse: 0.1626\n",
      "Epoch 595/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3581 - rmse: 0.3581 - val_loss: 0.1590 - val_rmse: 0.1590\n",
      "Epoch 596/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3541 - rmse: 0.3541 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 597/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3569 - rmse: 0.3569 - val_loss: 0.1596 - val_rmse: 0.1596\n",
      "Epoch 598/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3523 - rmse: 0.3523 - val_loss: 0.1579 - val_rmse: 0.1579\n",
      "Epoch 599/1000\n",
      "1166/1166 [==============================] - 0s 25us/step - loss: 0.3561 - rmse: 0.3561 - val_loss: 0.1746 - val_rmse: 0.1746\n",
      "Epoch 600/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.3563 - rmse: 0.3563 - val_loss: 0.1566 - val_rmse: 0.1566\n",
      "Epoch 601/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3638 - rmse: 0.3638 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 602/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3540 - rmse: 0.3540 - val_loss: 0.1580 - val_rmse: 0.1580\n",
      "Epoch 603/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3568 - rmse: 0.3568 - val_loss: 0.1708 - val_rmse: 0.1708\n",
      "Epoch 604/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3487 - rmse: 0.3487 - val_loss: 0.1538 - val_rmse: 0.1538\n",
      "Epoch 605/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3435 - rmse: 0.3435 - val_loss: 0.1676 - val_rmse: 0.1676\n",
      "Epoch 606/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3493 - rmse: 0.3493 - val_loss: 0.1558 - val_rmse: 0.1558\n",
      "Epoch 607/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3431 - rmse: 0.3431 - val_loss: 0.1520 - val_rmse: 0.1520\n",
      "Epoch 608/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3421 - rmse: 0.3421 - val_loss: 0.1612 - val_rmse: 0.1612\n",
      "Epoch 609/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3366 - rmse: 0.3366 - val_loss: 0.1723 - val_rmse: 0.1723\n",
      "Epoch 610/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3377 - rmse: 0.3377 - val_loss: 0.1533 - val_rmse: 0.1533\n",
      "Epoch 611/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3417 - rmse: 0.3417 - val_loss: 0.1586 - val_rmse: 0.1586\n",
      "Epoch 612/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3512 - rmse: 0.3512 - val_loss: 0.1500 - val_rmse: 0.1500\n",
      "Epoch 613/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3381 - rmse: 0.3381 - val_loss: 0.1565 - val_rmse: 0.1565\n",
      "Epoch 614/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3319 - rmse: 0.3319 - val_loss: 0.1532 - val_rmse: 0.1532\n",
      "Epoch 615/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3332 - rmse: 0.3332 - val_loss: 0.1635 - val_rmse: 0.1635\n",
      "Epoch 616/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3384 - rmse: 0.3384 - val_loss: 0.1642 - val_rmse: 0.1642\n",
      "Epoch 617/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3394 - rmse: 0.3394 - val_loss: 0.1540 - val_rmse: 0.1540\n",
      "Epoch 618/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3458 - rmse: 0.3458 - val_loss: 0.1487 - val_rmse: 0.1487\n",
      "Epoch 619/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3314 - rmse: 0.3314 - val_loss: 0.1651 - val_rmse: 0.1651\n",
      "Epoch 620/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3379 - rmse: 0.3379 - val_loss: 0.1545 - val_rmse: 0.1545\n",
      "Epoch 621/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3307 - rmse: 0.3307 - val_loss: 0.1685 - val_rmse: 0.1685\n",
      "Epoch 622/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3359 - rmse: 0.3359 - val_loss: 0.1581 - val_rmse: 0.1581\n",
      "Epoch 623/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3256 - rmse: 0.3256 - val_loss: 0.1708 - val_rmse: 0.1708\n",
      "Epoch 624/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3467 - rmse: 0.3467 - val_loss: 0.1539 - val_rmse: 0.1539\n",
      "Epoch 625/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3267 - rmse: 0.3267 - val_loss: 0.1434 - val_rmse: 0.1434\n",
      "Epoch 626/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3180 - rmse: 0.3180 - val_loss: 0.1470 - val_rmse: 0.1470\n",
      "Epoch 627/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3350 - rmse: 0.3350 - val_loss: 0.1469 - val_rmse: 0.1469\n",
      "Epoch 628/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3256 - rmse: 0.3256 - val_loss: 0.1548 - val_rmse: 0.1548\n",
      "Epoch 629/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3232 - rmse: 0.3232 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 630/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3332 - rmse: 0.3332 - val_loss: 0.1434 - val_rmse: 0.1434\n",
      "Epoch 631/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3224 - rmse: 0.3224 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 632/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3277 - rmse: 0.3277 - val_loss: 0.1564 - val_rmse: 0.1564\n",
      "Epoch 633/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3216 - rmse: 0.3216 - val_loss: 0.1511 - val_rmse: 0.1511\n",
      "Epoch 634/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3228 - rmse: 0.3228 - val_loss: 0.1483 - val_rmse: 0.1483\n",
      "Epoch 635/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3043 - rmse: 0.3043 - val_loss: 0.1537 - val_rmse: 0.1537\n",
      "Epoch 636/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3199 - rmse: 0.3199 - val_loss: 0.1530 - val_rmse: 0.1530\n",
      "Epoch 637/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3114 - rmse: 0.3114 - val_loss: 0.1397 - val_rmse: 0.1397\n",
      "Epoch 638/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3175 - rmse: 0.3175 - val_loss: 0.1498 - val_rmse: 0.1498\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3082 - rmse: 0.3082 - val_loss: 0.1653 - val_rmse: 0.1653\n",
      "Epoch 640/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3188 - rmse: 0.3188 - val_loss: 0.1573 - val_rmse: 0.1573\n",
      "Epoch 641/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3162 - rmse: 0.3162 - val_loss: 0.1422 - val_rmse: 0.1422\n",
      "Epoch 642/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3180 - rmse: 0.3180 - val_loss: 0.1466 - val_rmse: 0.1466\n",
      "Epoch 643/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3120 - rmse: 0.3120 - val_loss: 0.1512 - val_rmse: 0.1512\n",
      "Epoch 644/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.3187 - rmse: 0.3187 - val_loss: 0.1474 - val_rmse: 0.1474\n",
      "Epoch 645/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2994 - rmse: 0.2994 - val_loss: 0.1385 - val_rmse: 0.1385\n",
      "Epoch 646/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3111 - rmse: 0.3111 - val_loss: 0.1526 - val_rmse: 0.1526\n",
      "Epoch 647/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3036 - rmse: 0.3036 - val_loss: 0.1484 - val_rmse: 0.1484\n",
      "Epoch 648/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.3003 - rmse: 0.3003 - val_loss: 0.1462 - val_rmse: 0.1462\n",
      "Epoch 649/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2964 - rmse: 0.2964 - val_loss: 0.1371 - val_rmse: 0.1371\n",
      "Epoch 650/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.3093 - rmse: 0.3093 - val_loss: 0.1504 - val_rmse: 0.1504\n",
      "Epoch 651/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2950 - rmse: 0.2950 - val_loss: 0.1352 - val_rmse: 0.1352\n",
      "Epoch 652/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2972 - rmse: 0.2972 - val_loss: 0.1357 - val_rmse: 0.1357\n",
      "Epoch 653/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.3052 - rmse: 0.3052 - val_loss: 0.1484 - val_rmse: 0.1484\n",
      "Epoch 654/1000\n",
      "1166/1166 [==============================] - 0s 20us/step - loss: 0.2920 - rmse: 0.2920 - val_loss: 0.1454 - val_rmse: 0.1454\n",
      "Epoch 655/1000\n",
      "1166/1166 [==============================] - 0s 23us/step - loss: 0.2923 - rmse: 0.2923 - val_loss: 0.1419 - val_rmse: 0.1419\n",
      "Epoch 656/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.2927 - rmse: 0.2927 - val_loss: 0.1392 - val_rmse: 0.1392\n",
      "Epoch 657/1000\n",
      "1166/1166 [==============================] - 0s 20us/step - loss: 0.3021 - rmse: 0.3021 - val_loss: 0.1321 - val_rmse: 0.1321\n",
      "Epoch 658/1000\n",
      "1166/1166 [==============================] - 0s 21us/step - loss: 0.3065 - rmse: 0.3065 - val_loss: 0.1443 - val_rmse: 0.1443\n",
      "Epoch 659/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.2913 - rmse: 0.2913 - val_loss: 0.1381 - val_rmse: 0.1381\n",
      "Epoch 660/1000\n",
      "1166/1166 [==============================] - 0s 19us/step - loss: 0.2975 - rmse: 0.2975 - val_loss: 0.1394 - val_rmse: 0.1394\n",
      "Epoch 661/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.2902 - rmse: 0.2902 - val_loss: 0.1372 - val_rmse: 0.1372\n",
      "Epoch 662/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2887 - rmse: 0.2887 - val_loss: 0.1450 - val_rmse: 0.1450\n",
      "Epoch 663/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2916 - rmse: 0.2916 - val_loss: 0.1332 - val_rmse: 0.1332\n",
      "Epoch 664/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2855 - rmse: 0.2855 - val_loss: 0.1416 - val_rmse: 0.1416\n",
      "Epoch 665/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2914 - rmse: 0.2914 - val_loss: 0.1370 - val_rmse: 0.1370\n",
      "Epoch 666/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2986 - rmse: 0.2986 - val_loss: 0.1423 - val_rmse: 0.1423\n",
      "Epoch 667/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2847 - rmse: 0.2847 - val_loss: 0.1334 - val_rmse: 0.1334\n",
      "Epoch 668/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2916 - rmse: 0.2916 - val_loss: 0.1617 - val_rmse: 0.1617\n",
      "Epoch 669/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2841 - rmse: 0.2841 - val_loss: 0.1424 - val_rmse: 0.1424\n",
      "Epoch 670/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2810 - rmse: 0.2810 - val_loss: 0.1289 - val_rmse: 0.1289\n",
      "Epoch 671/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2767 - rmse: 0.2767 - val_loss: 0.1324 - val_rmse: 0.1324\n",
      "Epoch 672/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.2789 - rmse: 0.2789 - val_loss: 0.1359 - val_rmse: 0.1359\n",
      "Epoch 673/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2847 - rmse: 0.2847 - val_loss: 0.1294 - val_rmse: 0.1294\n",
      "Epoch 674/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.2838 - rmse: 0.2838 - val_loss: 0.1428 - val_rmse: 0.1428\n",
      "Epoch 675/1000\n",
      "1166/1166 [==============================] - 0s 20us/step - loss: 0.2717 - rmse: 0.2717 - val_loss: 0.1335 - val_rmse: 0.1335\n",
      "Epoch 676/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.2743 - rmse: 0.2743 - val_loss: 0.1302 - val_rmse: 0.1302\n",
      "Epoch 677/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.2693 - rmse: 0.2693 - val_loss: 0.1327 - val_rmse: 0.1327\n",
      "Epoch 678/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2747 - rmse: 0.2747 - val_loss: 0.1465 - val_rmse: 0.1465\n",
      "Epoch 679/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.2649 - rmse: 0.2649 - val_loss: 0.1364 - val_rmse: 0.1364\n",
      "Epoch 680/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2786 - rmse: 0.2786 - val_loss: 0.1307 - val_rmse: 0.1307\n",
      "Epoch 681/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2669 - rmse: 0.2669 - val_loss: 0.1477 - val_rmse: 0.1477\n",
      "Epoch 682/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2691 - rmse: 0.2691 - val_loss: 0.1475 - val_rmse: 0.1475\n",
      "Epoch 683/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2707 - rmse: 0.2707 - val_loss: 0.1362 - val_rmse: 0.1362\n",
      "Epoch 684/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2622 - rmse: 0.2622 - val_loss: 0.1300 - val_rmse: 0.1300\n",
      "Epoch 685/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.2650 - rmse: 0.2650 - val_loss: 0.1270 - val_rmse: 0.1270\n",
      "Epoch 686/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.2675 - rmse: 0.2675 - val_loss: 0.1434 - val_rmse: 0.1434\n",
      "Epoch 687/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2617 - rmse: 0.2617 - val_loss: 0.1280 - val_rmse: 0.1280\n",
      "Epoch 688/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2638 - rmse: 0.2638 - val_loss: 0.1332 - val_rmse: 0.1332\n",
      "Epoch 689/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2729 - rmse: 0.2729 - val_loss: 0.1504 - val_rmse: 0.1504\n",
      "Epoch 690/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2614 - rmse: 0.2614 - val_loss: 0.1342 - val_rmse: 0.1342\n",
      "Epoch 691/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2598 - rmse: 0.2598 - val_loss: 0.1456 - val_rmse: 0.1456\n",
      "Epoch 692/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2656 - rmse: 0.2656 - val_loss: 0.1375 - val_rmse: 0.1375\n",
      "Epoch 693/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2683 - rmse: 0.2683 - val_loss: 0.1408 - val_rmse: 0.1408\n",
      "Epoch 694/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2468 - rmse: 0.2468 - val_loss: 0.1273 - val_rmse: 0.1273\n",
      "Epoch 695/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2496 - rmse: 0.2496 - val_loss: 0.1444 - val_rmse: 0.1444\n",
      "Epoch 696/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2549 - rmse: 0.2549 - val_loss: 0.1345 - val_rmse: 0.1345\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2570 - rmse: 0.2570 - val_loss: 0.1266 - val_rmse: 0.1266\n",
      "Epoch 698/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2610 - rmse: 0.2610 - val_loss: 0.1256 - val_rmse: 0.1256\n",
      "Epoch 699/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2594 - rmse: 0.2594 - val_loss: 0.1358 - val_rmse: 0.1358\n",
      "Epoch 700/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2429 - rmse: 0.2429 - val_loss: 0.1262 - val_rmse: 0.1262\n",
      "Epoch 701/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2468 - rmse: 0.2468 - val_loss: 0.1230 - val_rmse: 0.1230\n",
      "Epoch 702/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.2545 - rmse: 0.2545 - val_loss: 0.1231 - val_rmse: 0.1231\n",
      "Epoch 703/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2641 - rmse: 0.2641 - val_loss: 0.1208 - val_rmse: 0.1208\n",
      "Epoch 704/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2502 - rmse: 0.2502 - val_loss: 0.1293 - val_rmse: 0.1293\n",
      "Epoch 705/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.2476 - rmse: 0.2476 - val_loss: 0.1207 - val_rmse: 0.1207\n",
      "Epoch 706/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2491 - rmse: 0.2491 - val_loss: 0.1219 - val_rmse: 0.1219\n",
      "Epoch 707/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2449 - rmse: 0.2449 - val_loss: 0.1195 - val_rmse: 0.1195\n",
      "Epoch 708/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2467 - rmse: 0.2467 - val_loss: 0.1300 - val_rmse: 0.1300\n",
      "Epoch 709/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2491 - rmse: 0.2491 - val_loss: 0.1386 - val_rmse: 0.1386\n",
      "Epoch 710/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2526 - rmse: 0.2526 - val_loss: 0.1207 - val_rmse: 0.1207\n",
      "Epoch 711/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.2352 - rmse: 0.2352 - val_loss: 0.1259 - val_rmse: 0.1259\n",
      "Epoch 712/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2371 - rmse: 0.2371 - val_loss: 0.1209 - val_rmse: 0.1209\n",
      "Epoch 713/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2516 - rmse: 0.2516 - val_loss: 0.1238 - val_rmse: 0.1238\n",
      "Epoch 714/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2435 - rmse: 0.2435 - val_loss: 0.1187 - val_rmse: 0.1187\n",
      "Epoch 715/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2374 - rmse: 0.2374 - val_loss: 0.1229 - val_rmse: 0.1229\n",
      "Epoch 716/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2296 - rmse: 0.2296 - val_loss: 0.1197 - val_rmse: 0.1197\n",
      "Epoch 717/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2382 - rmse: 0.2382 - val_loss: 0.1255 - val_rmse: 0.1255\n",
      "Epoch 718/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2253 - rmse: 0.2253 - val_loss: 0.1187 - val_rmse: 0.1187\n",
      "Epoch 719/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2407 - rmse: 0.2407 - val_loss: 0.1174 - val_rmse: 0.1174\n",
      "Epoch 720/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2381 - rmse: 0.2381 - val_loss: 0.1459 - val_rmse: 0.1459\n",
      "Epoch 721/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2434 - rmse: 0.2434 - val_loss: 0.1216 - val_rmse: 0.1216\n",
      "Epoch 722/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2336 - rmse: 0.2336 - val_loss: 0.1178 - val_rmse: 0.1178\n",
      "Epoch 723/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2267 - rmse: 0.2267 - val_loss: 0.1195 - val_rmse: 0.1195\n",
      "Epoch 724/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2277 - rmse: 0.2277 - val_loss: 0.1217 - val_rmse: 0.1217\n",
      "Epoch 725/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2371 - rmse: 0.2371 - val_loss: 0.1218 - val_rmse: 0.1218\n",
      "Epoch 726/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2269 - rmse: 0.2269 - val_loss: 0.1174 - val_rmse: 0.1174\n",
      "Epoch 727/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2367 - rmse: 0.2367 - val_loss: 0.1263 - val_rmse: 0.1263\n",
      "Epoch 728/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2270 - rmse: 0.2270 - val_loss: 0.1206 - val_rmse: 0.1206\n",
      "Epoch 729/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2246 - rmse: 0.2246 - val_loss: 0.1226 - val_rmse: 0.1226\n",
      "Epoch 730/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2273 - rmse: 0.2273 - val_loss: 0.1188 - val_rmse: 0.1188\n",
      "Epoch 731/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2335 - rmse: 0.2335 - val_loss: 0.1199 - val_rmse: 0.1199\n",
      "Epoch 732/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2227 - rmse: 0.2227 - val_loss: 0.1181 - val_rmse: 0.1181\n",
      "Epoch 733/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2285 - rmse: 0.2285 - val_loss: 0.1177 - val_rmse: 0.1177\n",
      "Epoch 734/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2220 - rmse: 0.2220 - val_loss: 0.1232 - val_rmse: 0.1232\n",
      "Epoch 735/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2191 - rmse: 0.2191 - val_loss: 0.1194 - val_rmse: 0.1194\n",
      "Epoch 736/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2233 - rmse: 0.2233 - val_loss: 0.1185 - val_rmse: 0.1185\n",
      "Epoch 737/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2273 - rmse: 0.2273 - val_loss: 0.1209 - val_rmse: 0.1209\n",
      "Epoch 738/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2265 - rmse: 0.2265 - val_loss: 0.1212 - val_rmse: 0.1212\n",
      "Epoch 739/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2172 - rmse: 0.2172 - val_loss: 0.1194 - val_rmse: 0.1194\n",
      "Epoch 740/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2123 - rmse: 0.2123 - val_loss: 0.1234 - val_rmse: 0.1234\n",
      "Epoch 741/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2188 - rmse: 0.2188 - val_loss: 0.1176 - val_rmse: 0.1176\n",
      "Epoch 742/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2167 - rmse: 0.2167 - val_loss: 0.1159 - val_rmse: 0.1159\n",
      "Epoch 743/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2108 - rmse: 0.2108 - val_loss: 0.1171 - val_rmse: 0.1171\n",
      "Epoch 744/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2127 - rmse: 0.2127 - val_loss: 0.1272 - val_rmse: 0.1272\n",
      "Epoch 745/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2100 - rmse: 0.2100 - val_loss: 0.1171 - val_rmse: 0.1171\n",
      "Epoch 746/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2145 - rmse: 0.2145 - val_loss: 0.1264 - val_rmse: 0.1264\n",
      "Epoch 747/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2160 - rmse: 0.2160 - val_loss: 0.1178 - val_rmse: 0.1178\n",
      "Epoch 748/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2082 - rmse: 0.2082 - val_loss: 0.1148 - val_rmse: 0.1148\n",
      "Epoch 749/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2033 - rmse: 0.2033 - val_loss: 0.1156 - val_rmse: 0.1156\n",
      "Epoch 750/1000\n",
      "1166/1166 [==============================] - ETA: 0s - loss: 0.2207 - rmse: 0.22 - 0s 13us/step - loss: 0.2121 - rmse: 0.2121 - val_loss: 0.1153 - val_rmse: 0.1153\n",
      "Epoch 751/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2059 - rmse: 0.2059 - val_loss: 0.1132 - val_rmse: 0.1132\n",
      "Epoch 752/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2056 - rmse: 0.2056 - val_loss: 0.1164 - val_rmse: 0.1164\n",
      "Epoch 753/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2031 - rmse: 0.2031 - val_loss: 0.1165 - val_rmse: 0.1165\n",
      "Epoch 754/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2080 - rmse: 0.2080 - val_loss: 0.1165 - val_rmse: 0.1165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 755/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.2040 - rmse: 0.2040 - val_loss: 0.1107 - val_rmse: 0.1107\n",
      "Epoch 756/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2090 - rmse: 0.2090 - val_loss: 0.1129 - val_rmse: 0.1129\n",
      "Epoch 757/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2054 - rmse: 0.2054 - val_loss: 0.1106 - val_rmse: 0.1106\n",
      "Epoch 758/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2088 - rmse: 0.2088 - val_loss: 0.1103 - val_rmse: 0.1103\n",
      "Epoch 759/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2024 - rmse: 0.2024 - val_loss: 0.1095 - val_rmse: 0.1095\n",
      "Epoch 760/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2017 - rmse: 0.2017 - val_loss: 0.1092 - val_rmse: 0.1092\n",
      "Epoch 761/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.2029 - rmse: 0.2029 - val_loss: 0.1152 - val_rmse: 0.1152\n",
      "Epoch 762/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.2003 - rmse: 0.2003 - val_loss: 0.1151 - val_rmse: 0.1151\n",
      "Epoch 763/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.2018 - rmse: 0.2018 - val_loss: 0.1291 - val_rmse: 0.1291\n",
      "Epoch 764/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.1988 - rmse: 0.1988 - val_loss: 0.1100 - val_rmse: 0.1100\n",
      "Epoch 765/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.1956 - rmse: 0.1956 - val_loss: 0.1117 - val_rmse: 0.1117\n",
      "Epoch 766/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1938 - rmse: 0.1938 - val_loss: 0.1095 - val_rmse: 0.1095\n",
      "Epoch 767/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1958 - rmse: 0.1958 - val_loss: 0.1083 - val_rmse: 0.1083\n",
      "Epoch 768/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1932 - rmse: 0.1932 - val_loss: 0.1168 - val_rmse: 0.1168\n",
      "Epoch 769/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1846 - rmse: 0.1846 - val_loss: 0.1264 - val_rmse: 0.1264\n",
      "Epoch 770/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1882 - rmse: 0.1882 - val_loss: 0.1236 - val_rmse: 0.1236\n",
      "Epoch 771/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.1862 - rmse: 0.1862 - val_loss: 0.1080 - val_rmse: 0.1080\n",
      "Epoch 772/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1890 - rmse: 0.1890 - val_loss: 0.1096 - val_rmse: 0.1096\n",
      "Epoch 773/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1903 - rmse: 0.1903 - val_loss: 0.1093 - val_rmse: 0.1093\n",
      "Epoch 774/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1905 - rmse: 0.1905 - val_loss: 0.1074 - val_rmse: 0.1074\n",
      "Epoch 775/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1960 - rmse: 0.1960 - val_loss: 0.1100 - val_rmse: 0.1100\n",
      "Epoch 776/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1859 - rmse: 0.1859 - val_loss: 0.1115 - val_rmse: 0.1115\n",
      "Epoch 777/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1851 - rmse: 0.1851 - val_loss: 0.1088 - val_rmse: 0.1088\n",
      "Epoch 778/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1822 - rmse: 0.1822 - val_loss: 0.1066 - val_rmse: 0.1066\n",
      "Epoch 779/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1844 - rmse: 0.1844 - val_loss: 0.1067 - val_rmse: 0.1067\n",
      "Epoch 780/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1829 - rmse: 0.1829 - val_loss: 0.1105 - val_rmse: 0.1105\n",
      "Epoch 781/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1070 - val_rmse: 0.1070\n",
      "Epoch 782/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1816 - rmse: 0.1816 - val_loss: 0.1119 - val_rmse: 0.1119\n",
      "Epoch 783/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1826 - rmse: 0.1826 - val_loss: 0.1059 - val_rmse: 0.1059\n",
      "Epoch 784/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1809 - rmse: 0.1809 - val_loss: 0.1080 - val_rmse: 0.1080\n",
      "Epoch 785/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1836 - rmse: 0.1836 - val_loss: 0.1086 - val_rmse: 0.1086\n",
      "Epoch 786/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1836 - rmse: 0.1836 - val_loss: 0.1138 - val_rmse: 0.1138\n",
      "Epoch 787/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1743 - rmse: 0.1743 - val_loss: 0.1111 - val_rmse: 0.1111\n",
      "Epoch 788/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1725 - rmse: 0.1725 - val_loss: 0.1051 - val_rmse: 0.1051\n",
      "Epoch 789/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.1730 - rmse: 0.1730 - val_loss: 0.1050 - val_rmse: 0.1050\n",
      "Epoch 790/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1776 - rmse: 0.1776 - val_loss: 0.1154 - val_rmse: 0.1154\n",
      "Epoch 791/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1819 - rmse: 0.1819 - val_loss: 0.1039 - val_rmse: 0.1039\n",
      "Epoch 792/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1734 - rmse: 0.1734 - val_loss: 0.1040 - val_rmse: 0.1040\n",
      "Epoch 793/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1727 - rmse: 0.1727 - val_loss: 0.1031 - val_rmse: 0.1031\n",
      "Epoch 794/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1741 - rmse: 0.1741 - val_loss: 0.1132 - val_rmse: 0.1132\n",
      "Epoch 795/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1740 - rmse: 0.1740 - val_loss: 0.1036 - val_rmse: 0.1036\n",
      "Epoch 796/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1712 - rmse: 0.1712 - val_loss: 0.1052 - val_rmse: 0.1052\n",
      "Epoch 797/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1683 - rmse: 0.1683 - val_loss: 0.1067 - val_rmse: 0.1067\n",
      "Epoch 798/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1717 - rmse: 0.1717 - val_loss: 0.1050 - val_rmse: 0.1050\n",
      "Epoch 799/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1672 - rmse: 0.1672 - val_loss: 0.1009 - val_rmse: 0.1009\n",
      "Epoch 800/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1661 - rmse: 0.1661 - val_loss: 0.1035 - val_rmse: 0.1035\n",
      "Epoch 801/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1653 - rmse: 0.1653 - val_loss: 0.1020 - val_rmse: 0.1020\n",
      "Epoch 802/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1721 - rmse: 0.1721 - val_loss: 0.1022 - val_rmse: 0.1022\n",
      "Epoch 803/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1671 - rmse: 0.1671 - val_loss: 0.1073 - val_rmse: 0.1073\n",
      "Epoch 804/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1650 - rmse: 0.1650 - val_loss: 0.1024 - val_rmse: 0.1024\n",
      "Epoch 805/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1610 - rmse: 0.1610 - val_loss: 0.1081 - val_rmse: 0.1081\n",
      "Epoch 806/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1642 - rmse: 0.1642 - val_loss: 0.1005 - val_rmse: 0.1005\n",
      "Epoch 807/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1583 - rmse: 0.1583 - val_loss: 0.1021 - val_rmse: 0.1021\n",
      "Epoch 808/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1587 - rmse: 0.1587 - val_loss: 0.1049 - val_rmse: 0.1049\n",
      "Epoch 809/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1667 - rmse: 0.1667 - val_loss: 0.0997 - val_rmse: 0.0997\n",
      "Epoch 810/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1574 - rmse: 0.1574 - val_loss: 0.1066 - val_rmse: 0.1066\n",
      "Epoch 811/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1547 - rmse: 0.1547 - val_loss: 0.0998 - val_rmse: 0.0998\n",
      "Epoch 812/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1516 - rmse: 0.1516 - val_loss: 0.0997 - val_rmse: 0.0997\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1564 - rmse: 0.1564 - val_loss: 0.0984 - val_rmse: 0.0984\n",
      "Epoch 814/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1580 - rmse: 0.1580 - val_loss: 0.0980 - val_rmse: 0.0980\n",
      "Epoch 815/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1563 - rmse: 0.1563 - val_loss: 0.0970 - val_rmse: 0.0970\n",
      "Epoch 816/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1563 - rmse: 0.1563 - val_loss: 0.0987 - val_rmse: 0.0987\n",
      "Epoch 817/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1573 - rmse: 0.1573 - val_loss: 0.1028 - val_rmse: 0.1028\n",
      "Epoch 818/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.1572 - rmse: 0.1572 - val_loss: 0.1091 - val_rmse: 0.1091\n",
      "Epoch 819/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.1549 - rmse: 0.1549 - val_loss: 0.0973 - val_rmse: 0.0973\n",
      "Epoch 820/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1620 - rmse: 0.1620 - val_loss: 0.1073 - val_rmse: 0.1073\n",
      "Epoch 821/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.1507 - rmse: 0.1507 - val_loss: 0.1053 - val_rmse: 0.1053\n",
      "Epoch 822/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1493 - rmse: 0.1493 - val_loss: 0.1011 - val_rmse: 0.1011\n",
      "Epoch 823/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1465 - rmse: 0.1465 - val_loss: 0.0997 - val_rmse: 0.0997\n",
      "Epoch 824/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1488 - rmse: 0.1488 - val_loss: 0.0984 - val_rmse: 0.0984\n",
      "Epoch 825/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1511 - rmse: 0.1511 - val_loss: 0.0985 - val_rmse: 0.0985\n",
      "Epoch 826/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1410 - rmse: 0.1410 - val_loss: 0.1167 - val_rmse: 0.1167\n",
      "Epoch 827/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1482 - rmse: 0.1482 - val_loss: 0.0966 - val_rmse: 0.0966\n",
      "Epoch 828/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1470 - rmse: 0.1470 - val_loss: 0.0970 - val_rmse: 0.0970\n",
      "Epoch 829/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1477 - rmse: 0.1477 - val_loss: 0.1049 - val_rmse: 0.1049\n",
      "Epoch 830/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1509 - rmse: 0.1509 - val_loss: 0.0967 - val_rmse: 0.0967\n",
      "Epoch 831/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1422 - rmse: 0.1422 - val_loss: 0.0979 - val_rmse: 0.0979\n",
      "Epoch 832/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1384 - rmse: 0.1384 - val_loss: 0.0974 - val_rmse: 0.0974\n",
      "Epoch 833/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1477 - rmse: 0.1477 - val_loss: 0.1010 - val_rmse: 0.1010\n",
      "Epoch 834/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1427 - rmse: 0.1427 - val_loss: 0.0994 - val_rmse: 0.0994\n",
      "Epoch 835/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1398 - rmse: 0.1398 - val_loss: 0.0987 - val_rmse: 0.0987\n",
      "Epoch 836/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1413 - rmse: 0.1413 - val_loss: 0.1035 - val_rmse: 0.1035\n",
      "Epoch 837/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1439 - rmse: 0.1439 - val_loss: 0.1004 - val_rmse: 0.1004\n",
      "Epoch 838/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1405 - rmse: 0.1405 - val_loss: 0.1083 - val_rmse: 0.1083\n",
      "Epoch 839/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1387 - rmse: 0.1387 - val_loss: 0.0983 - val_rmse: 0.0983\n",
      "Epoch 840/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1373 - rmse: 0.1373 - val_loss: 0.0972 - val_rmse: 0.0972\n",
      "Epoch 841/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1400 - rmse: 0.1400 - val_loss: 0.0994 - val_rmse: 0.0994\n",
      "Epoch 842/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1371 - rmse: 0.1371 - val_loss: 0.0973 - val_rmse: 0.0973\n",
      "Epoch 843/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1345 - rmse: 0.1345 - val_loss: 0.0981 - val_rmse: 0.0981\n",
      "Epoch 844/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1350 - rmse: 0.1350 - val_loss: 0.0975 - val_rmse: 0.0975\n",
      "Epoch 845/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1403 - rmse: 0.1403 - val_loss: 0.0974 - val_rmse: 0.0974\n",
      "Epoch 846/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1317 - rmse: 0.1317 - val_loss: 0.0958 - val_rmse: 0.0958\n",
      "Epoch 847/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1351 - rmse: 0.1351 - val_loss: 0.0971 - val_rmse: 0.0971\n",
      "Epoch 848/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1304 - rmse: 0.1304 - val_loss: 0.0964 - val_rmse: 0.0964\n",
      "Epoch 849/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.1327 - rmse: 0.1327 - val_loss: 0.0976 - val_rmse: 0.0976\n",
      "Epoch 850/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1313 - rmse: 0.1313 - val_loss: 0.0958 - val_rmse: 0.0958\n",
      "Epoch 851/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1342 - rmse: 0.1342 - val_loss: 0.0976 - val_rmse: 0.0976\n",
      "Epoch 852/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1362 - rmse: 0.1362 - val_loss: 0.0935 - val_rmse: 0.0935\n",
      "Epoch 853/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1304 - rmse: 0.1304 - val_loss: 0.0936 - val_rmse: 0.0936\n",
      "Epoch 854/1000\n",
      "1166/1166 [==============================] - 0s 12us/step - loss: 0.1319 - rmse: 0.1319 - val_loss: 0.0938 - val_rmse: 0.0938\n",
      "Epoch 855/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1313 - rmse: 0.1313 - val_loss: 0.0930 - val_rmse: 0.0930\n",
      "Epoch 856/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1318 - rmse: 0.1318 - val_loss: 0.0927 - val_rmse: 0.0927\n",
      "Epoch 857/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1322 - rmse: 0.1322 - val_loss: 0.1019 - val_rmse: 0.1019\n",
      "Epoch 858/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1331 - rmse: 0.1331 - val_loss: 0.0949 - val_rmse: 0.0949\n",
      "Epoch 859/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1257 - rmse: 0.1257 - val_loss: 0.0930 - val_rmse: 0.0930\n",
      "Epoch 860/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1250 - rmse: 0.1250 - val_loss: 0.0999 - val_rmse: 0.0999\n",
      "Epoch 861/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1265 - rmse: 0.1265 - val_loss: 0.0967 - val_rmse: 0.0967\n",
      "Epoch 862/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1300 - rmse: 0.1300 - val_loss: 0.0925 - val_rmse: 0.0925\n",
      "Epoch 863/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1262 - rmse: 0.1262 - val_loss: 0.0933 - val_rmse: 0.0933\n",
      "Epoch 864/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1276 - rmse: 0.1276 - val_loss: 0.0959 - val_rmse: 0.0959\n",
      "Epoch 865/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1304 - rmse: 0.1304 - val_loss: 0.1020 - val_rmse: 0.1020\n",
      "Epoch 866/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1287 - rmse: 0.1287 - val_loss: 0.0957 - val_rmse: 0.0957\n",
      "Epoch 867/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1255 - rmse: 0.1255 - val_loss: 0.0923 - val_rmse: 0.0923\n",
      "Epoch 868/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1234 - rmse: 0.1234 - val_loss: 0.1015 - val_rmse: 0.1015\n",
      "Epoch 869/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1269 - rmse: 0.1269 - val_loss: 0.0982 - val_rmse: 0.0982\n",
      "Epoch 870/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1283 - rmse: 0.1283 - val_loss: 0.0919 - val_rmse: 0.0919\n",
      "Epoch 871/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1206 - rmse: 0.1206 - val_loss: 0.0934 - val_rmse: 0.0934\n",
      "Epoch 872/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1209 - rmse: 0.1209 - val_loss: 0.0963 - val_rmse: 0.0963\n",
      "Epoch 873/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1271 - rmse: 0.1271 - val_loss: 0.1034 - val_rmse: 0.1034\n",
      "Epoch 874/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1249 - rmse: 0.1249 - val_loss: 0.0951 - val_rmse: 0.0951\n",
      "Epoch 875/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1212 - rmse: 0.1212 - val_loss: 0.0962 - val_rmse: 0.0962\n",
      "Epoch 876/1000\n",
      "1166/1166 [==============================] - 0s 18us/step - loss: 0.1200 - rmse: 0.1200 - val_loss: 0.0917 - val_rmse: 0.0917\n",
      "Epoch 877/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1246 - rmse: 0.1246 - val_loss: 0.1023 - val_rmse: 0.1023\n",
      "Epoch 878/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1185 - rmse: 0.1185 - val_loss: 0.0910 - val_rmse: 0.0910\n",
      "Epoch 879/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1213 - rmse: 0.1213 - val_loss: 0.0917 - val_rmse: 0.0917\n",
      "Epoch 880/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1204 - rmse: 0.1204 - val_loss: 0.1085 - val_rmse: 0.1085\n",
      "Epoch 881/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1233 - rmse: 0.1233 - val_loss: 0.0918 - val_rmse: 0.0918\n",
      "Epoch 882/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1217 - rmse: 0.1217 - val_loss: 0.0904 - val_rmse: 0.0904\n",
      "Epoch 883/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1186 - rmse: 0.1186 - val_loss: 0.0902 - val_rmse: 0.0902\n",
      "Epoch 884/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1180 - rmse: 0.1180 - val_loss: 0.0990 - val_rmse: 0.0990\n",
      "Epoch 885/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1204 - rmse: 0.1204 - val_loss: 0.0914 - val_rmse: 0.0914\n",
      "Epoch 886/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1187 - rmse: 0.1187 - val_loss: 0.0926 - val_rmse: 0.0926\n",
      "Epoch 887/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1159 - rmse: 0.1159 - val_loss: 0.0915 - val_rmse: 0.0915\n",
      "Epoch 888/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1182 - rmse: 0.1182 - val_loss: 0.0917 - val_rmse: 0.0917\n",
      "Epoch 889/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1139 - rmse: 0.1139 - val_loss: 0.0910 - val_rmse: 0.0910\n",
      "Epoch 890/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1148 - rmse: 0.1148 - val_loss: 0.0921 - val_rmse: 0.0921\n",
      "Epoch 891/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1126 - rmse: 0.1126 - val_loss: 0.1018 - val_rmse: 0.1018\n",
      "Epoch 892/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1191 - rmse: 0.1191 - val_loss: 0.1002 - val_rmse: 0.1002\n",
      "Epoch 893/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1157 - rmse: 0.1157 - val_loss: 0.0939 - val_rmse: 0.0939\n",
      "Epoch 894/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1134 - rmse: 0.1134 - val_loss: 0.0972 - val_rmse: 0.0972\n",
      "Epoch 895/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1201 - rmse: 0.1201 - val_loss: 0.0922 - val_rmse: 0.0922\n",
      "Epoch 896/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1149 - rmse: 0.1149 - val_loss: 0.0901 - val_rmse: 0.0901\n",
      "Epoch 897/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1136 - rmse: 0.1136 - val_loss: 0.0911 - val_rmse: 0.0911\n",
      "Epoch 898/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1125 - rmse: 0.1125 - val_loss: 0.0922 - val_rmse: 0.0922\n",
      "Epoch 899/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1168 - rmse: 0.1168 - val_loss: 0.0901 - val_rmse: 0.0901\n",
      "Epoch 900/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1171 - rmse: 0.1171 - val_loss: 0.0912 - val_rmse: 0.0912\n",
      "Epoch 901/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1104 - rmse: 0.1104 - val_loss: 0.0918 - val_rmse: 0.0918\n",
      "Epoch 902/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1144 - rmse: 0.1144 - val_loss: 0.0910 - val_rmse: 0.0910\n",
      "Epoch 903/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1106 - rmse: 0.1106 - val_loss: 0.0914 - val_rmse: 0.0914\n",
      "Epoch 904/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1163 - rmse: 0.1163 - val_loss: 0.0898 - val_rmse: 0.0898\n",
      "Epoch 905/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1161 - rmse: 0.1161 - val_loss: 0.0907 - val_rmse: 0.0907\n",
      "Epoch 906/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1184 - rmse: 0.1184 - val_loss: 0.0944 - val_rmse: 0.0944\n",
      "Epoch 907/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1132 - rmse: 0.1132 - val_loss: 0.0911 - val_rmse: 0.0911\n",
      "Epoch 908/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1144 - rmse: 0.1144 - val_loss: 0.0903 - val_rmse: 0.0903\n",
      "Epoch 909/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1159 - rmse: 0.1159 - val_loss: 0.0904 - val_rmse: 0.0904\n",
      "Epoch 910/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1116 - rmse: 0.1116 - val_loss: 0.0904 - val_rmse: 0.0904\n",
      "Epoch 911/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1105 - rmse: 0.1105 - val_loss: 0.0921 - val_rmse: 0.0921\n",
      "Epoch 912/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1110 - rmse: 0.1110 - val_loss: 0.0901 - val_rmse: 0.0901\n",
      "Epoch 913/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1093 - rmse: 0.1093 - val_loss: 0.0968 - val_rmse: 0.0968\n",
      "Epoch 914/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1120 - rmse: 0.1120 - val_loss: 0.0909 - val_rmse: 0.0909\n",
      "Epoch 915/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1128 - rmse: 0.1128 - val_loss: 0.0939 - val_rmse: 0.0939\n",
      "Epoch 916/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1074 - rmse: 0.1074 - val_loss: 0.0936 - val_rmse: 0.0936\n",
      "Epoch 917/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1081 - rmse: 0.1081 - val_loss: 0.0907 - val_rmse: 0.0907\n",
      "Epoch 918/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1098 - rmse: 0.1098 - val_loss: 0.0887 - val_rmse: 0.0887\n",
      "Epoch 919/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1079 - rmse: 0.1079 - val_loss: 0.0906 - val_rmse: 0.0906\n",
      "Epoch 920/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1095 - rmse: 0.1095 - val_loss: 0.0900 - val_rmse: 0.0900\n",
      "Epoch 921/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1092 - rmse: 0.1092 - val_loss: 0.0930 - val_rmse: 0.0930\n",
      "Epoch 922/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1147 - rmse: 0.1147 - val_loss: 0.0881 - val_rmse: 0.0881\n",
      "Epoch 923/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1104 - rmse: 0.1104 - val_loss: 0.0879 - val_rmse: 0.0879\n",
      "Epoch 924/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1108 - rmse: 0.1108 - val_loss: 0.0879 - val_rmse: 0.0879\n",
      "Epoch 925/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1104 - rmse: 0.1104 - val_loss: 0.0878 - val_rmse: 0.0878\n",
      "Epoch 926/1000\n",
      "1166/1166 [==============================] - 0s 16us/step - loss: 0.1062 - rmse: 0.1062 - val_loss: 0.0934 - val_rmse: 0.0934\n",
      "Epoch 927/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1075 - rmse: 0.1075 - val_loss: 0.0880 - val_rmse: 0.0880\n",
      "Epoch 928/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1105 - rmse: 0.1105 - val_loss: 0.0885 - val_rmse: 0.0885\n",
      "Epoch 929/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1081 - rmse: 0.1081 - val_loss: 0.0903 - val_rmse: 0.0903\n",
      "Epoch 930/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1086 - rmse: 0.1086 - val_loss: 0.0886 - val_rmse: 0.0886\n",
      "Epoch 931/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1096 - rmse: 0.1096 - val_loss: 0.0887 - val_rmse: 0.0887\n",
      "Epoch 932/1000\n",
      "1166/1166 [==============================] - 0s 21us/step - loss: 0.1078 - rmse: 0.1078 - val_loss: 0.0971 - val_rmse: 0.0971\n",
      "Epoch 933/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1059 - rmse: 0.1059 - val_loss: 0.0900 - val_rmse: 0.0900\n",
      "Epoch 934/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1039 - rmse: 0.1039 - val_loss: 0.0914 - val_rmse: 0.0914\n",
      "Epoch 935/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1038 - rmse: 0.1038 - val_loss: 0.0890 - val_rmse: 0.0890\n",
      "Epoch 936/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1054 - rmse: 0.1054 - val_loss: 0.0905 - val_rmse: 0.0905\n",
      "Epoch 937/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1043 - rmse: 0.1043 - val_loss: 0.0895 - val_rmse: 0.0895\n",
      "Epoch 938/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1037 - rmse: 0.1037 - val_loss: 0.0916 - val_rmse: 0.0916\n",
      "Epoch 939/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1059 - rmse: 0.1059 - val_loss: 0.0890 - val_rmse: 0.0890\n",
      "Epoch 940/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1046 - rmse: 0.1046 - val_loss: 0.0901 - val_rmse: 0.0901\n",
      "Epoch 941/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1079 - rmse: 0.1079 - val_loss: 0.0972 - val_rmse: 0.0972\n",
      "Epoch 942/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1034 - rmse: 0.1034 - val_loss: 0.0937 - val_rmse: 0.0937\n",
      "Epoch 943/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1086 - rmse: 0.1086 - val_loss: 0.0880 - val_rmse: 0.0880\n",
      "Epoch 944/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1042 - rmse: 0.1042 - val_loss: 0.0889 - val_rmse: 0.0889\n",
      "Epoch 945/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1040 - rmse: 0.1040 - val_loss: 0.0888 - val_rmse: 0.0888\n",
      "Epoch 946/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1039 - rmse: 0.1039 - val_loss: 0.0891 - val_rmse: 0.0891\n",
      "Epoch 947/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1055 - rmse: 0.1055 - val_loss: 0.0887 - val_rmse: 0.0887\n",
      "Epoch 948/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1040 - rmse: 0.1040 - val_loss: 0.0891 - val_rmse: 0.0891\n",
      "Epoch 949/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1024 - rmse: 0.1024 - val_loss: 0.0884 - val_rmse: 0.0884\n",
      "Epoch 950/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1032 - rmse: 0.1032 - val_loss: 0.0888 - val_rmse: 0.0888\n",
      "Epoch 951/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1043 - rmse: 0.1043 - val_loss: 0.0907 - val_rmse: 0.0907\n",
      "Epoch 952/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1043 - rmse: 0.1043 - val_loss: 0.0939 - val_rmse: 0.0939\n",
      "Epoch 953/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1047 - rmse: 0.1047 - val_loss: 0.0928 - val_rmse: 0.0928\n",
      "Epoch 954/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1043 - rmse: 0.1043 - val_loss: 0.0886 - val_rmse: 0.0886\n",
      "Epoch 955/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1050 - rmse: 0.1050 - val_loss: 0.0940 - val_rmse: 0.0940\n",
      "Epoch 956/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1066 - rmse: 0.1066 - val_loss: 0.0928 - val_rmse: 0.0928\n",
      "Epoch 957/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1021 - rmse: 0.1021 - val_loss: 0.0921 - val_rmse: 0.0921\n",
      "Epoch 958/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1052 - rmse: 0.1052 - val_loss: 0.0900 - val_rmse: 0.0900\n",
      "Epoch 959/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.0996 - rmse: 0.0996 - val_loss: 0.0876 - val_rmse: 0.0876\n",
      "Epoch 960/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1046 - rmse: 0.1046 - val_loss: 0.0889 - val_rmse: 0.0889\n",
      "Epoch 961/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1015 - rmse: 0.1015 - val_loss: 0.0881 - val_rmse: 0.0881\n",
      "Epoch 962/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1020 - rmse: 0.1020 - val_loss: 0.0939 - val_rmse: 0.0939\n",
      "Epoch 963/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1061 - rmse: 0.1061 - val_loss: 0.0883 - val_rmse: 0.0883\n",
      "Epoch 964/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1048 - rmse: 0.1048 - val_loss: 0.0878 - val_rmse: 0.0878\n",
      "Epoch 965/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1019 - rmse: 0.1019 - val_loss: 0.0889 - val_rmse: 0.0889\n",
      "Epoch 966/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1026 - rmse: 0.1026 - val_loss: 0.0877 - val_rmse: 0.0877\n",
      "Epoch 967/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.0983 - rmse: 0.0983 - val_loss: 0.0933 - val_rmse: 0.0933\n",
      "Epoch 968/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1030 - rmse: 0.1030 - val_loss: 0.0881 - val_rmse: 0.0881\n",
      "Epoch 969/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1024 - rmse: 0.1024 - val_loss: 0.0879 - val_rmse: 0.0879\n",
      "Epoch 970/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1027 - rmse: 0.1027 - val_loss: 0.0882 - val_rmse: 0.0882\n",
      "Epoch 971/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.0992 - rmse: 0.0992 - val_loss: 0.0874 - val_rmse: 0.0874\n",
      "Epoch 972/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1005 - rmse: 0.1005 - val_loss: 0.1007 - val_rmse: 0.1007\n",
      "Epoch 973/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1020 - rmse: 0.1020 - val_loss: 0.0870 - val_rmse: 0.0870\n",
      "Epoch 974/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1028 - rmse: 0.1028 - val_loss: 0.1040 - val_rmse: 0.1040\n",
      "Epoch 975/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1014 - rmse: 0.1014 - val_loss: 0.0877 - val_rmse: 0.0877\n",
      "Epoch 976/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1000 - rmse: 0.1000 - val_loss: 0.0904 - val_rmse: 0.0904\n",
      "Epoch 977/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1018 - rmse: 0.1018 - val_loss: 0.0880 - val_rmse: 0.0880\n",
      "Epoch 978/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.1025 - rmse: 0.1025 - val_loss: 0.0874 - val_rmse: 0.0874\n",
      "Epoch 979/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1049 - rmse: 0.1049 - val_loss: 0.0911 - val_rmse: 0.0911\n",
      "Epoch 980/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1017 - rmse: 0.1017 - val_loss: 0.0962 - val_rmse: 0.0962\n",
      "Epoch 981/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1016 - rmse: 0.1016 - val_loss: 0.1004 - val_rmse: 0.1004\n",
      "Epoch 982/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1021 - rmse: 0.1021 - val_loss: 0.0885 - val_rmse: 0.0885\n",
      "Epoch 983/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1010 - rmse: 0.1010 - val_loss: 0.0886 - val_rmse: 0.0886\n",
      "Epoch 984/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1019 - rmse: 0.1019 - val_loss: 0.0892 - val_rmse: 0.0892\n",
      "Epoch 985/1000\n",
      "1166/1166 [==============================] - 0s 13us/step - loss: 0.0989 - rmse: 0.0989 - val_loss: 0.1003 - val_rmse: 0.1003\n",
      "Epoch 986/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1005 - rmse: 0.1005 - val_loss: 0.0901 - val_rmse: 0.0901\n",
      "Epoch 987/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1004 - rmse: 0.1004 - val_loss: 0.0925 - val_rmse: 0.0925\n",
      "Epoch 988/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1046 - rmse: 0.1046 - val_loss: 0.0884 - val_rmse: 0.0884\n",
      "Epoch 989/1000\n",
      "1166/1166 [==============================] - 0s 17us/step - loss: 0.1029 - rmse: 0.1029 - val_loss: 0.0881 - val_rmse: 0.0881\n",
      "Epoch 990/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.0982 - rmse: 0.0982 - val_loss: 0.0876 - val_rmse: 0.0876\n",
      "Epoch 991/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1025 - rmse: 0.1025 - val_loss: 0.0890 - val_rmse: 0.0890\n",
      "Epoch 992/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1019 - rmse: 0.1019 - val_loss: 0.0996 - val_rmse: 0.0996\n",
      "Epoch 993/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.1002 - rmse: 0.1002 - val_loss: 0.0946 - val_rmse: 0.0946\n",
      "Epoch 994/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1007 - rmse: 0.1007 - val_loss: 0.0881 - val_rmse: 0.0881\n",
      "Epoch 995/1000\n",
      "1166/1166 [==============================] - 0s 14us/step - loss: 0.0984 - rmse: 0.0984 - val_loss: 0.0960 - val_rmse: 0.0960\n",
      "Epoch 996/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1006 - rmse: 0.1006 - val_loss: 0.0895 - val_rmse: 0.0895\n",
      "Epoch 997/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.0991 - rmse: 0.0991 - val_loss: 0.0871 - val_rmse: 0.0871\n",
      "Epoch 998/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1034 - rmse: 0.1034 - val_loss: 0.0894 - val_rmse: 0.0894\n",
      "Epoch 999/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1047 - rmse: 0.1047 - val_loss: 0.0926 - val_rmse: 0.0926\n",
      "Epoch 1000/1000\n",
      "1166/1166 [==============================] - 0s 15us/step - loss: 0.1057 - rmse: 0.1057 - val_loss: 0.0955 - val_rmse: 0.0955\n"
     ]
    }
   ],
   "source": [
    "history = NN_model.fit(X, y, batch_size=100, epochs=1000,\n",
    "              validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot loss function and accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_perf(history):\n",
    "    acc = history.history['rmse']\n",
    "    val_acc = history.history['val_rmse']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "# plot loss and accuracy\n",
    "plot_perf(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
